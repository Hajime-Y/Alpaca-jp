{"id": "amenokaku_0", "instruction": "pandasでレシート明細データ（df_receipt）から全項目の先頭10件を表示し、どのようなデータを保有しているか目視で確認せよ。", "instances": [{"input": "", "output": "import pandas as pd\n\ndf_receipt.head(10)"}], "task": "code_generation", "index": 0, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_1", "instruction": "pandasでレシート明細データ（df_receipt）から売上年月日（sales_ymd）、顧客ID（customer_id）、商品コード（product_cd）、売上金額（amount）の順に列を指定し、10件表示せよ。", "instances": [{"input": "", "output": "import pandas as pd\n\ndf_receipt[['sales_ymd', 'customer_id', 'product_cd', 'amount']].head(10)"}], "task": "code_generation", "index": 1, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_2", "instruction": "pandasでレシート明細データ（df_receipt）から売上年月日（sales_ymd）、顧客ID（customer_id）、商品コード（product_cd）、売上金額（amount）の順に列を指定し、10件表示せよ。ただし、sales_ymdsales_dateに項目名を変更しながら抽出すること。", "instances": [{"input": "", "output": "import pandas as pd\n\ndf_receipt[['sales_ymd', 'customer_id', 'product_cd', 'amount']]. \\\n                        rename(columns={'sales_ymd': 'sales_date'}).head(10)"}], "task": "code_generation", "index": 2, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_3", "instruction": "pandasでレシート明細データ（df_receipt）から売上日（sales_ymd）、顧客ID（customer_id）、商品コード（product_cd）、売上金額（amount）の順に列を指定し、以下の条件を満たすデータを抽出せよ。", "instances": [{"input": "", "output": "import pandas as pd\n\n# コード例1（queryを使う場合）\ndf_receipt[['sales_ymd', 'customer_id', 'product_cd', 'amount']]. \\\n                                query('customer_id == \"CS018205000001\"')\n\n# コード例1（queryを使わない場合）\ndf = df_receipt[['sales_ymd', 'customer_id', 'product_cd', 'amount']]\ndf[df['customer_id'] == 'CS018205000001']"}], "task": "code_generation", "index": 3, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_4", "instruction": "pandasを用いてシート明細データ（df_receipt）から売上日（sales_ymd）、顧客ID（customer_id）、商品コード（product_cd）、売上金額（amount）の順に列を指定し、以下の全ての条件を満たすデータを抽出せよ。\n\n顧客ID（customer_id）が\"CS018205000001\"\n売上金額（amount）が1,000以上", "instances": [{"input": "", "output": "import pandas as pd\n\ndf_receipt[['sales_ymd', 'customer_id', 'product_cd', 'amount']] \\\n            .query('customer_id == \"CS018205000001\" & amount >= 1000')"}], "task": "code_generation", "index": 4, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_5", "instruction": "pandasを用いてレシート明細データ（df_receipt）から売上日（sales_ymd）、顧客ID（customer_id）、商品コード（product_cd）、売上数量（quantity）、売上金額（amount）の順に列を指定し、以下の全ての条件を満たすデータを抽出せよ。\n\n顧客ID（customer_id）が\"CS018205000001\"\n売上金額（amount）が1,000以上または売上数量（quantity）が5以上", "instances": [{"input": "", "output": "df_receipt[['sales_ymd', 'customer_id', 'product_cd', 'quantity', 'amount']].\\\n    query('customer_id == \"CS018205000001\" & (amount >= 1000 | quantity >=5)')"}], "task": "code_generation", "index": 5, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_6", "instruction": "pandasを用いてレシート明細データ（df_receipt）から売上日（sales_ymd）、顧客ID（customer_id）、商品コード（product_cd）、売上金額（amount）の順に列を指定し、以下の全ての条件を満たすデータを抽出せよ。\n\n顧客ID（customer_id）が\"CS018205000001\"\n売上金額（amount）が1,000以上2,000以下", "instances": [{"input": "", "output": "import pandas as pd\n\ndf_receipt[['sales_ymd', 'customer_id', 'product_cd', 'amount']] \\\n    .query('customer_id == \"CS018205000001\" & 1000 <= amount <= 2000')"}], "task": "code_generation", "index": 6, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_7", "instruction": "pandasを用いてレシート明細データ（df_receipt）から売上日（sales_ymd）、顧客ID（customer_id）、商品コード（product_cd）、売上金額（amount）の順に列を指定し、以下の全ての条件を満たすデータを抽出せよ。\n\n顧客ID（customer_id）が\"CS018205000001\"\n商品コード（product_cd）が\"P071401019\"以外", "instances": [{"input": "", "output": "import pandas as pd\n\ndf_receipt[['sales_ymd', 'customer_id', 'product_cd', 'amount']] \\\n    .query('customer_id == \"CS018205000001\" & product_cd != \"P071401019\"')"}], "task": "code_generation", "index": 7, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_8", "instruction": "pandasを用いて以下の処理において、出力結果を変えずにORをANDに書き換えよ。\n\ndf_store.query('not(prefecture_cd == \"13\" | floor_area > 900)')", "instances": [{"input": "", "output": "import pandas as pd\n\ndf_store.query('prefecture_cd != \"13\" & floor_area <= 900')"}], "task": "code_generation", "index": 8, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_9", "instruction": "pandasを用いて店舗データ（df_store）から、店舗コード（store_cd）が\"S14\"で始まるものだけ全項目抽出し、10件表示せよ。", "instances": [{"input": "", "output": "import pandas as pd\n\ndf_store.query(\"store_cd.str.startswith('S14')\", engine='python').head(10)"}], "task": "code_generation", "index": 9, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_10", "instruction": "pandasを用いて顧客データ（df_customer）から顧客ID（customer_id）の末尾が1のものだけ全項目抽出し、10件表示せよ。", "instances": [{"input": "", "output": "import pandas as pd\n\ndf_customer.query(\"customer_id.str.endswith('1')\", engine='python').head(10)"}], "task": "code_generation", "index": 10, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_11", "instruction": "pandasを用いて店舗データ（df_store）から、住所 (address) に\"横浜市\"が含まれるものだけ全項目表示せよ。", "instances": [{"input": "", "output": "import pandas as pd\n\ndf_store.query(\"address.str.contains('横浜市')\", engine='python')"}], "task": "code_generation", "index": 11, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_12", "instruction": "pandasを用いて顧客データ（df_customer）から、ステータスコード（status_cd）の先頭がアルファベットのA〜Fで始まるデータを全項目抽出し、10件表示せよ。", "instances": [{"input": "", "output": "import pandas as pd\n\ndf_customer.query(\"status_cd.str.contains(r'^[A-F]')\", \n                  engine='python').head(10)"}], "task": "code_generation", "index": 12, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_13", "instruction": "pandasを用いて顧客データ（df_customer）から、ステータスコード（status_cd）の末尾が数字の1〜9で終わるデータを全項目抽出し、10件表示せよ。", "instances": [{"input": "", "output": "import pandas as pd\n\n# regexのオプションをつけることもできる（Falseにすれば正規表現ではなくそのままの文字列として扱われる）\ndf_customer.query(\"status_cd.str.contains(r'[1-9]$', regex=True)\", \n                  engine='python').head(10)"}], "task": "code_generation", "index": 13, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_14", "instruction": "pandasを用いて顧客データ（df_customer）から、ステータスコード（status_cd）の先頭がアルファベットのA〜Fで始まり、末尾が数字の1〜9で終わるデータを全項目抽出し、10件表示せよ。", "instances": [{"input": "", "output": "import pandas as pd\n\ndf_customer.query(\"status_cd.str.contains(r'^[A-F].*[1-9]$')\", \n                  engine='python').head(10)"}], "task": "code_generation", "index": 14, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_15", "instruction": "pandasを用いて店舗データ（df_store）から、電話番号（tel_no）が3桁-3桁-4桁のデータを全項目表示せよ。", "instances": [{"input": "", "output": "import pandas as pd\n\ndf_store.query(\"tel_no.str.contains(r'^[0-9]{3}-[0-9]{3}-[0-9]{4}$')\", \n               engine='python')"}], "task": "code_generation", "index": 15, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_16", "instruction": "pandasを用いて顧客データ（df_customer）を生年月日（birth_day）で高齢順にソートし、先頭から全項目を10件表示せよ。", "instances": [{"input": "", "output": "import pandas as pd\n\ndf_customer.sort_values('birth_day').head(10)"}], "task": "code_generation", "index": 16, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_17", "instruction": "pandasを用いて顧客データ（df_customer）を生年月日（birth_day）で若い順にソートし、先頭から全項目を10件表示せよ。", "instances": [{"input": "", "output": "import pandas as pd\n\ndf_customer.sort_values('birth_day', ascending=False).head(10)"}], "task": "code_generation", "index": 17, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_18", "instruction": "pandasを用いてレシート明細データ（df_receipt）に対し、1件あたりの売上金額（amount）が高い順にランクを付与し、先頭から10件表示せよ。項目は顧客ID（customer_id）、売上金額（amount）、付与したランクを表示させること。なお、売上金額（amount）が等しい場合は同一順位を付与するものとする。", "instances": [{"input": "", "output": "import pandas as pd\n\ndf_tmp = pd.concat([df_receipt[['customer_id', 'amount']] \n                     ,df_receipt['amount'].rank(method='min', \n                                                ascending=False)], axis=1)\n\ndf_tmp.columns = ['customer_id', 'amount', 'ranking']\n\ndf_tmp.sort_values('ranking').head(10)"}], "task": "code_generation", "index": 18, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_19", "instruction": "pandasを用いてレシート明細データ（df_receipt）に対し、1件あたりの売上金額（amount）が高い順にランクを付与し、先頭から10件表示せよ。項目は顧客ID（customer_id）、売上金額（amount）、付与したランクを表示させレシート明細データ（df_receipt）に対し、1件あたりの売上金額（amount）が高い順にランクを付与し、先頭から10件表示せよ。項目は顧客ID（customer_id）、売上金額（amount）、付与したランクを表示させること。なお、売上金額（amount）が等しい場合でも別順位を付与すること。", "instances": [{"input": "", "output": "import pandas as pd\n\ndf_tmp = pd.concat([df_receipt[['customer_id', 'amount']] \n                     ,df_receipt['amount'].rank(method='first', \n                                                ascending=False)], axis=1)\n\ndf_tmp.columns = ['customer_id', 'amount', 'ranking']\n\ndf_tmp.sort_values('ranking').head(10)"}], "task": "code_generation", "index": 19, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_20", "instruction": "pandasを用いてレシート明細データ（df_receipt）に対し、件数をカウントせよ。", "instances": [{"input": "", "output": "import pandas as pd\n\nlen(df_receipt)"}], "task": "code_generation", "index": 20, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_21", "instruction": "pandasを用いてレシート明細データ（df_receipt）の顧客ID（customer_id）に対し、ユニーク件数をカウントせよ。", "instances": [{"input": "", "output": "import pandas as pd\n\nlen(df_receipt['customer_id'].unique())"}], "task": "code_generation", "index": 21, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_22", "instruction": "pandasを用いてレシート明細データ（df_receipt）に対し、店舗コード（store_cd）ごとに売上金額（amount）と売上数量（quantity）を合計せよ。", "instances": [{"input": "", "output": "import pandas as pd\n\n# コード例1\ndf_receipt.groupby('store_cd').agg({'amount':'sum', \n                                    'quantity':'sum'}).reset_index()\n\n# コード例2\ndf_receipt.groupby('store_cd')[['amount','quantity']].agg('sum').reset_index()"}], "task": "code_generation", "index": 22, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_23", "instruction": "pandasを用いてレシート明細データ（df_receipt）に対し、顧客ID（customer_id）ごとに最も新しい売上年月日（sales_ymd）を求め、10件表示せよ。", "instances": [{"input": "", "output": "import pandas as pd\n\ndf_receipt.groupby('customer_id').agg({'sales_ymd': 'max'}).reset_index().head(10)"}], "task": "code_generation", "index": 23, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_24", "instruction": "pandasを用いてレシート明細データ（df_receipt）に対し、顧客ID（customer_id）ごとに最も古い売上年月日（sales_ymd）を求め、10件表示せよ。", "instances": [{"input": "", "output": "import pandas as pd\n\ndf_receipt.groupby('customer_id').sales_ymd.min().reset_index().head(10)"}], "task": "code_generation", "index": 24, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_25", "instruction": "pandasを用いてレシート明細データ（df_receipt）に対し、顧客ID（customer_id）ごとに最も新しい売上年月日（sales_ymd）と古い売上年月日を求め、両者が異なるデータを10件表示せよ。", "instances": [{"input": "", "output": "import pandas as pd\n\ndf_tmp = df_receipt.groupby('customer_id'). \\\n                agg({'sales_ymd':['max','min']}).reset_index()\n\n# マルチインデックス（項目）の階層を\"_\"でつなぎながら1階層のインデックス（項目）にする\n# df_tmp.columns = ['customer_id', 'sales_ymd_max', 'sales_ymd_min'] としても良い\ndf_tmp.columns = [\"_\".join(pair) for pair in df_tmp.columns]\n\ndf_tmp.query('sales_ymd_max != sales_ymd_min').head(10)"}], "task": "code_generation", "index": 25, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_26", "instruction": "pandasを用いてレシート明細データ（df_receipt）に対し、店舗コード（store_cd）ごとに売上金額（amount）の平均を計算し、降順でTOP5を表示せよ。", "instances": [{"input": "", "output": "import pandas as pd\n\ndf_receipt.groupby('store_cd').agg({'amount':'mean'}).reset_index(). \\\n                            sort_values('amount', ascending=False).head(5)"}], "task": "code_generation", "index": 26, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_27", "instruction": "pandasを用いてレシート明細データ（df_receipt）に対し、店舗コード（store_cd）ごとに売上金額（amount）の中央値を計算し、降順でTOP5を表示せよ。", "instances": [{"input": "", "output": "import pandas as pd\n\ndf_receipt.groupby('store_cd').agg({'amount':'median'}).reset_index(). \\\n                            sort_values('amount', ascending=False).head(5)"}], "task": "code_generation", "index": 27, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_28", "instruction": "pandasを用いてレシート明細データ（df_receipt）に対し、店舗コード（store_cd）ごとに商品コード（product_cd）の最頻値を求め、10件表示させよ。", "instances": [{"input": "", "output": "import pandas as pd\n\ndf_receipt.groupby('store_cd').product_cd. \\\n            apply(lambda x: x.mode()).reset_index().head(10)"}], "task": "code_generation", "index": 28, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_29", "instruction": "pandasを用いてレシート明細データ（df_receipt）に対し、店舗コード（store_cd）ごとに売上金額（amount）の分散を計算し、降順で5件表示せよ。", "instances": [{"input": "", "output": "import pandas as pd\n\ndf_receipt.groupby('store_cd').amount.var(ddof=0).reset_index(). \\\n                            sort_values('amount', ascending=False).head(5)"}], "task": "code_generation", "index": 29, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_30", "instruction": "pandasを用いてレシート明細データ（df_receipt）に対し、店舗コード（store_cd）ごとに売上金額（amount）の標準偏差を計算し、降順で5件表示せよ。", "instances": [{"input": "", "output": "import pandas as pd\n\ndf_receipt.groupby('store_cd').amount.std(ddof=0).reset_index(). \\\n                            sort_values('amount', ascending=False).head(5)\n\nTIPS:\nPandasとNumpyでddofのデフォルト値が異なることに注意しましょう\n\nPandas：\nDataFrame.std(self, axis=None, skipna=None, level=None, ddof=1, numeric_only=None, **kwargs)\nNumpy:\nnumpy.std(a, axis=None, dtype=None, out=None, ddof=0, keepdims=)"}], "task": "code_generation", "index": 30, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_31", "instruction": "pandasを用いてレシート明細データ（df_receipt）の売上金額（amount）について、25％刻みでパーセンタイル値を求めよ。", "instances": [{"input": "", "output": "import pandas as pd\n\n# コード例1\nnp.percentile(df_receipt['amount'], q=np.arange(1, 5) * 25)\n\n# コード例2\ndf_receipt.amount.quantile(q=np.arange(1, 5) / 4)"}], "task": "code_generation", "index": 31, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_32", "instruction": "pandasを用いてレシート明細データ（df_receipt）に対し、店舗コード（store_cd）ごとに売上金額（amount）の平均を計算し、330以上のものを抽出せよ。", "instances": [{"input": "", "output": "import pandas as pd\n\ndf_receipt.groupby('store_cd').amount.mean(). \\\n                    reset_index().query('amount >= 330')"}], "task": "code_generation", "index": 32, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_33", "instruction": "pandasを用いてレシート明細データ（df_receipt）に対し、顧客ID（customer_id）ごとに売上金額（amount）を合計して全顧客の平均を求めよ。ただし、顧客IDが\"Z\"から始まるものは非会員を表すため、除外して計算すること。", "instances": [{"input": "", "output": "import pandas as pd\n\n# コード例1: queryを使わない書き方\ndf_receipt[~df_receipt['customer_id'].str.startswith(\"Z\")]. \\\n                            groupby('customer_id').amount.sum().mean()\n\n# コード例2: queryを使う書き方\ndf_receipt.query('not customer_id.str.startswith(\"Z\")', \n                 engine='python').groupby('customer_id').amount.sum().mean()"}], "task": "code_generation", "index": 33, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_34", "instruction": "pandasを用いてレシート明細データ（df_receipt）に対し、顧客ID（customer_id）ごとに売上金額（amount）を合計して全顧客の平均を求め、平均以上に買い物をしている顧客を抽出し、10件表示せよ。ただし、顧客IDが\"Z\"から始まるものは非会員を表すため、除外して計算すること。", "instances": [{"input": "", "output": "import pandas as pd\n\ndf_amount_sum = df_receipt[~df_receipt['customer_id'].str.startswith(\"Z\")].\\\n                                    groupby('customer_id').amount.sum()\n\namount_mean = df_amount_sum.mean()\n\ndf_amount_sum = df_amount_sum.reset_index()\n\ndf_amount_sum[df_amount_sum['amount'] >= amount_mean].head(10)"}], "task": "code_generation", "index": 34, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_35", "instruction": "pandasを用いてレシート明細データ（df_receipt）と店舗データ（df_store）を内部結合し、レシート明細データの全項目と店舗データの店舗名（store_name）を10件表示せよ。", "instances": [{"input": "", "output": "import pandas as pd\n\npd.merge(df_receipt, df_store[['store_cd','store_name']], \n         how='inner', on='store_cd').head(10)"}], "task": "code_generation", "index": 35, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_36", "instruction": "pandasを用いて商品データ（df_product）とカテゴリデータ（df_category）を内部結合し、商品データの全項目とカテゴリデータのカテゴリ小区分名（category_small_name）を10件表示せよ。", "instances": [{"input": "", "output": "import pandas as pd\n\npd.merge(df_product\n         , df_category[['category_small_cd','category_small_name']]\n         , how='inner', on='category_small_cd').head(10)"}], "task": "code_generation", "index": 36, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_37", "instruction": "pandasを用いて顧客データ（df_customer）とレシート明細データ（df_receipt）から、顧客ごとの売上金額合計を求め、10件表示せよ。ただし、売上実績がない顧客については売上金額を0として表示させること。また、顧客は性別コード（gender_cd）が女性（1）であるものを対象とし、非会員（顧客IDが\"Z\"から始まるもの）は除外すること。", "instances": [{"input": "", "output": "import pandas as pd\n\ndf_amount_sum = df_receipt.groupby('customer_id').amount.sum().reset_index()\n\ndf_tmp = df_customer. \\\n            query('gender_cd == \"1\" and not customer_id.str.startswith(\"Z\")', \n                  engine='python')\n\npd.merge(df_tmp['customer_id'], df_amount_sum, \n         how='left', on='customer_id').fillna(0).head(10)"}], "task": "code_generation", "index": 37, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_38", "instruction": "pandasを用いてレシート明細データ（df_receipt）から、売上日数の多い顧客の上位20件を抽出したデータと、売上金額合計の多い顧客の上位20件を抽出したデータをそれぞれ作成し、さらにその2つを完全外部結合せよ。ただし、非会員（顧客IDが\"Z\"から始まるもの）は除外すること。", "instances": [{"input": "", "output": "import pandas as pd\n\ndf_data = df_receipt \\\n          .query('not customer_id.str.startswith(\"Z\")', engine='python')\n\ndf_cnt = df_data[~df_data.duplicated(subset=['customer_id', 'sales_ymd'])] \\\n         .groupby('customer_id').sales_ymd.count().reset_index() \\\n         .sort_values('sales_ymd', ascending=False).head(20)\n\ndf_sum = df_data.groupby('customer_id').amount.sum().reset_index() \\\n         .sort_values('amount', ascending=False).head(20)\n\npd.merge(df_cnt, df_sum, how='outer', on='customer_id')"}], "task": "code_generation", "index": 38, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_39", "instruction": "pandasを用いて全ての店舗と全ての商品を組み合わせたデータを作成したい。店舗データ（df_store）と商品データ（df_product）を直積し、件数を計算せよ。", "instances": [{"input": "", "output": "import pandas as pd\n\ndf_store_tmp = df_store.copy()\ndf_product_tmp = df_product.copy()\n\ndf_store_tmp['key'] = 0\ndf_product_tmp['key'] = 0\n\nlen(pd.merge(df_store_tmp, df_product_tmp, how='outer', on='key'))"}], "task": "code_generation", "index": 39, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_40", "instruction": "pandasを用いてレシート明細データ（df_receipt）の売上金額（amount）を日付（sales_ymd）ごとに集計し、前回売上があった日からの売上金額増減を計算せよ。そして結果を10件表示せよ。", "instances": [{"input": "", "output": "import pandas as pd\n\ndf_sales_amount_by_date = df_receipt[['sales_ymd', 'amount']].\\\n                                groupby('sales_ymd').sum().reset_index()\n\ndf_sales_amount_by_date = pd.concat([df_sales_amount_by_date, \n                                     df_sales_amount_by_date.shift()], axis=1)\n\ndf_sales_amount_by_date.columns = ['sales_ymd','amount','lag_ymd','lag_amount']\n\ndf_sales_amount_by_date['diff_amount'] = \\\n    df_sales_amount_by_date['amount'] - df_sales_amount_by_date['lag_amount']\n\ndf_sales_amount_by_date.head(10)"}], "task": "code_generation", "index": 40, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_41", "instruction": "pandasを用いてレシート明細データ（df_receipt）の売上金額（amount）を日付（sales_ymd）ごとに集計し、各日付のデータに対し、前回、前々回、3回前に売上があった日のデータを結合せよ。そして結果を10件表示せよ。", "instances": [{"input": "", "output": "import pandas as pd\n\n# コード例1:縦持ちケース\ndf_sales_amount_by_date = df_receipt[['sales_ymd', 'amount']]. \\\n                                groupby('sales_ymd').sum().reset_index()\n\nfor i in range(1, 4):\n    df_tmp = pd.concat([df_sales_amount_by_date, \n                        df_sales_amount_by_date.shift(i)], axis=1)\n    if i == 1:\n        df_lag = df_tmp\n    else:\n        # DataFrameでappendが削除されるためappend -> concatに変更\n        df_lag = pd.concat([df_lag, df_tmp], axis=0)\n\ndf_lag.columns = ['sales_ymd', 'amount', 'lag_ymd', 'lag_amount']\n\ndf_lag.dropna().astype(int).sort_values(['sales_ymd','lag_ymd']).head(10)\n\n\n# コード例2:横持ちケース\ndf_sales_amount_by_date = df_receipt[['sales_ymd', 'amount']].\\\n                                groupby('sales_ymd').sum().reset_index()\n\ndf_lag = df_sales_amount_by_date\n\nfor i in range(1, 4):\n    df_lag = pd.concat([df_lag, df_sales_amount_by_date.shift(i)], axis=1)\n    columns = [f'lag_ymd_{i}', f'lag_amount_{i}']\n    df_lag.columns = list(df_lag.columns)[:-len(columns)] + columns\n\ndf_lag.dropna().astype(int).sort_values(['sales_ymd']).head(10)"}], "task": "code_generation", "index": 41, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_42", "instruction": "pandasを用いてレシート明細データ（df_receipt）と顧客データ（df_customer）を結合し、性別コード（gender_cd）と年代（ageから計算）ごとに売上金額（amount）を合計した売上サマリデータを作成せよ。性別コードは0が男性、1が女性、9が不明を表すものとする。\n\nただし、項目構成は年代、女性の売上金額、男性の売上金額、性別不明の売上金額の4項目とすること（縦に年代、横に性別のクロス集計）。また、年代は10歳ごとの階級とすること。", "instances": [{"input": "", "output": "import pandas as pd\n\n# コード例1\ndf_tmp = pd.merge(df_receipt, df_customer, how ='inner', on=\"customer_id\")\n\ndf_tmp['era'] = df_tmp['age'].apply(lambda x: math.floor(x / 10) * 10)\n\ndf_sales_summary = pd.pivot_table(\n                        df_tmp, index='era',\n                        columns='gender_cd', \n                        values='amount',\n                        aggfunc='sum'\n                    ).reset_index()\n\ndf_sales_summary.columns = ['era', 'male', 'female', 'unknown']\n\ndf_sales_summary\n\n\n# コード例2\ndf_tmp = pd.merge(df_receipt, df_customer, how ='inner', on=\"customer_id\")\n\ndf_tmp['era'] = np.floor(df_tmp['age'] / 10).astype(int)  * 10\n\ndf_sales_summary = pd.pivot_table(df_tmp, index='era', columns='gender_cd', \n                                  values='amount', aggfunc='sum').reset_index()\n\ndf_sales_summary.columns = ['era', 'male', 'female', 'unknown']\n\ndf_sales_summary"}], "task": "code_generation", "index": 42, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_43", "instruction": "売上サマリデータ（df_sales_summary）は性別の売上を横持ちさせたものである。pandasを用いてこのデータから性別を縦持ちさせ、年代、性別コード、売上金額の3項目に変換せよ。ただし、性別コードは男性を\"00\"、女性を\"01\"、不明を\"99\"とする。", "instances": [{"input": "", "output": "import pandas as pd\n\ndf_sales_summary.set_index('era'). \\\n    stack().reset_index().replace({'female':'01','male':'00','unknown':'99'}). \\\n    rename(columns={'level_1':'gender_cd', 0: 'amount'})"}], "task": "code_generation", "index": 43, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_44", "instruction": "pandasを用いて顧客データ（df_customer）の生年月日（birth_day）は日付型でデータを保有している。これをYYYYMMDD形式の文字列に変換し、顧客ID（customer_id）とともに10件表示せよ。", "instances": [{"input": "", "output": "import pandas as pd\n\n# 以下の書き方でYYYYMMDD形式の文字列に変換できる\n# pd.to_datetime(df_customer['birth_day']).dt.strftime('%Y%m%d')\n\npd.concat([df_customer['customer_id'],\n           pd.to_datetime(df_customer['birth_day']).dt.strftime('%Y%m%d')],\n          axis = 1).head(10)"}], "task": "code_generation", "index": 44, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_45", "instruction": "pandasを用いて顧客データ（df_customer）の申し込み日（application_date）はYYYYMMDD形式の文字列型でデータを保有している。これを日付型に変換し、顧客ID（customer_id）とともに10件表示せよ。", "instances": [{"input": "", "output": "import pandas as pd\n\npd.concat([df_customer['customer_id'],\n           pd.to_datetime(df_customer['application_date'])], axis=1).head(10)"}], "task": "code_generation", "index": 45, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_46", "instruction": "pandasを用いてレシート明細データ（df_receipt）の売上日（sales_ymd）はYYYYMMDD形式の数値型でデータを保有している。これを日付型に変換し、レシート番号（receipt_no）、レシートサブ番号（receipt_sub_no）とともに10件表示せよ。", "instances": [{"input": "", "output": "import pandas as pd\n\npd.concat([df_receipt[['receipt_no', 'receipt_sub_no']],\n           pd.to_datetime(df_receipt['sales_ymd'].astype('str'))],\n          axis=1).head(10)"}], "task": "code_generation", "index": 46, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_47", "instruction": "pandasを用いてレシート明細データ（df_receipt）の売上エポック秒（sales_epoch）は数値型のUNIX秒でデータを保有している。これを日付型に変換し、レシート番号(receipt_no)、レシートサブ番号（receipt_sub_no）とともに10件表示せよ。", "instances": [{"input": "", "output": "import pandas as pd\n\npd.concat([df_receipt[['receipt_no', 'receipt_sub_no']],\n    pd.to_datetime(df_receipt['sales_epoch'], unit='s').rename('sales_ymd')], \n    axis=1).head(10)"}], "task": "code_generation", "index": 47, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_48", "instruction": "pandasを用いてレシート明細データ（df_receipt）の売上エポック秒（sales_epoch）を日付型に変換し、「年」だけ取り出してレシート番号(receipt_no)、レシートサブ番号（receipt_sub_no）とともに10件表示せよ。", "instances": [{"input": "", "output": "import pandas as pd\n\npd.concat([df_receipt[['receipt_no', 'receipt_sub_no']],\n           pd.to_datetime(df_receipt['sales_epoch'], \n                          unit='s').dt.year.rename('sales_year')],\n          axis=1).head(10)"}], "task": "code_generation", "index": 48, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_49", "instruction": "pandasを用いてレシート明細データ（df_receipt）の売上エポック秒（sales_epoch）を日付型に変換し、「月」だけ取り出してレシート番号(receipt_no)、レシートサブ番号（receipt_sub_no）とともに10件表示せよ。なお、「月」は0埋め2桁で取り出すこと。", "instances": [{"input": "", "output": "import pandas as pd\n\n# dt.monthでも月を取得できるが、ここでは0埋め２桁で取り出すためstrftimeを利用している\ndf_datetime = pd.to_datetime(df_receipt['sales_epoch'], \n                             unit='s').rename('sales_month')\n\npd.concat([df_receipt[['receipt_no', 'receipt_sub_no']],\n           df_datetime.dt.strftime('%m')],axis=1).head(10)"}], "task": "code_generation", "index": 49, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_50", "instruction": "pandasを用いてレシート明細データ（df_receipt）の売上エポック秒を日付型に変換し、「日」だけ取り出してレシート番号(receipt_no)、レシートサブ番号（receipt_sub_no）とともに10件表示せよ。なお、「日」は0埋め2桁で取り出すこと。", "instances": [{"input": "", "output": "import pandas as pd\n\n# dt.dayでも日を取得できるが、ここでは0埋め２桁で取り出すためstrftimeを利用している\ndf_datetime = pd.to_datetime(df_receipt['sales_epoch'], \n                             unit='s').rename('sales_day')\n\npd.concat([df_receipt[['receipt_no', 'receipt_sub_no']],\n           df_datetime.dt.strftime('%d')], axis=1).head(10)"}], "task": "code_generation", "index": 50, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_51", "instruction": "pandasを用いてレシート明細データ（df_receipt）の売上金額（amount）を顧客ID（customer_id）ごとに合計の上、売上金額合計に対して2,000円以下を0、2,000円より大きい金額を1に二値化し、顧客ID、売上金額合計とともに10件表示せよ。ただし、顧客IDが\"Z\"から始まるのものは非会員を表すため、除外して計算すること。", "instances": [{"input": "", "output": "import pandas as pd\n\n# コード例1\ndf_sales_amount = df_receipt.query('not customer_id.str.startswith(\"Z\")', \n                                   engine='python')\n\ndf_sales_amount = df_sales_amount[['customer_id', 'amount']]. \\\n                                    groupby('customer_id').sum().reset_index()\n\ndf_sales_amount['sales_flg'] = df_sales_amount['amount']. \\\n                                    apply(lambda x: 1 if x > 2000 else 0)\n\ndf_sales_amount.head(10)\n\n\n# コード例2（np.whereの活用）\ndf_sales_amount = df_receipt.query('not customer_id.str.startswith(\"Z\")', \n                                   engine='python')\n\ndf_sales_amount = df_sales_amount[['customer_id', 'amount']]. \\\n                                    groupby('customer_id').sum().reset_index()\n\ndf_sales_amount['sales_flg'] = np.where(df_sales_amount['amount'] > 2000, 1, 0)\n\ndf_sales_amount.head(10)"}], "task": "code_generation", "index": 51, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_52", "instruction": "pandasを用いて顧客データ（df_customer）の郵便番号（postal_cd）に対し、東京（先頭3桁が100〜209のもの）を1、それ以外のものを0に二値化せよ。さらにレシート明細データ（df_receipt）と結合し、全期間において売上実績のある顧客数を、作成した二値ごとにカウントせよ。", "instances": [{"input": "", "output": "import pandas as pd\n\n# コード例1\ndf_tmp = df_customer[['customer_id', 'postal_cd']].copy()\n\ndf_tmp['postal_flg'] = df_tmp['postal_cd']. \\\n                        apply(lambda x: 1 if 100 <= int(x[0:3]) <= 209 else 0)\n\npd.merge(df_tmp, df_receipt, how='inner', on='customer_id'). \\\n    groupby('postal_flg').agg({'customer_id':'nunique'})\n\n\n# コード例2（np.where、betweenの活用）\ndf_tmp = df_customer[['customer_id', 'postal_cd']].copy()\n\ndf_tmp['postal_flg'] = np.where(df_tmp['postal_cd'].str[0:3].astype(int)\n                                .between(100, 209), 1, 0)\n\npd.merge(df_tmp, df_receipt, how='inner', on='customer_id'). \\\n    groupby('postal_flg').agg({'customer_id':'nunique'})"}], "task": "code_generation", "index": 52, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_53", "instruction": "pandasを用いて顧客データ（df_customer）の住所（address）は、埼玉県、千葉県、東京都、神奈川県のいずれかとなっている。都道府県毎にコード値を作成し、顧客ID、住所とともに10件表示せよ。値は埼玉県を11、千葉県を12、東京都を13、神奈川県を14とすること。", "instances": [{"input": "", "output": "import pandas as pd\n\n# コード例1（固定で切り出す）\ndf_customer_tmp = df_customer[['customer_id', 'address']].copy()\n\ndf_customer_tmp['prefecture_cd'] = \\\n            df_customer['address'].str[0:3].map({'埼玉県': '11',\n                                                '千葉県':'12', \n                                                '東京都':'13', \n                                                '神奈川':'14'})\n\ndf_customer_tmp.head(10)\n\n\n# コード例2（正規表現を使う）\ndf_customer_tmp = df_customer[['customer_id', 'address']].copy()\n\ndf_customer_tmp['prefecture_cd'] = \\\n            df_customer['address'].str.extract(r'(^.*?[都道府県])')[0].\\\n                                            map({'埼玉県': '11',\n                                                 '千葉県':'12', \n                                                 '東京都':'13', \n                                                 '神奈川県':'14'})\n\ndf_customer_tmp.head(10)"}], "task": "code_generation", "index": 53, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_54", "instruction": "pandasを用いてレシート明細（df_receipt）データの売上金額（amount）を顧客ID（customer_id）ごとに合計し、その合計金額の四分位点を求めよ。その上で、顧客ごとの売上金額合計に対して以下の基準でカテゴリ値を作成し、顧客ID、売上金額合計とともに10件表示せよ。カテゴリ値は順に1〜4とする。\n\n最小値以上第1四分位未満 ・・・ 1を付与\n第1四分位以上第2四分位未満 ・・・ 2を付与\n第2四分位以上第3四分位未満 ・・・ 3を付与\n第3四分位以上 ・・・ 4を付与", "instances": [{"input": "", "output": "import pandas as pd\n\n# コード例1\ndf_sales_amount = df_receipt[['customer_id', 'amount']]. \\\n                        groupby('customer_id').sum().reset_index()\n\npct25 = np.quantile(df_sales_amount['amount'], 0.25)\npct50 = np.quantile(df_sales_amount['amount'], 0.5)\npct75 = np.quantile(df_sales_amount['amount'], 0.75)\n\ndef pct_group(x):\n    if x < pct25:\n        return 1\n    elif pct25 <= x < pct50:\n        return 2\n    elif pct50 <= x < pct75:\n        return 3\n    elif pct75 <= x:\n        return 4\n\ndf_sales_amount['pct_group'] = df_sales_amount['amount'].apply(pct_group)\n\ndf_sales_amount.head(10)\n\n# 確認用コード\nprint('pct25:', pct25)\nprint('pct50:', pct50)\nprint('pct75:', pct75)\n\n\n# コード例2（cutを使った例、四分位範囲も参考までに追加表示）\ndf_temp = df_receipt[['customer_id', 'amount']]. \\\n                        groupby('customer_id').sum().reset_index()\n\npct25 = np.quantile(df_sales_amount['amount'], 0.25)\npct50 = np.quantile(df_sales_amount['amount'], 0.5)\npct75 = np.quantile(df_sales_amount['amount'], 0.75)\npct_max = df_sales_amount['amount'].max()\n\ndf_temp['quantile'] = pd.cut(df_sales_amount['amount'],[0.0, pct25, pct50, pct75,pct_max+0.1], right=False)\n\ndf_temp['pct_group'] = df_temp.groupby('quantile').ngroup() + 1\n\ndf_temp.head(10)\n\n\n# 参考コード（qcutを使った例、境界値の含む／含まないが逆になっており題意を満たさないが参考までに記載）\ndf_temp = df_receipt.groupby('customer_id')[['amount']].sum()\n\ndf_temp['quantile'], bins = \\\n    pd.qcut(df_receipt.groupby('customer_id')['amount'].sum(), 4, retbins=True) \n\ndf_temp['pct_group'] = df_temp.groupby('quantile').ngroup() + 1\n\ndf_temp.reset_index(inplace=True)\n\ndisplay(df_temp.head(10))\n\nprint('quantiles:', bins)"}], "task": "code_generation", "index": 54, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_55", "instruction": "pandasを用いて顧客データ（df_customer）の年齢（age）をもとに10歳刻みで年代を算出し、顧客ID（customer_id）、生年月日（birth_day）とともに10件表示せよ。ただし、60歳以上は全て60歳代とすること。年代を表すカテゴリ名は任意とする。", "instances": [{"input": "", "output": "import pandas as pd\n\n# コード例1\ndf_customer_era = df_customer[['customer_id', 'birth_day']].copy()\n\ndf_customer_era['era'] = df_customer['age']. \\\n                                apply(lambda x: min(math.floor(x / 10) * 10, 60))\n\ndf_customer_era.head(10)\n\n\n# コード例2（cutの例、カテゴリは範囲で出力）\ndf_customer_era = df_customer[['customer_id', 'birth_day']].copy()\n\ndf_customer_era['era'] = pd.cut(df_customer['age'], \n                                  bins=[0, 10, 20, 30, 40, 50, 60, np.inf], \n                                  right=False)\n\ndf_customer_era[['customer_id', 'birth_day', 'era']].head(10)"}], "task": "code_generation", "index": 55, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_56", "instruction": "pandasを用いて性別コード（gender_cd）により、新たに性別×年代の組み合わせを表すカテゴリデータを作成し、10件表示せよ。組み合わせを表すカテゴリの値は任意とする。", "instances": [{"input": "", "output": "import pandas as pd\n\n# 性別コード１桁と年代コード２桁を連結した性年代コードを生成する\n\ndf_customer_era = df_customer[['customer_id', 'birth_day']].copy()\n\ndf_customer_era['era'] = df_customer['age']. \\\n                                apply(lambda x: min(math.floor(x / 10) * 10, 60))\n\ndf_customer_era['gender_era'] = \\\n    df_customer['gender_cd'] + df_customer_era['era'].astype('str').str.zfill(2)\n\ndf_customer_era.head(10)"}], "task": "code_generation", "index": 56, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_57", "instruction": "pandasを用いて顧客データ（df_customer）の性別コード（gender_cd）をダミー変数化し、顧客ID（customer_id）とともに10件表示せよ。", "instances": [{"input": "", "output": "import pandas as pd\n\n# コード例1（すべてのコード値を項目化）\npd.get_dummies(df_customer[['customer_id', 'gender_cd']], \n               columns=['gender_cd']).head(10)\n\n\n# コード例2（項目を一つ削ったり区切り文字を変えたりできる）\npd.get_dummies(df_customer[['customer_id', 'gender_cd']], \n               columns=['gender_cd'], \n               drop_first=True, prefix='gen', prefix_sep='#').head(10)"}], "task": "code_generation", "index": 57, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_58", "instruction": "pandasを用いてレシート明細データ（df_receipt）の売上金額（amount）を顧客ID（customer_id）ごとに合計し、売上金額合計を平均0、標準偏差1に標準化して顧客ID、売上金額合計とともに10件表示せよ。標準化に使用する標準偏差は、分散の平方根、もしくは不偏分散の平方根のどちらでも良いものとする。ただし、顧客IDが\"Z\"から始まるのものは非会員を表すため、除外して計算すること。", "instances": [{"input": "", "output": "import pandas as pd\n\n# skleanのpreprocessing.scaleを利用するため、データの標準偏差で計算されている\ndf_sales_amount = df_receipt.query('not customer_id.str.startswith(\"Z\")', \n                                   engine='python'). \\\n                                   groupby('customer_id'). \\\n                                   agg({'amount':'sum'}).reset_index()\n\ndf_sales_amount['std_amount'] = preprocessing.scale(df_sales_amount['amount'])\n\ndf_sales_amount.head(10)\n\n\n# コード例2（fitを行うことで、別のデータでも同じ平均・標準偏差で標準化を行える）\ndf_sales_amount = df_receipt.query('not customer_id.str.startswith(\"Z\")', \n                                   engine='python'). \\\n                                   groupby('customer_id'). \\\n                                   agg({'amount':'sum'}).reset_index()\n\nscaler = preprocessing.StandardScaler()\n\nscaler.fit(df_sales_amount[['amount']])\n\ndf_sales_amount['std_amount'] = scaler.transform(df_sales_amount[['amount']])\n\ndf_sales_amount.head(10)\n\n\nTIPS:\nquery()の引数engineで'python'か'numexpr'かを選択でき、デフォルトはインストールされていればnumexprが、無ければpythonが使われます。さらに、文字列メソッドはengine='python'でないとquery()内で使えません。"}], "task": "code_generation", "index": 58, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_59", "instruction": "pandasを用いてレシート明細データ（df_receipt）の売上金額（amount）を顧客ID（customer_id）ごとに合計し、売上金額合計を最小値0、最大値1に正規化して顧客ID、売上金額合計とともに10件表示せよ。ただし、顧客IDが\"Z\"から始まるのものは非会員を表すため、除外して計算すること。", "instances": [{"input": "", "output": "import pandas as pd\n\n# コード例1\ndf_sales_amount = df_receipt.query('not customer_id.str.startswith(\"Z\")', \n                                   engine='python'). \\\n                                   groupby('customer_id'). \\\n                                   agg({'amount':'sum'}).reset_index()\n\ndf_sales_amount['scale_amount'] = \\\n                        preprocessing.minmax_scale(df_sales_amount['amount'])\n\ndf_sales_amount.head(10)\n\n\n# コード例2（fitを行うことで、別のデータでも同じ最小値・最大値で標準化を行える）\ndf_sales_amount = df_receipt.query('not customer_id.str.startswith(\"Z\")', \n                                   engine='python'). \\\n                                   groupby('customer_id'). \\\n                                   agg({'amount':'sum'}).reset_index()\n\nscaler = preprocessing.MinMaxScaler()\n\nscaler.fit(df_sales_amount[['amount']])\n\ndf_sales_amount['scale_amount'] = scaler.transform(df_sales_amount[['amount']])\n\ndf_sales_amount.head(10)"}], "task": "code_generation", "index": 59, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_60", "instruction": "pandasを用いてレシート明細データ（df_receipt）の売上金額（amount）を顧客ID（customer_id）ごとに合計し、売上金額合計を常用対数化（底10）して顧客ID、売上金額合計とともに10件表示せよ。ただし、顧客IDが\"Z\"から始まるのものは非会員を表すため、除外して計算すること。", "instances": [{"input": "", "output": "import pandas as pd\n\ndf_sales_amount = df_receipt.query('not customer_id.str.startswith(\"Z\")', \n                                   engine='python'). \\\n                                   groupby('customer_id'). \\\n                                   agg({'amount':'sum'}).reset_index()\n\ndf_sales_amount['log_amount'] = np.log10(df_sales_amount['amount'] + 0.5)\n\ndf_sales_amount.head(10)"}], "task": "code_generation", "index": 60, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_61", "instruction": "pandasを用いてレシート明細データ（df_receipt）の売上金額（amount）を顧客ID（customer_id）ごとに合計し、売上金額合計を自然対数化（底e）して顧客ID、売上金額合計とともに10件表示せよ。ただし、顧客IDが\"Z\"から始まるのものは非会員を表すため、除外して計算すること。", "instances": [{"input": "", "output": "import pandas as pd\n\ndf_sales_amount = df_receipt.query('not customer_id.str.startswith(\"Z\")', \n                                   engine='python'). \\\n                                   groupby('customer_id'). \\\n                                   agg({'amount':'sum'}).reset_index()\n\ndf_sales_amount['log_amount'] = np.log(df_sales_amount['amount'] + 0.5)\n\ndf_sales_amount.head(10)"}], "task": "code_generation", "index": 61, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_62", "instruction": "pandasを用いて商品データ（df_product）の単価（unit_price）と原価（unit_cost）から各商品の利益額を算出し、結果を10件表示せよ。", "instances": [{"input": "", "output": "import pandas as pd\n\ndf_tmp = df_product.copy()\n\ndf_tmp['unit_profit'] = df_tmp['unit_price'] - df_tmp['unit_cost']\n\ndf_tmp.head(10)"}], "task": "code_generation", "index": 62, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_63", "instruction": "pandasを用いて商品データ（df_product）の単価（unit_price）と原価（unit_cost）から、各商品の利益率の全体平均を算出せよ。ただし、単価と原価には欠損が生じていることに注意せよ。", "instances": [{"input": "", "output": "import pandas as pd\n\ndf_tmp = df_product.copy()\n\ndf_tmp['unit_profit_rate'] = \\\n            (df_tmp['unit_price'] - df_tmp['unit_cost']) / df_tmp['unit_price']\n\ndf_tmp['unit_profit_rate'].mean(skipna=True)"}], "task": "code_generation", "index": 63, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_64", "instruction": "pandasを用いて商品データ（df_product）の各商品について、利益率が30%となる新たな単価を求めよ。ただし、1円未満は切り捨てること。そして結果を10件表示させ、利益率がおよそ30％付近であることを確認せよ。ただし、単価（unit_price）と原価（unit_cost）には欠損が生じていることに注意せよ。", "instances": [{"input": "", "output": "import pandas as pd\n\ndf_tmp = df_product[['product_cd', 'unit_price', 'unit_cost']].copy()\n\ndf_tmp['new_price'] = np.floor(df_tmp['unit_cost'] / 0.7)\n\ndf_tmp['new_profit_rate'] = \\\n            (df_tmp['new_price'] - df_tmp['unit_cost']) / df_tmp['new_price']\n\ndf_tmp.head(10)"}], "task": "code_generation", "index": 64, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_65", "instruction": "pandasを用いて商品データ（df_product）の各商品について、利益率が30%となる新たな単価を求めよ。今回は、1円未満を丸めること（四捨五入または偶数への丸めで良い）。そして結果を10件表示させ、利益率がおよそ30％付近であることを確認せよ。ただし、単価（unit_price）と原価（unit_cost）には欠損が生じていることに注意せよ。", "instances": [{"input": "", "output": "import pandas as pd\n\ndf_tmp = df_product[['product_cd', 'unit_price', 'unit_cost']].copy()\n\ndf_tmp['new_price'] = np.round(df_tmp['unit_cost'] / 0.7)\n\ndf_tmp['new_profit_rate'] = \\\n            (df_tmp['new_price'] - df_tmp['unit_cost']) / df_tmp['new_price']\n\ndf_tmp.head(10)"}], "task": "code_generation", "index": 65, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_66", "instruction": "pandasを用いて商品データ（df_product）の各商品について、利益率が30%となる新たな単価を求めよ。今回は、1円未満を切り上げること。そして結果を10件表示させ、利益率がおよそ30％付近であることを確認せよ。ただし、単価（unit_price）と原価（unit_cost）には欠損が生じていることに注意せよ。", "instances": [{"input": "", "output": "import pandas as pd\n\ndf_tmp = df_product[['product_cd', 'unit_price', 'unit_cost']].copy()\n\ndf_tmp['new_price'] = np.ceil(df_tmp['unit_cost'] / 0.7)\n\ndf_tmp['new_profit_rate'] = \\\n            (df_tmp['new_price'] - df_tmp['unit_cost']) / df_tmp['new_price']\n\ndf_tmp.head(10)"}], "task": "code_generation", "index": 66, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_67", "instruction": "pandasを用いて商品データ（df_product）の各商品について、消費税率10％の税込み金額を求めよ。1円未満の端数は切り捨てとし、結果を10件表示せよ。ただし、単価（unit_price）には欠損が生じていることに注意せよ。", "instances": [{"input": "", "output": "import pandas as pd\n\ndf_tmp = df_tmp = df_product[['product_cd', 'unit_price']].copy()\n\ndf_tmp['tax_price'] = np.floor(df_tmp['unit_price'] * 1.1)\n\ndf_tmp.head(10)"}], "task": "code_generation", "index": 67, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_68", "instruction": "pandasを用いてレシート明細データ（df_receipt）と商品データ（df_product）を結合し、顧客毎に全商品の売上金額合計と、カテゴリ大区分コード（category_major_cd）が\"07\"（瓶詰缶詰）の売上金額合計を計算の上、両者の比率を求めよ。抽出対象はカテゴリ大区分コード\"07\"（瓶詰缶詰）の売上実績がある顧客のみとし、結果を10件表示せよ。", "instances": [{"input": "", "output": "import pandas as pd\n\n# コード例1\ndf_tmp_1 = df_receipt.groupby('customer_id').agg({'amount':'sum'}). \\\n                reset_index().rename(columns={'amount':'sum_all'})\n\ndf_tmp_2 = pd.merge(df_receipt, df_product.query('category_major_cd == \"07\"'), \n                how='inner', on='product_cd').groupby('customer_id').\\\n                agg({'amount':'sum'}).reset_index().\\\n                rename(columns={'amount':'sum_07'})\n\ndf_tmp_3 = pd.merge(df_tmp_1, df_tmp_2, how='inner', on='customer_id')\n\ndf_tmp_3['sales_rate'] = df_tmp_3['sum_07'] / df_tmp_3['sum_all']\n\ndf_tmp_3.head(10)\n\n\n# コード例2（参考、unstackと横方向のsumを使った例）\ndf_temp = df_receipt.merge(df_product, how='left', on='product_cd'). \\\n        groupby(['customer_id', 'category_major_cd'])['amount'].sum().unstack()\n\ndf_temp = df_temp[df_temp['07'] > 0]\n\ndf_temp['sum_all'] = df_temp.sum(axis=1)\n\ndf_temp['sales_rate'] = df_temp['07'] / df_temp['sum_all']\n\n# 以降はデータフレームの整形と表示のための処理\ndf_temp.columns.name = ''\n\ndf_temp = df_temp.reset_index()\n\ndf_temp.head(10)"}], "task": "code_generation", "index": 68, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_69", "instruction": "pandasを用いてレシート明細データ（df_receipt）の売上日（sales_ymd）に対し、顧客データ（df_customer）の会員申込日（application_date）からの経過日数を計算し、顧客ID（customer_id）、売上日、会員申込日とともに10件表示せよ（sales_ymdは数値、application_dateは文字列でデータを保持している点に注意）。", "instances": [{"input": "", "output": "import pandas as pd\n\ndf_tmp = df_receipt[['customer_id', 'sales_ymd']].drop_duplicates()\n\ndf_tmp = pd.merge(df_tmp, df_customer[['customer_id', 'application_date']],\n                  how='inner', on='customer_id')\n\ndf_tmp['sales_ymd'] = pd.to_datetime(df_tmp['sales_ymd'].astype('str'))\n\ndf_tmp['application_date'] = pd.to_datetime(df_tmp['application_date'])\n\ndf_tmp['elapsed_days'] = df_tmp['sales_ymd'] - df_tmp['application_date']\n\ndf_tmp['elapsed_days'] = df_tmp['elapsed_days'].dt.days\n\ndf_tmp.head(10)"}], "task": "code_generation", "index": 69, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_70", "instruction": "pandasを用いてレシート明細データ（df_receipt）の売上日（sales_ymd）に対し、顧客データ（df_customer）の会員申込日（application_date）からの経過月数を計算し、顧客ID（customer_id）、売上日、会員申込日とともに10件表示せよ（sales_ymdは数値、application_dateは文字列でデータを保持している点に注意）。1ヶ月未満は切り捨てること。", "instances": [{"input": "", "output": "import pandas as pd\n\ndf_tmp = df_receipt[['customer_id', 'sales_ymd']].drop_duplicates()\n\ndf_tmp = pd.merge(df_tmp, df_customer[['customer_id', 'application_date']],\n                  how='inner', on='customer_id')\n\ndf_tmp['sales_ymd'] = pd.to_datetime(df_tmp['sales_ymd'].astype('str'))\n\ndf_tmp['application_date'] = pd.to_datetime(df_tmp['application_date'])\n\ndf_tmp['elapsed_months'] = df_tmp[['sales_ymd', 'application_date']]. \\\n    apply(lambda x: relativedelta(x[0], x[1]).years * 12 + \\\n                    relativedelta(x[0], x[1]).months, axis=1)\n\ndf_tmp.head(10)"}], "task": "code_generation", "index": 70, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_71", "instruction": "pandasを用いてレシート明細データ（df_receipt）の売上日（df_customer）に対し、顧客データ（df_customer）の会員申込日（application_date）からの経過年数を計算し、顧客ID（customer_id）、売上日、会員申込日とともに10件表示せよ（sales_ymdは数値、application_dateは文字列でデータを保持している点に注意）。1年未満は切り捨てること。", "instances": [{"input": "", "output": "import pandas as pd\n\ndf_tmp = df_receipt[['customer_id', 'sales_ymd']].drop_duplicates()\n\ndf_tmp = pd.merge(df_tmp, df_customer[['customer_id', 'application_date']],\n                  how='inner', on='customer_id')\n\ndf_tmp['sales_ymd'] = pd.to_datetime(df_tmp['sales_ymd'].astype('str'))\n\ndf_tmp['application_date'] = pd.to_datetime(df_tmp['application_date'])\n\ndf_tmp['elapsed_years'] = df_tmp[['sales_ymd', 'application_date']]. \\\n                    apply(lambda x: relativedelta(x[0], x[1]).years, axis=1)\n\ndf_tmp.head(10)"}], "task": "code_generation", "index": 71, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_72", "instruction": "pandasを用いてレシート明細データ（df_receipt）の売上日（sales_ymd）に対し、顧客データ（df_customer）の会員申込日（application_date）からのエポック秒による経過時間を計算し、顧客ID（customer_id）、売上日、会員申込日とともに10件表示せよ（なお、sales_ymdは数値、application_dateは文字列でデータを保持している点に注意）。なお、時間情報は保有していないため各日付は0時0分0秒を表すものとする。", "instances": [{"input": "", "output": "import pandas as pd\n\ndf_tmp = df_receipt[['customer_id', 'sales_ymd']].drop_duplicates()\n\ndf_tmp = pd.merge(df_tmp, df_customer[['customer_id', 'application_date']],\n                  how='inner', on='customer_id')\n\ndf_tmp['sales_ymd'] = pd.to_datetime(df_tmp['sales_ymd'].astype('str'))\n\ndf_tmp['application_date'] = pd.to_datetime(df_tmp['application_date'])\n\ndf_tmp['elapsed_epoch'] = df_tmp['sales_ymd'].view(np.int64) - \\\n                                    df_tmp['application_date'].view(np.int64)\n\ndf_tmp['elapsed_epoch'] = df_tmp['elapsed_epoch'] / 10**9\n\ndf_tmp.head(10)"}], "task": "code_generation", "index": 72, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_73", "instruction": "pandasを用いてレシート明細データ（df_receipt）の売上日（sales_ymd）に対し、当該週の月曜日からの経過日数を計算し、売上日、直前の月曜日付とともに10件表示せよ（sales_ymdは数値でデータを保持している点に注意）。", "instances": [{"input": "", "output": "import pandas as pd\n\ndf_tmp = df_receipt[['sales_ymd']].copy()\n\ndf_tmp['sales_ymd'] = pd.to_datetime(df_tmp['sales_ymd'].astype('str'))\n\ndf_tmp['elapsed_days']  = df_tmp['sales_ymd'].apply(lambda x:x.weekday())\n\ndf_tmp['monday'] = \\\n    df_tmp['sales_ymd'].apply(lambda x: x - relativedelta(days=x.weekday()))\n\ndf_tmp.head(10)"}], "task": "code_generation", "index": 73, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_74", "instruction": "pandasを用いて顧客データ（df_customer）からランダムに1%のデータを抽出し、先頭から10件表示せよ。", "instances": [{"input": "", "output": "import pandas as pd\n\ndf_customer.sample(frac=0.01).head(10)"}], "task": "code_generation", "index": 74, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_75", "instruction": "pandasを用いて顧客データ（df_customer）から性別コード（gender_cd）の割合に基づきランダムに10%のデータを層化抽出し、性別コードごとに件数を集計せよ。", "instances": [{"input": "", "output": "import pandas as pd\n\n# sklearn.model_selection.train_test_splitを使用した例\n_, df_tmp = train_test_split(df_customer, test_size=0.1, \n                             stratify=df_customer['gender_cd'])\n\ndf_tmp.groupby('gender_cd').agg({'customer_id' : 'count'})"}], "task": "code_generation", "index": 75, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_76", "instruction": "pandasを用いてレシート明細データ（df_receipt）の売上金額を顧客単位に合計し、合計した売上金額の外れ値を抽出せよ。なお、外れ値は売上金額合計を対数化したうえで平均と標準偏差を計算し、その平均から3σを超えて離れたものとする（自然対数と常用対数のどちらでも可）。結果は10件表示せよ。", "instances": [{"input": "", "output": "import pandas as pd\n\ndf_sales_amount = df_receipt.groupby('customer_id'). \\\n                                agg({'amount':'sum'}).reset_index()\n\ndf_sales_amount['log_sum_amount'] = np.log(df_sales_amount['amount'] + 0.5)\n\ndf_sales_amount['log_sum_amount_ss'] = preprocessing.scale(df_sales_amount['log_sum_amount'])\n\ndf_sales_amount.query('abs(log_sum_amount_ss) > 3').head(10)"}], "task": "code_generation", "index": 76, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_77", "instruction": "pandasを用いてレシート明細データ（df_receipt）の売上金額（amount）を顧客単位に合計し、合計した売上金額の外れ値を抽出せよ。ただし、顧客IDが\"Z\"から始まるのものは非会員を表すため、除外して計算すること。なお、ここでは外れ値を第1四分位と第3四分位の差であるIQRを用いて、「第1四分位数-1.5×IQR」を下回るもの、または「第3四分位数+1.5×IQR」を超えるものとする。結果は10件表示せよ。", "instances": [{"input": "", "output": "import pandas as pd\n\ndf_sales_amount = df_receipt.query('not customer_id.str.startswith(\"Z\")', \n                                   engine='python'). \\\n                                   groupby('customer_id'). \\\n                                   agg({'amount':'sum'}).reset_index()\n\npct25 = np.percentile(df_sales_amount['amount'], q=25)\npct75 = np.percentile(df_sales_amount['amount'], q=75)\n\niqr = pct75 - pct25\namount_low = pct25 - (iqr * 1.5)\namount_hight = pct75 + (iqr * 1.5)\n\ndf_sales_amount.query('amount < @amount_low or @amount_hight < amount').head(10)"}], "task": "code_generation", "index": 77, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_78", "instruction": "pandasを用いて商品データ（df_product）の各項目に対し、欠損数を確認せよ。", "instances": [{"input": "", "output": "import pandas as pd\n\ndf_product.isnull().sum()"}], "task": "code_generation", "index": 78, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_79", "instruction": "pandasを用いて商品データ（df_product）のいずれかの項目に欠損が発生しているレコードを全て削除した新たな商品データを作成せよ。なお、削除前後の件数を表示させ、079で確認した件数だけ減少していることも確認すること。", "instances": [{"input": "", "output": "import pandas as pd\n\ndf_product_1 = df_product.copy()\n\ndf_product_1.dropna(inplace=True)\n\nprint('削除前:', len(df_product))\nprint('削除後:', len(df_product_1))"}], "task": "code_generation", "index": 79, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_80", "instruction": "pandasを用いて単価（unit_price）と原価（unit_cost）の欠損値について、それぞれの平均値で補完した新たな商品データを作成せよ。なお、平均値については1円未満を丸めること（四捨五入または偶数への丸めで良い）。補完実施後、各項目について欠損が生じていないことも確認すること。", "instances": [{"input": "", "output": "import pandas as pd\n\n# コード例1（Pandasのfillna）\ndf_product_2 = df_product.fillna({\n    'unit_price':np.round(np.nanmean(df_product['unit_price'])), \n    'unit_cost':np.round(np.nanmean(df_product['unit_cost']))})\n\ndf_product_2.isnull().sum()\n\n\n# コード例2（scikit-learnのSimpleImputer）\nimp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n\nimp_values = imp_mean.fit_transform(df_product[['unit_price', 'unit_cost']])\n\ndf_product_2 = df_product.copy()\n\ndf_product_2[['unit_price', 'unit_cost']] = imp_values.round()\n\ndf_product_2.isnull().sum()"}], "task": "code_generation", "index": 80, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_81", "instruction": "pandasを用いて単価（unit_price）と原価（unit_cost）の欠損値について、それぞれの中央値で補完した新たな商品データを作成せよ。なお、中央値については1円未満を丸めること（四捨五入または偶数への丸めで良い）。補完実施後、各項目について欠損が生じていないことも確認すること。", "instances": [{"input": "", "output": "import pandas as pd\n\n# コード例1（Pandasのfillna）\ndf_product_3 = df_product.fillna({\n    'unit_price':np.round(np.nanmedian(df_product['unit_price'])), \n    'unit_cost':np.round(np.nanmedian(df_product['unit_cost']))})\n\ndf_product_3.isnull().sum()\n\n\n# コード例2（scikit-learnのSimpleImputer）\nimp_mean = SimpleImputer(missing_values=np.nan, strategy='median')\n\nimp_values = imp_mean.fit_transform(df_product[['unit_price', 'unit_cost']])\n\ndf_product_3 = df_product.copy()\n\ndf_product_3[['unit_price', 'unit_cost']] = imp_values.round()\n\ndf_product_3.isnull().sum()"}], "task": "code_generation", "index": 81, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_82", "instruction": "pandasを用いて単価（unit_price）と原価（unit_cost）の欠損値について、各商品のカテゴリ小区分コード（category_small_cd）ごとに算出した中央値で補完した新たな商品データを作成せよ。なお、中央値については1円未満を丸めること（四捨五入または偶数への丸めで良い）。補完実施後、各項目について欠損が生じていないことも確認すること。", "instances": [{"input": "", "output": "import pandas as pd\n\n# コード例1\ndf_tmp = (df_product.groupby('category_small_cd')\n          .agg(median_price=('unit_price', 'median'), \n               median_cost=('unit_cost', 'median')).reset_index())\n\ndf_product_4 = pd.merge(df_product, df_tmp, how='inner', on='category_small_cd')\n\ndf_product_4['unit_price'] = df_product_4[['unit_price', 'median_price']]. \\\n            apply(lambda x: np.round(x[1]) if np.isnan(x[0]) else x[0], axis=1)\n\ndf_product_4['unit_cost'] = df_product_4[['unit_cost', 'median_cost']]. \\\n            apply(lambda x: np.round(x[1]) if np.isnan(x[0]) else x[0], axis=1)\n\ndf_product_4.isnull().sum()\n\n\n# コード例2（maskの活用）\ndf_tmp = (df_product.groupby('category_small_cd')\n          .agg(median_price=('unit_price', 'median'), \n               median_cost=('unit_cost', 'median')).reset_index())\n\ndf_product_4 = df_product.merge(df_tmp, how='inner', on='category_small_cd')\n\ndf_product_4['unit_price'] = (df_product_4['unit_price']\n                              .mask(df_product_4['unit_price'].isnull(), \n                                    df_product_4['median_price'].round()))\n\ndf_product_4['unit_cost'] = (df_product_4['unit_cost']\n                              .mask(df_product_4['unit_cost'].isnull(), \n                                    df_product_4['median_cost'].round()))\n\ndf_product_4.isnull().sum()\n\n\n# コード例3（fillna、transformの活用）\ndf_product_4 = df_product.copy()\n\nfor x in ['unit_price', 'unit_cost']: \n    df_product_4[x] = (df_product_4[x]\n                       .fillna(df_product_4.groupby('category_small_cd')[x]\n                               .transform('median')\n                               .round()))\n\ndf_product_4.isnull().sum()"}], "task": "code_generation", "index": 82, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_83", "instruction": "pandasを用いて顧客データ（df_customer）の全顧客に対して全期間の売上金額に占める2019年売上金額の割合を計算し、新たなデータを作成せよ。ただし、売上実績がない場合は0として扱うこと。そして計算した割合が0超のものを抽出し、結果を10件表示せよ。また、作成したデータに欠損が存在しないことを確認せよ。", "instances": [{"input": "", "output": "import pandas as pd\n\ndf_receipt_2019 = df_receipt.query('20190101 <= sales_ymd <= 20191231') \\\n                            .groupby('customer_id') \\\n                            .agg(amount_2019=('amount', 'sum')) \\\n                            .reset_index()\n\ndf_receipt_all = df_receipt.groupby('customer_id')\\\n                            .agg(amount_all=('amount', 'sum')) \\\n                            .reset_index()\n\ndf_sales_rate = df_customer[['customer_id']] \\\n                    .merge(df_receipt_2019, how='left', on='customer_id') \\\n                    .merge(df_receipt_all, how='left', on='customer_id')\n\ndf_sales_rate['amount_2019'] = df_sales_rate['amount_2019'].fillna(0)\ndf_sales_rate['amount_all'] = df_sales_rate['amount_all'].fillna(0)\n\ndf_sales_rate['amount_rate'] = \\\n                df_sales_rate[['amount_2019','amount_all']] \\\n                    .apply(lambda x: 0 if x[0] == 0 else x[0] / x[1], axis=1)\n\ndf_sales_rate['amount_rate'] = df_sales_rate['amount_rate'].fillna(0)\n\ndf_sales_rate.query('amount_rate > 0').head(10)"}], "task": "code_generation", "index": 83, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_84", "instruction": "pandasを用いて顧客データ（df_customer）の全顧客に対し、郵便番号（postal_cd）を用いてジオコードデータ（df_geocode）を紐付け、新たな顧客データを作成せよ。ただし、1つの郵便番号（postal_cd）に複数の経度（longitude）、緯度（latitude）情報が紐づく場合は、経度（longitude）、緯度（latitude）の平均値を算出して使用すること。また、作成結果を確認するために結果を10件表示せよ。", "instances": [{"input": "", "output": "import pandas as pd\n\ndf_geocode_1 = df_geocode.groupby('postal_cd') \\\n                .agg(m_longitude=('longitude', 'mean'), \n                     m_latitude=('latitude', 'mean')).reset_index()\n\ndf_customer_1 = pd.merge(df_customer, df_geocode_1, \n                         how='inner', on='postal_cd')\n\ndf_customer_1.head(10)"}], "task": "code_generation", "index": 84, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_85", "instruction": "pandasを用いて緯度経度つき顧客データに対し、会員申込店舗コード（application_store_cd）をキーに店舗データ（df_store）と結合せよ。そして申込み店舗の緯度（latitude）・経度情報（longitude)と顧客住所（address）の緯度・経度を用いて申込み店舗と顧客住所の距離（単位：km）を求め、顧客ID（customer_id）、顧客住所（address）、店舗住所（address）とともに表示せよ。計算式は以下の簡易式で良いものとするが、その他精度の高い方式を利用したライブラリを利用してもかまわない。結果は10件表示せよ。\n\n緯度（ラジアン）：ϕ経度（ラジアン）：λ距離L=6371∗arccos(sinϕ1∗sinϕ2cosϕ1∗cosϕ2∗cos(λ1−λ2))", "instances": [{"input": "", "output": "import pandas as pd\n\n# コード例1\ndef calc_distance(x1, y1, x2, y2):\n    distance = 6371 * math.acos(math.sin(math.radians(x1)) \n                    * math.sin(math.radians(x2)) \n                    + math.cos(math.radians(x1)) \n                    * math.cos(math.radians(x2)) \n                    * math.cos(math.radians(y1) - math.radians(y2)))\n    return distance\n\ndf_tmp = pd.merge(df_customer_1, df_store, \n                  how='inner', \n                  left_on='application_store_cd', \n                  right_on='store_cd') \\\n        .rename(columns={'address_x':'customer_address', \n                         'address_y':'store_address'})\n\ndf_tmp['distance'] =   df_tmp[['m_latitude', \n                               'm_longitude',\n                               'latitude', \n                               'longitude']] \\\n                        .apply(lambda x: calc_distance(x[0], x[1], x[2], x[3]), \n                               axis=1)\n\ndf_tmp[['customer_id', 'customer_address', \n        'store_address', 'distance']].head(10)\n\n\n# コード例2\ndef calc_distance_numpy(x1, y1, x2, y2):\n    x1_r = np.radians(x1)\n    x2_r = np.radians(x2)\n    y1_r = np.radians(y1)\n    y2_r = np.radians(y2)\n    return 6371 * np.arccos(np.sin(x1_r) * np.sin(x2_r) \n                            + np.cos(x1_r) * np.cos(x2_r) \n                            * np.cos(y1_r - y2_r))\n\ndf_tmp = df_customer_1.merge(df_store, \n                             how='inner', \n                             left_on='application_store_cd', \n                             right_on='store_cd') \\\n                      .rename(columns={'address_x':'customer_address', \n                                       'address_y':'store_address'})\n\ndf_tmp['distance'] =  calc_distance_numpy(df_tmp['m_latitude'], \n                                          df_tmp['m_longitude'],\n                                          df_tmp['latitude'], \n                                          df_tmp['longitude'])\n\ndf_tmp[['customer_id', 'customer_address', \n        'store_address', 'distance']].head(10)"}], "task": "code_generation", "index": 85, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_86", "instruction": "pandasを用いて顧客データ（df_customer）では、異なる店舗での申込みなどにより同一顧客が複数登録されている。名前（customer_name）と郵便番号（postal_cd）が同じ顧客は同一顧客とみなして1顧客1レコードとなるように名寄せした名寄顧客データを作成し、顧客データの件数、名寄顧客データの件数、重複数を算出せよ。ただし、同一顧客に対しては売上金額合計が最も高いものを残し、売上金額合計が同一もしくは売上実績がない顧客については顧客ID（customer_id）の番号が小さいものを残すこととする。", "instances": [{"input": "", "output": "import pandas as pd\n\ndf_receipt_tmp = df_receipt.groupby('customer_id') \\\n                    .agg(sum_amount=('amount','sum')).reset_index()\n\ndf_customer_u = pd.merge(df_customer, df_receipt_tmp, \n                         how='left', \n                         on='customer_id')\n\ndf_customer_u['sum_amount'] = df_customer_u['sum_amount'].fillna(0)\n\ndf_customer_u = df_customer_u.sort_values(['sum_amount', 'customer_id'], \n                                          ascending=[False, True])\n\ndf_customer_u.drop_duplicates(subset=['customer_name', 'postal_cd'], \n                              keep='first', inplace=True)\n\nprint('df_customer_cnt:', len(df_customer),\n      'df_customer_u_cnt:', len(df_customer_u),\n      'diff:', len(df_customer) - len(df_customer_u))"}], "task": "code_generation", "index": 86, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_87", "instruction": "pandasを用いて作成したデータを元に、顧客データに統合名寄IDを付与したデータを作成せよ。ただし、統合名寄IDは以下の仕様で付与するものとする。\n\n重複していない顧客：顧客ID（customer_id）を設定\n重複している顧客：前設問で抽出したレコードの顧客IDを設定\n顧客IDのユニーク件数と、統合名寄IDのユニーク件数の差も確認すること。", "instances": [{"input": "", "output": "import pandas as pd\n\ndf_customer_n = pd.merge(df_customer, \n                         df_customer_u[['customer_name', \n                                        'postal_cd', 'customer_id']],\n                         how='inner', on =['customer_name', 'postal_cd'])\n\ndf_customer_n.rename(columns={'customer_id_x':'customer_id', \n                              'customer_id_y':'integration_id'}, inplace=True)\n\nprint('ID数の差', len(df_customer_n['customer_id'].unique()) \n                         - len(df_customer_n['integration_id'].unique()))"}], "task": "code_generation", "index": 87, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_88", "instruction": "pandasを用いて売上実績がある顧客を、予測モデル構築のため学習用データとテスト用データに分割したい。それぞれ8:2の割合でランダムにデータを分割せよ。", "instances": [{"input": "", "output": "import pandas as pd\n\ndf_sales_customer = df_receipt.groupby('customer_id') \\\n                                    .agg({'amount':sum}).reset_index()\n\ndf_sales_customer = df_sales_customer.query('amount > 0')\n\ndf_tmp = pd.merge(df_customer, df_sales_customer['customer_id'], \n                  how='inner', on='customer_id')\n\ndf_train, df_test = train_test_split(df_tmp, test_size=0.2, random_state=71)\n\nprint('学習データ割合: ', len(df_train) / len(df_tmp))\nprint('テストデータ割合: ', len(df_test) / len(df_tmp))"}], "task": "code_generation", "index": 88, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_89", "instruction": "pandasを用いてレシート明細データ（df_receipt）は2017年1月1日〜2019年10月31日までのデータを有している。売上金額（amount）を月次で集計し、学習用に12ヶ月、テスト用に6ヶ月の時系列モデル構築用データを3セット作成せよ。", "instances": [{"input": "", "output": "import pandas as pd\n\n# コード例1（自作関数）\ndf_ts_amount = df_receipt[['sales_ymd', 'amount']].copy()\n\ndf_ts_amount['sales_ym'] = df_ts_amount['sales_ymd'].astype('str').str[0:6]\n\ndf_ts_amount = df_ts_amount.groupby('sales_ym') \\\n                                        .agg({'amount':'sum'}).reset_index()\n\n# 長期間データに対する多数のデータセットもループなどで処理できるように関数化\ndef split_data(df, train_size, test_size, slide_window, start_point):\n    train_start = start_point * slide_window\n    test_start = train_start + train_size\n    return df[train_start:test_start], df[test_start:test_start + test_size]\n\ndf_train_1, df_test_1 = split_data(df_ts_amount, train_size=12, \n                                   test_size=6, slide_window=6, start_point=0)\n\ndf_train_2, df_test_2 = split_data(df_ts_amount, train_size=12, \n                                   test_size=6, slide_window=6, start_point=1)\n\ndf_train_3, df_test_3 = split_data(df_ts_amount, train_size=12, \n                                   test_size=6, slide_window=6, start_point=2)\n\n# df_train_2とdf_train_3の表示は割愛\ndf_train_1\n\n# df_test_2とdf_test_３の表示は割愛\ndf_test_1\n\n\n# コード例2（scikit-learnのTimeSeriesSplit）\ntscv = TimeSeriesSplit(gap=0, max_train_size=12, n_splits=3, test_size=6)\n\n# TimeSeriesSplitは最新のデータが使われるように分割されるが、\n# SQL、Rの解答例と同じとなるようにデータ期間を調整\n# できる限り最新データを使うようにするなら不要\ndf_ts_amount = df_ts_amount.query('sales_ym <= \"201906\"')\n\nseries_list = []\nfor train_index, test_index in tscv.split(df_ts_amount):\n    series_list.append((df_ts_amount.loc[train_index], \n                        df_ts_amount.loc[test_index]))\n    \ndf_train_1, df_test_1 = series_list[0]\ndf_train_2, df_test_2 = series_list[1]\ndf_train_3, df_test_3 = series_list[2]\n\n# df_train_2とdf_train_3の表示は割愛\ndf_train_1\n\n# df_test_2とdf_test_３の表示は割愛\ndf_test_1"}], "task": "code_generation", "index": 89, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_90", "instruction": "pandasを用いて顧客データ（df_customer）の各顧客に対し、売上実績がある顧客数と売上実績がない顧客数が1:1となるようにアンダーサンプリングで抽出せよ。", "instances": [{"input": "", "output": "import pandas as pd\n\ndf_tmp = df_receipt.groupby('customer_id').agg({'amount':'sum'}).reset_index()\n\ndf_tmp = pd.merge(df_customer, df_tmp, how='left', on='customer_id')\n\ndf_tmp['is_buy_flag'] = np.where(df_tmp['amount'].isnull(), 0, 1)\n\nrs = RandomUnderSampler(random_state=71)\n\ndf_down_sampling, _ = rs.fit_resample(df_tmp, df_tmp.is_buy_flag)\n\nprint('0の件数', len(df_down_sampling.query('is_buy_flag == 0')))\nprint('1の件数', len(df_down_sampling.query('is_buy_flag == 1')))"}], "task": "code_generation", "index": 90, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_91", "instruction": "pandasを用いて顧客データ（df_customer）の性別について、第三正規形へと正規化せよ。", "instances": [{"input": "", "output": "import pandas as pd\n\ndf_gender_std = df_customer[['gender_cd', 'gender']].drop_duplicates()\n\ndf_customer_std = df_customer.drop(columns='gender')\n\n# データの内容確認\ndf_customer_std.head(3)\n\n# データの内容確認\ndf_gender_std.head(3)"}], "task": "code_generation", "index": 91, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_92", "instruction": "pandasを用いて商品データ（df_product）では各カテゴリのコード値だけを保有し、カテゴリ名は保有していない。カテゴリデータ（df_category）と組み合わせて非正規化し、カテゴリ名を保有した新たな商品データを作成せよ。", "instances": [{"input": "", "output": "import pandas as pd\n\ndf_product_full = pd.merge(df_product, df_category[['category_small_cd', \n                                                    'category_major_name',\n                                                    'category_medium_name',\n                                                    'category_small_name']], \n                           how = 'inner', on = 'category_small_cd')\n\n# データの内容確認\ndf_product_full.head(3)"}], "task": "code_generation", "index": 92, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_93", "instruction": "pandasを用いてカテゴリ名付き商品データを以下の仕様でファイル出力せよ。\n\nファイル形式：csv\nヘッダ有無：有り\n文字エンコーディング：UTF-8\nファイル出力先：./data", "instances": [{"input": "", "output": "import pandas as pd\n\n# コード例1\ndf_product_full.to_csv('./data/P_df_product_full_UTF-8_header.csv', \n                       encoding='UTF-8', index=False)\n\n\n# コード例2（BOM付きでExcelの文字化けを防ぐ）\ndf_product_full.to_csv('./data/P_df_product_full_UTF-8BOM_header.csv', \n                       encoding='utf_8_sig', index=False)"}], "task": "code_generation", "index": 93, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_94", "instruction": "pandasを用いてカテゴリ名付き商品データを以下の仕様でファイル出力せよ。\n\nファイル形式：csv\nヘッダ有無：有り\n文字エンコーディング：CP932\nファイル出力先：./data", "instances": [{"input": "", "output": "import pandas as pd\n\ndf_product_full.to_csv('./data/P_df_product_full_CP932_header.csv', \n                       encoding='CP932', index=False)"}], "task": "code_generation", "index": 94, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_95", "instruction": "pandasを用いてpandasを用いてカテゴリ名付き商品データを以下の仕様でファイル出力せよ。\n\nファイル形式：csv\nヘッダ有無：無し\n文字エンコーディング：UTF-8\nファイル出力先：./data", "instances": [{"input": "", "output": "import pandas as pd\n\ndf_product_full.to_csv('./data/P_df_product_full_UTF-8_noh.csv', \n                       header=False, encoding='UTF-8', index=False)"}], "task": "code_generation", "index": 95, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_96", "instruction": "pandasを用いて作成した以下形式のファイルを読み込み、データを3件を表示させて正しく取り込まれていることを確認せよ。\n\nファイル形式：csv\nヘッダ有無：有り\n文字エンコーディング：UTF-8", "instances": [{"input": "", "output": "import pandas as pd\n\ndf_product_full = pd.read_csv('./data/P_df_product_full_UTF-8_header.csv',\n                    dtype={'category_major_cd':str,\n                           'category_medium_cd':str,\n                           'category_small_cd':str},\n                    encoding='UTF-8')\n\ndf_product_full.head(3)"}], "task": "code_generation", "index": 96, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_97", "instruction": "pandasを用いて作成した以下形式のファイルを読み込み、データを3件を表示させて正しく取り込まれていることを確認せよ。\n\nファイル形式：csv\nヘッダ有無：無し\n文字エンコーディング：UTF-8", "instances": [{"input": "", "output": "import pandas as pd\n\n# コード例1（後から項目名をつける）\ndf_product_full = pd.read_csv('../data/P_df_product_full_UTF-8_noh.csv',\n                     dtype={1:str,\n                            2:str,\n                            3:str},\n                     encoding='UTF-8', header=None)\n\ndf_product_full.columns = ['product_cd','category_major_cd',\n                           'category_medium_cd', 'category_small_cd',\n                           'unit_price','unit_cost','category_major_name',\n                           'category_medium_name', 'category_small_name']\n\ndf_product_full.head(3)\n\n\n# コード例2（先に項目名を定義する）\nc_names = ['product_cd','category_major_cd','category_medium_cd',\n           'category_small_cd','unit_price','unit_cost',\n           'category_major_name','category_medium_name','category_small_name']\n\ndf_product_full = pd.read_csv('../data/P_df_product_full_UTF-8_noh.csv',\n                     names=c_names,\n                     dtype={'category_major_cd':str,\n                            'category_medium_cd':str,\n                            'category_small_cd':str},\n                     encoding='UTF-8', header=None)\n\ndf_product_full.head(3)"}], "task": "code_generation", "index": 97, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_98", "instruction": "pandasを用いて作成したカテゴリ名付き商品データを以下の仕様でファイル出力せよ。\n\nファイル形式：tsv\nヘッダ有無：有り\n文字エンコーディング：UTF-8\n出力先：./data", "instances": [{"input": "", "output": "import pandas as pd\n\ndf_product_full.to_csv('../data/P_df_product_full_UTF-8_header.tsv', \n                       sep='\\t', encoding='UTF-8', index=False)"}], "task": "code_generation", "index": 98, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_99", "instruction": "pandasを用いて作成した以下形式のファイルを読み込み、データを3件を表示させて正しく取り込まれていることを確認せよ。\n\nファイル形式：tsv\nヘッダ有無：有り\n文字エンコーディング：UTF-8", "instances": [{"input": "", "output": "import pandas as pd\n\n# コード例1（read_table）\ndf_product_full = pd.read_table('../data/P_df_product_full_UTF-8_header.tsv', \n                       dtype={'category_major_cd':str,\n                              'category_medium_cd':str,\n                              'category_small_cd':str},\n                       encoding='UTF-8')\ndf_product_full.head(3)\n\n\n# コード例2（read_csv）\ndf_product_full = pd.read_csv('../data/P_df_product_full_UTF-8_header.tsv', \n                       dtype={'category_major_cd':str,\n                              'category_medium_cd':str,\n                              'category_small_cd':str},\n                       sep='\\t', encoding='UTF-8')\ndf_product_full.head(3)"}], "task": "code_generation", "index": 99, "liscence": "MIT", "source": "datascience_100_knocks_python"}
{"id": "amenokaku_200", "instruction": "pythonを用いて、画像を読み込み、RGBをBGRの順に入れ替えよ。", "instances": [{"input": "", "output": "import cv2\n\n# function: BGR -> RGB\ndef BGR2RGB(img):\n    b = img[:, :, 0].copy()\n    g = img[:, :, 1].copy()\n    r = img[:, :, 2].copy()\n\n    # RGB > BGR\n    img[:, :, 0] = r\n    img[:, :, 1] = g\n    img[:, :, 2] = b\n\n    return img\n\n# Read image\nimg = cv2.imread(\"sample.jpg\")\n\n# BGR -> RGB\nimg = BGR2RGB(img)\n\n# Save result\ncv2.imwrite(\"out.jpg\", img)\ncv2.imshow(\"result\", img)\ncv2.waitKey(0)\ncv2.destroyAllWindows()"}], "task": "code_generation", "index": 200, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_201", "instruction": "pythonを用いて、画像をグレースケールにせよ。 グレースケールとは、画像の輝度表現方法の一種であり下式で計算される。\n\nY = 0.2126 R + 0.7152 G + 0.0722 B", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\n\n# Gray scale\ndef BGR2GRAY(img):\n\tb = img[:, :, 0].copy()\n\tg = img[:, :, 1].copy()\n\tr = img[:, :, 2].copy()\n\n\t# Gray scale\n\tout = 0.2126 * r + 0.7152 * g + 0.0722 * b\n\tout = out.astype(np.uint8)\n\n\treturn out\n\n\n# Read image\nimg = cv2.imread(\"sample.jpg\").astype(np.float)\n\n# Grayscale\nout = BGR2GRAY(img)\n\n# Save result\ncv2.imwrite(\"out.jpg\", out)\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)\ncv2.destroyAllWindows()"}], "task": "code_generation", "index": 201, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_202", "instruction": "pythonを用いて、画像を二値化せよ。 二値化とは、画像を黒と白の二値で表現する方法である。 ここでは、グレースケールにおいて閾値を128に設定し、下式で二値化する。\n\ny = { 0 (if y < 128)\n     255 (else) ", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\n\n# Gray scale\ndef BGR2GRAY(img):\n\tb = img[:, :, 0].copy()\n\tg = img[:, :, 1].copy()\n\tr = img[:, :, 2].copy()\n\n\t# Gray scale\n\tout = 0.2126 * r + 0.7152 * g + 0.0722 * b\n\tout = out.astype(np.uint8)\n\n\treturn out\n\n# binalization\ndef binarization(img, th=128):\n\timg[img < th] = 0\n\timg[img >= th] = 255\n\treturn img\n\t\n\n# Read image\nimg = cv2.imread(\"sample.jpg\").astype(np.float32)\n\n# Grayscale\nout = BGR2GRAY(img)\n\n# Binarization\nout = binarization(out)\n\n# Save result\ncv2.imwrite(\"out.jpg\", out)\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)\ncv2.destroyAllWindows()"}], "task": "code_generation", "index": 202, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_203", "instruction": "pythonを用いて、大津の二値化を実装せよ。\n大津の二値化とは判別分析法と呼ばれ、二値化における分離の閾値を自動決定する手法である。\nこれは**クラス内分散**と**クラス間分散**の比から計算される。\n\n\n- 閾値t未満をクラス0, t以上をクラス1とする。\n- w0, w1 ... 閾値tにより分離された各クラスの画素数の割合 (w0 + w1 = 1を満たす)\n- S0^2, S1^2 ... 各クラスの画素値の分散\n- M0, M1 ... 各クラスの画素値の平均値\n\nとすると、\n\nクラス内分散 Sw^2 = w0 * S0^2 + w1 * S1^2\nクラス間分散 Sb^2 = w0 * (M0 - Mt)^2 + w1 * (M1 - Mt)^2 = w0 * w1 * (M0 - M1) ^2\n画像全体の画素の分散 St^2 = Sw^2 + Sb^2 = (const)\n以上より、分離度は次式で定義される。\n分離度 X = Sb^2 / Sw^2 = Sb^2 / (St^2 - Sb^2)\n\nとなり、\n\nargmax_{t} X = argmax_{t} Sb^2\n\nとなる。すなわち、Sb^2 =  w0 * w1 * (M0 - M1) ^2 が最大となる、閾値tを二値化の閾値とすれば良い。", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\n\n\n# Gray scale\ndef BGR2GRAY(img):\n\tb = img[:, :, 0].copy()\n\tg = img[:, :, 1].copy()\n\tr = img[:, :, 2].copy()\n\n\t# Gray scale\n\tout = 0.2126 * r + 0.7152 * g + 0.0722 * b\n\tout = out.astype(np.uint8)\n\n\treturn out\n\n# Otsu Binalization\ndef otsu_binarization(img, th=128):\n\timax_sigma = 0\n\tmax_t = 0\n\n\t# determine threshold\n\tfor _t in range(1, 255):\n\t\tv0 = out[np.where(out < _t)]\n\t\tm0 = np.mean(v0) if len(v0) > 0 else 0.\n\t\tw0 = len(v0) / (H * W)\n\t\tv1 = out[np.where(out >= _t)]\n\t\tm1 = np.mean(v1) if len(v1) > 0 else 0.\n\t\tw1 = len(v1) / (H * W)\n\t\tsigma = w0 * w1 * ((m0 - m1) ** 2)\n\t\tif sigma > max_sigma:\n\t\t\tmax_sigma = sigma\n\t\t\tmax_t = _t\n\n\t# Binarization\n\tprint(\"threshold >>\", max_t)\n\tth = max_t\n\tout[out < th] = 0\n\tout[out >= th] = 255\n\n\treturn out\n\n\n# Read image\nimg = cv2.imread(\"imori.jpg\").astype(np.float32)\n\n\n# Grayscale\nout = BGR2GRAY(img)\n\n# Otsu's binarization\nout = otsu_binalization(out)\n\n# Save result\ncv2.imwrite(\"out.jpg\", out)\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)\ncv2.destroyAllWindows()"}], "task": "code_generation", "index": 203, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_204", "instruction": "pythonを用いて、HSV変換を実装して、色相Hを反転せよ。\n\nHSV変換とは、Hue(色相)、Saturation(彩度)、Value(明度) で色を表現する手法である。", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\n\n\n# BGR -> HSV\ndef BGR2HSV(_img):\n\timg = _img.copy() / 255.\n\n\thsv = np.zeros_like(img, dtype=np.float32)\n\n\t# get max and min\n\tmax_v = np.max(img, axis=2).copy()\n\tmin_v = np.min(img, axis=2).copy()\n\tmin_arg = np.argmin(img, axis=2)\n\n\t# H\n\thsv[..., 0][np.where(max_v == min_v)]= 0\n\t## if min == B\n\tind = np.where(min_arg == 0)\n\thsv[..., 0][ind] = 60 * (img[..., 1][ind] - img[..., 2][ind]) / (max_v[ind] - min_v[ind]) + 60\n\t## if min == R\n\tind = np.where(min_arg == 2)\n\thsv[..., 0][ind] = 60 * (img[..., 0][ind] - img[..., 1][ind]) / (max_v[ind] - min_v[ind]) + 180\n\t## if min == G\n\tind = np.where(min_arg == 1)\n\thsv[..., 0][ind] = 60 * (img[..., 2][ind] - img[..., 0][ind]) / (max_v[ind] - min_v[ind]) + 300\n\t\t\n\t# S\n\thsv[..., 1] = max_v.copy() - min_v.copy()\n\n\t# V\n\thsv[..., 2] = max_v.copy()\n\t\n\treturn hsv\n\n\ndef HSV2BGR(_img, hsv):\n\timg = _img.copy() / 255.\n\n\t# get max and min\n\tmax_v = np.max(img, axis=2).copy()\n\tmin_v = np.min(img, axis=2).copy()\n\n\tout = np.zeros_like(img)\n\n\tH = hsv[..., 0]\n\tS = hsv[..., 1]\n\tV = hsv[..., 2]\n\n\tC = S\n\tH_ = H / 60.\n\tX = C * (1 - np.abs( H_ % 2 - 1))\n\tZ = np.zeros_like(H)\n\n\tvals = [[Z,X,C], [Z,C,X], [X,C,Z], [C,X,Z], [C,Z,X], [X,Z,C]]\n\n\tfor i in range(6):\n\t\tind = np.where((i <= H_) & (H_ < (i+1)))\n\t\tout[..., 0][ind] = (V - C)[ind] + vals[i][0][ind]\n\t\tout[..., 1][ind] = (V - C)[ind] + vals[i][1][ind]\n\t\tout[..., 2][ind] = (V - C)[ind] + vals[i][2][ind]\n\n\tout[np.where(max_v == min_v)] = 0\n\tout = np.clip(out, 0, 1)\n\tout = (out * 255).astype(np.uint8)\n\n\treturn out\n\n\n# Read image\nimg = cv2.imread(\"imori.jpg\").astype(np.float32)\n\n# RGB > HSV\nhsv = BGR2HSV(img)\n\n# Transpose Hue\nhsv[..., 0] = (hsv[..., 0] + 180) % 360\n\n# HSV > RGB\nout = HSV2BGR(img, hsv)\n\n# Save result\ncv2.imwrite(\"out.jpg\", out)\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n\tout = np.clip(out, 0, 1)\n\tout = (out * 255).astype(np.uint8)\n\n\treturn out\n\n\n# Read image\nimg = cv2.imread(\"imori.jpg\").astype(np.float32)\n\n# RGB > HSV\nhsv = BGR2HSV(img)\n\n# Transpose Hue\nhsv[..., 0] = (hsv[..., 0] + 180) % 360\n\n# HSV > RGB\nout = HSV2BGR(img, hsv)\n\n# Save result\ncv2.imwrite(\"out.jpg\", out)\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)\ncv2.destroyAllWindows()"}], "task": "code_generation", "index": 204, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_205", "instruction": "pythonを用いて、画像の値を256^3から4^3、すなわちR,G,B in {32, 96, 160, 224}の各4値に減色せよ。これは量子化操作である。各値に関して、以下の様に定義する。\n\nval = {  32  (  0 <= val <  64)\n         96  ( 64 <= val < 128)\n        160  (128 <= val < 192)\n        224  (192 <= val < 256)", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\n\n\n# Dicrease color\ndef dicrease_color(img):\n\tout = img.copy()\n\n\tout = out // 64 * 64 + 32\n\n\treturn out\n\n\n# Read image\nimg = cv2.imread(\"imori.jpg\")\n\n# Dicrease color\nout = dicrease_color(img)\n\ncv2.imwrite(\"out.jpg\", out)\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)\ncv2.destroyAllWindows()"}], "task": "code_generation", "index": 205, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_206", "instruction": "ここでは画像をグリッド分割(ある固定長の領域に分ける)し、かく領域内(セル)の平均値でその領域内の値を埋める。このようにグリッド分割し、その領域内の代表値を求める操作はPooling(プーリング)と呼ばれる。これらプーリング操作はCNN(Convolutional Neural Network) において重要な役割を持つ。\n\nこれは次式で定義される。\n\nv = 1/|R| * Sum_{i in R} v_i\n\nここではpythonを用いて、128x128のimori.jpgを8x8にグリッド分割し、平均プーリングせよ。", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\n\n\n# average pooling\ndef average_pooling(img, G=8):\n    out = img.copy()\n\n    H, W, C = img.shape\n    Nh = int(H / G)\n    Nw = int(W / G)\n\n    for y in range(Nh):\n        for x in range(Nw):\n            for c in range(C):\n                out[G*y:G*(y+1), G*x:G*(x+1), c] = np.mean(out[G*y:G*(y+1), G*x:G*(x+1), c]).astype(np.int)\n    \n    return out\n\n\n# Read image\nimg = cv2.imread(\"imori.jpg\")\n\n# Average Pooling\nout = average_pooling(img)\n\n# Save result\ncv2.imwrite(\"out.jpg\", out)\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)\ncv2.destroyAllWindows()"}], "task": "code_generation", "index": 206, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_207", "instruction": "pythonを用いて、128x128のimori.jpgを8x8にグリッド分割し、最大値でプーリングせよ。", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\n\n# max pooling\ndef max_pooling(img, G=8):\n    # Max Pooling\n    out = img.copy()\n\n    H, W, C = img.shape\n    Nh = int(H / G)\n    Nw = int(W / G)\n\n    for y in range(Nh):\n        for x in range(Nw):\n            for c in range(C):\n                out[G*y:G*(y+1), G*x:G*(x+1), c] = np.max(out[G*y:G*(y+1), G*x:G*(x+1), c])\n\n    return out\n\n\n# Read image\nimg = cv2.imread(\"imori.jpg\")\n\n# Max pooling\nout = max_pooling(img)\n\n# Save result\ncv2.imwrite(\"out.jpg\", out)\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)\ncv2.destroyAllWindows()"}], "task": "code_generation", "index": 207, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_208", "instruction": "pythonを用いて、ガウシアンフィルタ(3x3、標準偏差1.3)を実装し、imori_noise.jpgのノイズを除去せよ。\n\nガウシアンフィルタとは画像の平滑化（滑らかにする）を行うフィルタの一種であり、ノイズ除去にも使われる。\n\nノイズ除去には他にも、メディアンフィルタ、平滑化フィルタ、LoGフィルタなどがある。\n\nガウシアンフィルタは注目画素の周辺画素を、ガウス分布による重み付けで平滑化し、次式で定義される。このような重みはカーネルやフィルタと呼ばれる。\n\nただし、画像の端はこのままではフィルタリングできないため、画素が足りない部分は0で埋める。これを0パディングと呼ぶ。かつ、重みは正規化する。(sum g = 1)\n\n重みはガウス分布から次式になる。", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\n\n\n# Gaussian filter\ndef gaussian_filter(img, K_size=3, sigma=1.3):\n\tif len(img.shape) == 3:\n\t\tH, W, C = img.shape\n\telse:\n\t\timg = np.expand_dims(img, axis=-1)\n\t\tH, W, C = img.shape\n\n\t\t\n\t## Zero padding\n\tpad = K_size // 2\n\tout = np.zeros((H + pad * 2, W + pad * 2, C), dtype=np.float)\n\tout[pad: pad + H, pad: pad + W] = img.copy().astype(np.float)\n\n\t## prepare Kernel\n\tK = np.zeros((K_size, K_size), dtype=np.float)\n\tfor x in range(-pad, -pad + K_size):\n\t\tfor y in range(-pad, -pad + K_size):\n\t\t\tK[y + pad, x + pad] = np.exp( -(x ** 2 + y ** 2) / (2 * (sigma ** 2)))\n\tK /= (2 * np.pi * sigma * sigma)\n\tK /= K.sum()\n\n\ttmp = out.copy()\n\n\t# filtering\n\tfor y in range(H):\n\t\tfor x in range(W):\n\t\t\tfor c in range(C):\n\t\t\t\tout[pad + y, pad + x, c] = np.sum(K * tmp[y: y + K_size, x: x + K_size, c])\n\n\tout = np.clip(out, 0, 255)\n\tout = out[pad: pad + H, pad: pad + W].astype(np.uint8)\n\n\treturn out\n\n\n# Read image\nimg = cv2.imread(\"imori_noise.jpg\")\n\n\n# Gaussian Filter\nout = gaussian_filter(img, K_size=3, sigma=1.3)\n\n\n# Save result\ncv2.imwrite(\"out.jpg\", out)\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)\ncv2.destroyAllWindows()"}], "task": "code_generation", "index": 208, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_209", "instruction": "メディアンフィルタ(3x3)を実装し、imori_noise.jpgのノイズを除去せよ。\n\nメディアンフィルタとは画像の平滑化を行うフィルタの一種である。\n\nこれは注目画素の3x3の領域内の、メディアン値(中央値)を出力するフィルタである。\npythonを用いてこれをゼロパディングせよ。", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\n\n\n# Median filter\ndef median_filter(img, K_size=3):\n    H, W, C = img.shape\n\n    ## Zero padding\n    pad = K_size // 2\n    out = np.zeros((H + pad*2, W + pad*2, C), dtype=np.float)\n    out[pad:pad+H, pad:pad+W] = img.copy().astype(np.float)\n\n    tmp = out.copy()\n\n    # filtering\n    for y in range(H):\n        for x in range(W):\n            for c in range(C):\n                out[pad+y, pad+x, c] = np.median(tmp[y:y+K_size, x:x+K_size, c])\n\n    out = out[pad:pad+H, pad:pad+W].astype(np.uint8)\n\n    return out\n\n\n# Read image\nimg = cv2.imread(\"imori_noise.jpg\")\n\n\n# Median Filter\nout = median_filter(img, K_size=3)\n\n\n# Save result\ncv2.imwrite(\"out.jpg\", out)\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)\ncv2.destroyAllWindows()"}], "task": "code_generation", "index": 209, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_210", "instruction": "pythonを用いて、平滑化フィルタ(3x3)を実装せよ。\n\n平滑化フィルタはフィルタ内の画素の平均値を出力するフィルタである。", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\n\n# mean filter\ndef mean_filter(img, K_size=3):\n    H, W, C = img.shape\n\n    # zero padding\n    pad = K_size // 2\n    out = np.zeros((H + pad * 2, W + pad * 2, C), dtype=np.float)\n    out[pad: pad + H, pad: pad + W] = img.copy().astype(np.float)\n    tmp = out.copy()\n\n    # filtering\n    for y in range(H):\n        for x in range(W):\n            for c in range(C):\n                out[pad + y, pad + x, c] = np.mean(tmp[y: y + K_size, x: x + K_size, c])\n\n    out = out[pad: pad + H, pad: pad + W].astype(np.uint8)\n\n    return out\n\n# Read image\nimg = cv2.imread(\"imori.jpg\")\n\n# Mean Filter\nout = mean_filter(img, K_size=3)\n\n# Save result\ncv2.imwrite(\"out.jpg\", out)\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)\ncv2.destroyAllWindows()"}], "task": "code_generation", "index": 210, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_211", "instruction": "pythonを用いて、モーションフィルタ(3x3)を実装せよ。\n\nモーションフィルタとは対角方向の平均値を取るフィルタであり、次式で定義される。\n\n  1/3  0   0\n[  0  1/3  0 ]\n   0   0  1/3", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\n\n# motion filter\ndef motion_filter(img, K_size=3):\n    H, W, C = img.shape\n\n    # Kernel\n    K = np.diag( [1] * K_size ).astype(np.float)\n    K /= K_size\n\n    # zero padding\n    pad = K_size // 2\n    out = np.zeros((H + pad * 2, W + pad * 2, C), dtype=np.float)\n    out[pad: pad + H, pad: pad + W] = img.copy().astype(np.float)\n    tmp = out.copy()\n\n    # filtering\n    for y in range(H):\n        for x in range(W):\n            for c in range(C):\n                out[pad + y, pad + x, c] = np.sum(K * tmp[y: y + K_size, x: x + K_size, c])\n\n    out = out[pad: pad + H, pad: pad + W].astype(np.uint8)\n\n    return out\n\n\n# Read image\nimg = cv2.imread(\"imori.jpg\")\n\n# motion filtering\nout = motion_filter(img, K_size=3)\n\n# Save result\ncv2.imwrite(\"out.jpg\", out)\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)\ncv2.destroyAllWindows()"}], "task": "code_generation", "index": 211, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_212", "instruction": "pythonを用いて、MAX-MINフィルタ(3x3)を実装せよ。\n\nMAX-MINフィルタとはフィルタ内の画素の最大値と最小値の差を出力するフィルタであり、エッジ検出のフィルタの一つである。エッジ検出とは画像内の線を検出るすることであり、このような画像内の情報を抜き出す操作を特徴抽出と呼ぶ。エッジ検出では多くの場合、グレースケール画像に対してフィルタリングを行う。", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\n\n# Gray scale\ndef BGR2GRAY(img):\n\tb = img[:, :, 0].copy()\n\tg = img[:, :, 1].copy()\n\tr = img[:, :, 2].copy()\n\n\t# Gray scale\n\tout = 0.2126 * r + 0.7152 * g + 0.0722 * b\n\tout = out.astype(np.uint8)\n\n\treturn out\n\n# max-min filter\ndef max_min_filter(img, K_size=3):\n\tH, W, C = img.shape\n\n\t## Zero padding\n\tpad = K_size // 2\n\tout = np.zeros((H + pad * 2, W + pad * 2), dtype=np.float)\n\tout[pad: pad + H, pad: pad + W] = gray.copy().astype(np.float)\n\ttmp = out.copy()\n\n\t# filtering\n\tfor y in range(H):\n\t\tfor x in range(W):\n\t\t\tout[pad + y, pad + x] = np.max(tmp[y: y + K_size, x: x + K_size]) - np.min(tmp[y: y + K_size, x: x + K_size])\n\n\tout = out[pad: pad + H, pad: pad + W].astype(np.uint8)\n\n\treturn out\n\n\n# Read image\nimg = cv2.imread(\"imori.jpg\").astype(np.float)\n\n# grayscale\ngray = BGR2GRAY(img)\n\n# Max-Min filtering\nout = max_min_filter(gray, K_size=3)\n\n# Save result\ncv2.imwrite(\"out.jpg\", out)\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)\ncv2.destroyAllWindows()"}], "task": "code_generation", "index": 212, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_213", "instruction": "pythonを用いて、微分フィルタ(3x3)を実装せよ。\n\n微分フィルタは輝度の急激な変化が起こっている部分のエッジを取り出すフィルタであり、隣り合う画素同士の差を取る。\n\n\n    (a)縦方向         (b)横方向\n      0 -1  0            0 0 0\nK = [ 0  1  0 ]   K = [ -1 1 0 ]\n      0  0  0            0 0 0\n", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\n\n# Gray scale\ndef BGR2GRAY(img):\n\tb = img[:, :, 0].copy()\n\tg = img[:, :, 1].copy()\n\tr = img[:, :, 2].copy()\n\n\t# Gray scale\n\tout = 0.2126 * r + 0.7152 * g + 0.0722 * b\n\tout = out.astype(np.uint8)\n\n\treturn out\n\n# different filter\ndef different_filter(img, K_size=3):\n\tH, W, C = img.shape\n\n\t# Zero padding\n\tpad = K_size // 2\n\tout = np.zeros((H + pad * 2, W + pad * 2), dtype=np.float)\n\tout[pad: pad + H, pad: pad + W] = gray.copy().astype(np.float)\n\ttmp = out.copy()\n\n\tout_v = out.copy()\n\tout_h = out.copy()\n\n\t# vertical kernel\n\tKv = [[0., -1., 0.],[0., 1., 0.],[0., 0., 0.]]\n\t# horizontal kernel\n\tKh = [[0., 0., 0.],[-1., 1., 0.], [0., 0., 0.]]\n\n\t# filtering\n\tfor y in range(H):\n\t\tfor x in range(W):\n\t\t\tout_v[pad + y, pad + x] = np.sum(Kv * (tmp[y: y + K_size, x: x + K_size]))\n\t\t\tout_h[pad + y, pad + x] = np.sum(Kh * (tmp[y: y + K_size, x: x + K_size]))\n\n\tout_v = np.clip(out_v, 0, 255)\n\tout_h = np.clip(out_h, 0, 255)\n\n\tout_v = out_v[pad: pad + H, pad: pad + W].astype(np.uint8)\n\tout_h = out_h[pad: pad + H, pad: pad + W].astype(np.uint8)\n\n\treturn out_v, out_h\n\n# Read image\nimg = cv2.imread(\"imori.jpg\").astype(np.float)\n\n# grayscale\ngray = BGR2GRAY(img)\n\n# different filtering\nout_v, out_h = different_filter(gray, K_size=3)\n\n\n\n# Save result\ncv2.imwrite(\"out_v.jpg\", out_v)\ncv2.imshow(\"result\", out_v)\ncv2.waitKey(0)\n\ncv2.imwrite(\"out_h.jpg\", out_h)\ncv2.imshow(\"result\", out_h)\ncv2.waitKey(0)\ncv2.destroyAllWindows()"}], "task": "code_generation", "index": 213, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_214", "instruction": "pythonを用いて、Sobelフィルタ(3x3)を実装せよ。\n\nソーベルフィルタ(Sobelフィルタ)は特定方向（縦や横）のエッジのみを抽出するフィルタであり、次式でそれぞれ定義される。\n\n\n    (a)縦方向       (b)横方向\n       1  2  1           1  0 -1\nK = [  0  0  0 ]   K = [ 2  0 -2 ]\n      -1 -2 -1           1  0 -1\n", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\n\n# Gray scale\ndef BGR2GRAY(img):\n\tb = img[:, :, 0].copy()\n\tg = img[:, :, 1].copy()\n\tr = img[:, :, 2].copy()\n\n\t# Gray scale\n\tout = 0.2126 * r + 0.7152 * g + 0.0722 * b\n\tout = out.astype(np.uint8)\n\n\treturn out\n\n\n# sobel filter\ndef sobel_filter(img, K_size=3):\n\tif len(img.shape) == 3:\n\t\tH, W, C = img.shape\n\telse:\n\t\timg = np.expand_dims(img, axis=-1)\n\t\tH, W, C = img.shape\n\n\t# Zero padding\n\tpad = K_size // 2\n\tout = np.zeros((H + pad * 2, W + pad * 2), dtype=np.float)\n\tout[pad: pad + H, pad: pad + W] = gray.copy().astype(np.float)\n\ttmp = out.copy()\n\n\tout_v = out.copy()\n\tout_h = out.copy()\n\n\t## Sobel vertical\n\tKv = [[1., 2., 1.],[0., 0., 0.], [-1., -2., -1.]]\n\t## Sobel horizontal\n\tKh = [[1., 0., -1.],[2., 0., -2.],[1., 0., -1.]]\n\n\t# filtering\n\tfor y in range(H):\n\t\tfor x in range(W):\n\t\t\tout_v[pad + y, pad + x] = np.sum(Kv * (tmp[y: y + K_size, x: x + K_size]))\n\t\t\tout_h[pad + y, pad + x] = np.sum(Kh * (tmp[y: y + K_size, x: x + K_size]))\n\n\tout_v = np.clip(out_v, 0, 255)\n\tout_h = np.clip(out_h, 0, 255)\n\n\tout_v = out_v[pad: pad + H, pad: pad + W].astype(np.uint8)\n\tout_h = out_h[pad: pad + H, pad: pad + W].astype(np.uint8)\n\n\treturn out_v, out_h\n\n# Read image\nimg = cv2.imread(\"imori.jpg\").astype(np.float)\n\n# grayscale\ngray = BGR2GRAY(img)\n\n# sobel filtering\nout_v, out_h = sobel_filter(gray, K_size=3)\n\n# Save result\ncv2.imwrite(\"out_v.jpg\", out_v)\ncv2.imshow(\"result\", out_v)\ncv2.waitKey(0)\n\ncv2.imwrite(\"out_h.jpg\", out_h)\ncv2.imshow(\"result\", out_h)\ncv2.waitKey(0)\ncv2.destroyAllWindows()"}], "task": "code_generation", "index": 214, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_215", "instruction": "pythonを用いて、Prewittフィルタ(3x3)を実装せよ。\n\nPrewittフィルタはエッジ抽出フィルタの一種であり、次式で定義される。\n\n\n    (a)縦方向          (b)横方向\n      -1 -1 -1          -1 0 1\nK = [  0  0  0 ]  K = [ -1 0 1 ]\n       1  1  1          -1 0 1\n", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\n\n\n# Gray scale\ndef BGR2GRAY(img):\n\tb = img[:, :, 0].copy()\n\tg = img[:, :, 1].copy()\n\tr = img[:, :, 2].copy()\n\n\t# Gray scale\n\tout = 0.2126 * r + 0.7152 * g + 0.0722 * b\n\tout = out.astype(np.uint8)\n\n\treturn out\n\n\n# prewitt filter\ndef prewitt_filter(img, K_size=3):\n\tH, W, C = img.shape\n\n\t# Zero padding\n\tpad = K_size // 2\n\tout = np.zeros((H + pad * 2, W + pad * 2), dtype=np.float)\n\tout[pad: pad + H, pad: pad + W] = gray.copy().astype(np.float)\n\ttmp = out.copy()\n\n\tout_v = out.copy()\n\tout_h = out.copy()\n\n\t## prewitt vertical kernel\n\tKv = [[-1., -1., -1.],[0., 0., 0.], [1., 1., 1.]]\n\t## prewitt horizontal kernel\n\tKh = [[-1., 0., 1.],[-1., 0., 1.],[-1., 0., 1.]]\n\n\t# filtering\n\tfor y in range(H):\n\t\tfor x in range(W):\n\t\t\tout_v[pad + y, pad + x] = np.sum(Kv * (tmp[y: y + K_size, x: x + K_size]))\n\t\t\tout_h[pad + y, pad + x] = np.sum(Kh * (tmp[y: y + K_size, x: x + K_size]))\n\n\n\tout_v = np.clip(out_v, 0, 255)\n\tout_h = np.clip(out_h, 0, 255)\n\n\tout_v = out_v[pad: pad + H, pad: pad + W].astype(np.uint8)\n\tout_h = out_h[pad: pad + H, pad: pad + W].astype(np.uint8)\n\n\treturn out_v, out_h\n\n# Read image\nimg = cv2.imread(\"imori.jpg\").astype(np.float)\n\n# grayscale\ngray = BGR2GRAY(img)\n\n# prewitt filtering\nout_v, out_h = prewitt_filter(gray, K_size=3)\n\n\n# Save result\ncv2.imwrite(\"out_v.jpg\", out_v)\ncv2.imshow(\"result\", out_v)\ncv2.waitKey(0)\n\ncv2.imwrite(\"out_h.jpg\", out_h)\ncv2.imshow(\"result\", out_h)\ncv2.waitKey(0)\ncv2.destroyAllWindows()"}], "task": "code_generation", "index": 215, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_216", "instruction": "pythonを用いて、Laplacianフィルタを実装せよ。\n\nLaplacian（ラプラシアン）フィルタとは輝度の二次微分をとることでエッジ検出を行うフィルタである。\n\nデジタル画像は離散データであるので、x方向・y方向の一次微分は、それぞれ次式で表される。\n\n\nIx(x,y) = (I(x+1, y) - I(x,y)) / ((x+1)-x) = I(x+1, y) - I(x,y)\nIy(x,y) = (I(x, y+1) - I(x,y)) / ((y+1)-y) = I(x, y+1) - I(x,y)\n\n\nさらに二次微分は、次式で表される。\n\n\nIxx(x,y) = (Ix(x,y) - Ix(x-1,y)) / ((x+1)-x) = Ix(x,y) - Ix(x-1,y)\n         = (I(x+1, y) - I(x,y)) - (I(x, y) - I(x-1,y))\n         = I(x+1,y) - 2 * I(x,y) + I(x-1,y)\nIyy(x,y) = ... = I(x,y+1) - 2 * I(x,y) + I(x,y-1)\n\n\nこれらより、ラプラシアン は次式で定義される。\n\n\nD^2 I(x,y) = Ixx(x,y) + Iyy(x,y)\n           = I(x-1,y) + I(x,y-1) - 4 * I(x,y) + I(x+1,y) + I(x,y+1)\n\n\nこれをカーネル化すると、次のようになる。\n\n\n      0  1  0\nK = [ 1 -4  1 ]\n      0  1  0\n", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\n\n\n# Gray scale\ndef BGR2GRAY(img):\n\tb = img[:, :, 0].copy()\n\tg = img[:, :, 1].copy()\n\tr = img[:, :, 2].copy()\n\n\t# Gray scale\n\tout = 0.2126 * r + 0.7152 * g + 0.0722 * b\n\tout = out.astype(np.uint8)\n\n\treturn out\n\n# laplacian filter\ndef laplacian_filter(img, K_size=3):\n\tH, W, C = img.shape\n\n\t# zero padding\n\tpad = K_size // 2\n\tout = np.zeros((H + pad * 2, W + pad * 2), dtype=np.float)\n\tout[pad: pad + H, pad: pad + W] = gray.copy().astype(np.float)\n\ttmp = out.copy()\n\n\t# laplacian kernle\n\tK = [[0., 1., 0.],[1., -4., 1.], [0., 1., 0.]]\n\n\t# filtering\n\tfor y in range(H):\n\t\tfor x in range(W):\n\t\t\tout[pad + y, pad + x] = np.sum(K * (tmp[y: y + K_size, x: x + K_size]))\n\n\tout = np.clip(out, 0, 255)\n\tout = out[pad: pad + H, pad: pad + W].astype(np.uint8)\n\n\treturn out\n\n# Read image\nimg = cv2.imread(\"imori.jpg\").astype(np.float)\n\n# grayscale\ngray = BGR2GRAY(img)\n\n# prewitt filtering\nout = laplacian_filter(gray, K_size=3)\n\n\n# Save result\ncv2.imwrite(\"out.jpg\", out)\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)\ncv2.destroyAllWindows()"}], "task": "code_generation", "index": 216, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_217", "instruction": "pythonを用いて、Embossフィルタを実装せよ。\n\nEmbossフィルタとは輪郭部分を浮き出しにするフィルタで、次式で定義される。\n\n\n      -2 -1  0\nK = [ -1  1  1 ]\n       0  1  2\n", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\n\n# Gray scale\ndef BGR2GRAY(img):\n\tb = img[:, :, 0].copy()\n\tg = img[:, :, 1].copy()\n\tr = img[:, :, 2].copy()\n\n\t# Gray scale\n\tout = 0.2126 * r + 0.7152 * g + 0.0722 * b\n\tout = out.astype(np.uint8)\n\n\treturn out\n\n# emboss filter\ndef emboss_filter(img, K_size=3):\n\tH, W, C = img.shape\n\n\t# zero padding\n\tpad = K_size // 2\n\tout = np.zeros((H + pad * 2, W + pad * 2), dtype=np.float)\n\tout[pad: pad + H, pad: pad + W] = gray.copy().astype(np.float)\n\ttmp = out.copy()\n\n\t# emboss kernel\n\tK = [[-2., -1., 0.],[-1., 1., 1.], [0., 1., 2.]]\n\n\t# filtering\n\tfor y in range(H):\n\t\tfor x in range(W):\n\t\t\tout[pad + y, pad + x] = np.sum(K * (tmp[y: y + K_size, x: x + K_size]))\n\n\tout = np.clip(out, 0, 255)\n\tout = out[pad: pad + H, pad: pad + W].astype(np.uint8)\n\n\treturn out\n\n\n# Read image\nimg = cv2.imread(\"imori.jpg\").astype(np.float)\n\n# grayscale\ngray = BGR2GRAY(img)\n\n# emboss filtering\nout = emboss_filter(gray, K_size=3)\n\n\n# Save result\ncv2.imwrite(\"out.jpg\", out)\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)\ncv2.destroyAllWindows()"}], "task": "code_generation", "index": 217, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_218", "instruction": "pythonを用いて、LoGフィルタ(sigma=3、カーネルサイズ=5)を実装し、imori_noise.jpgのエッジを検出せよ。\n\nLoGフィルタとはLaplacian of Gaussianであり、ガウシアンフィルタで画像を平滑化した後にラプラシアンフィルタで輪郭を取り出すフィルタである。\n\nLaplcianフィルタは二次微分をとるのでノイズが強調されるのを防ぐために、予めGaussianフィルタでノイズを抑える。\n\nLoGフィルタは次式で定義される。\n\n\nLoG(x,y) = (x^2 + y^2 - sigma^2) / (2 * pi * sigma^6) * exp(-(x^2+y^2) / (2*sigma^2))\n", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\n\n\n\n# Gray scale\ndef BGR2GRAY(img):\n\tb = img[:, :, 0].copy()\n\tg = img[:, :, 1].copy()\n\tr = img[:, :, 2].copy()\n\n\t# Gray scale\n\tout = 0.2126 * r + 0.7152 * g + 0.0722 * b\n\tout = out.astype(np.uint8)\n\n\treturn out\n\n# LoG filter\ndef LoG_filter(img, K_size=5, sigma=3):\n\tH, W, C = img.shape\n\n\t# zero padding\n\tpad = K_size // 2\n\tout = np.zeros((H + pad * 2, W + pad * 2), dtype=np.float)\n\tout[pad: pad + H, pad: pad + W] = gray.copy().astype(np.float)\n\ttmp = out.copy()\n\n\t# LoG Kernel\n\tK = np.zeros((K_size, K_size), dtype=np.float)\n\tfor x in range(-pad, -pad + K_size):\n\t\tfor y in range(-pad, -pad + K_size):\n\t\t\tK[y + pad, x + pad] = (x ** 2 + y ** 2 - sigma ** 2) * np.exp( -(x ** 2 + y ** 2) / (2 * (sigma ** 2)))\n\tK /= (2 * np.pi * (sigma ** 6))\n\tK /= K.sum()\n\n\t# filtering\n\tfor y in range(H):\n\t\tfor x in range(W):\n\t\t\tout[pad + y, pad + x] = np.sum(K * tmp[y: y + K_size, x: x + K_size])\n\n\tout = np.clip(out, 0, 255)\n\tout = out[pad: pad + H, pad: pad + W].astype(np.uint8)\n\n\treturn out\n\n\n# Read image\nimg = cv2.imread(\"imori_noise.jpg\")\n\n# grayscale\ngray = BGR2GRAY(img)\n\n# LoG filtering\nout = LoG_filter(gray, K_size=5, sigma=3)\n\n# Save result\ncv2.imwrite(\"out.jpg\", out)\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)\ncv2.destroyAllWindows()"}], "task": "code_generation", "index": 218, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_219", "instruction": "pythonを用いて、matplotlibを用いてimori_dark.jpgのヒストグラムを表示せよ。\n\nヒストグラムとは画素の出現回数をグラフにしたものである。\nmatplotlibではhist()という関数がすでにあるので、それを利用する。", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Read image\nimg = cv2.imread(\"imori_dark.jpg\").astype(np.float)\n\n# Display histogram\nplt.hist(img.ravel(), bins=255, rwidth=0.8, range=(0, 255))\nplt.savefig(\"out.png\")\nplt.show()"}], "task": "code_generation", "index": 219, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_220", "instruction": "pythonを用いて、ヒストグラム正規化を実装せよ。\n\nヒストグラムは偏りを持っていることが伺える。\n例えば、0に近い画素が多ければ画像は全体的に暗く、255に近い画素が多ければ画像は明るくなる。\nヒストグラムが局所的に偏っていることをダイナミックレンジが狭いなどと表現する。\nそのため画像を人の目に見やすくするために、ヒストグラムを正規化したり平坦化したりなどの処理が必要である。\n\nこのヒストグラム正規化は濃度階調変換(gray-scale transformation) と呼ばれ、[c,d]の画素値を持つ画像を[a,b]のレンジに変換する場合は次式で実現できる。\n今回はimori_dark.jpgを[0, 255]のレンジにそれぞれ変換する。", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# histogram normalization\ndef hist_normalization(img, a=0, b=255):\n\t# get max and min\n\tc = img.min()\n\td = img.max()\n\n\tout = img.copy()\n\n\t# normalization\n\tout = (b-a) / (d - c) * (out - c) + a\n\tout[out < a] = a\n\tout[out > b] = b\n\tout = out.astype(np.uint8)\n\t\n\treturn out\n\n\n# Read image\nimg = cv2.imread(\"imori_dark.jpg\").astype(np.float)\nH, W, C = img.shape\n\n# histogram normalization\nout = hist_normalization(img)\n\n# Display histogram\nplt.hist(out.ravel(), bins=255, rwidth=0.8, range=(0, 255))\nplt.savefig(\"out_his.png\")\nplt.show()\n\n# Save result\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)\ncv2.imwrite(\"out.jpg\", out)"}], "task": "code_generation", "index": 220, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_221", "instruction": "pythonを用いて、ヒストグラムの平均値をm0=128、標準偏差をs0=52になるように操作せよ。\n\nこれはヒストグラムのダイナミックレンジを変更するのではなく、ヒストグラムを平坦に変更する操作である。\n\n平均値m、標準偏差s、のヒストグラムを平均値m0, 標準偏差s0に変更するには、次式によって変換する。\n\n\nxout = s0 / s * (xin - m) + m0\n", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n# histogram manipulation\ndef hist_mani(img, m0=128, s0=52):\n\tm = np.mean(img)\n\ts = np.std(img)\n\n\tout = img.copy()\n\n\t# normalize\n\tout = s0 / s * (out - m) + m0\n\tout[out < 0] = 0\n\tout[out > 255] = 255\n\tout = out.astype(np.uint8)\n\n\treturn out\n\n\n# Read image\nimg = cv2.imread(\"imori_dark.jpg\").astype(np.float)\n\nout = hist_mani(img)\n\n# Display histogram\nplt.hist(out.ravel(), bins=255, rwidth=0.8, range=(0, 255))\nplt.savefig(\"out_his.png\")\nplt.show()\n\n# Save result\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)\ncv2.imwrite(\"out.jpg\", out)"}], "task": "code_generation", "index": 221, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_222", "instruction": "pythonを用いて、ヒストグラム平坦化を実装せよ。\n\nヒストグラム平坦化とはヒストグラムを平坦に変更する操作であり、上記の平均値や標準偏差などを必要とせず、ヒストグラム値を均衡にする操作である。\n\nこれは次式で定義される。\nただし、S ... 画素値の総数、Zmax ... 画素値の最大値、h(z) ... 濃度zの度数\n\nZ' = Zmax / S * Sum{i=0:z} h(z)\n", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# histogram equalization\ndef hist_equal(img, z_max=255):\n\tH, W, C = img.shape\n\tS = H * W * C * 1.\n\n\tout = img.copy()\n\n\tsum_h = 0.\n\n\tfor i in range(1, 255):\n\t\tind = np.where(img == i)\n\t\tsum_h += len(img[ind])\n\t\tz_prime = z_max / S * sum_h\n\t\tout[ind] = z_prime\n\n\tout = out.astype(np.uint8)\n\n\treturn out\n\n\n# Read image\nimg = cv2.imread(\"imori.jpg\").astype(np.float)\n\n# histogram normalization\nout = hist_equal(img)\n\n# Display histogram\nplt.hist(out.ravel(), bins=255, rwidth=0.8, range=(0, 255))\nplt.savefig(\"out_his.png\")\nplt.show()\n\n# Save result\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)\ncv2.imwrite(\"out.jpg\", out)"}], "task": "code_generation", "index": 222, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_223", "instruction": "pythonを用いて、imori_gamma.jpgに対してガンマ補正(c=1, g=2.2)を実行せよ。\n\nガンマ補正とは、カメラなどの媒体の経由によって画素値が非線形的に変換された場合の補正である。ディスプレイなどで画像をそのまま表示すると画面が暗くなってしまうため、RGBの値を予め大きくすることで、ディスプレイの特性を排除した画像表示を行うことがガンマ補正の目的である。\n\n非線形変換は次式で起こるとされる。\nただしxは[0,1]に正規化されている。\nc ... 定数、g ... ガンマ特性(通常は2.2)\n\n\nx' = c * Iin ^ g\n", "instances": [{"input": "", "output": "mport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n# gamma correction\ndef gamma_correction(img, c=1, g=2.2):\n\tout = img.copy()\n\tout /= 255.\n\tout = (1/c * out) ** (1/g)\n\n\tout *= 255\n\tout = out.astype(np.uint8)\n\n\treturn out\n\n\n# Read image\nimg = cv2.imread(\"imori_gamma.jpg\").astype(np.float)\n\n# Gammma correction\nout = gamma_correction(img)\n\n# Save result\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)\ncv2.imwrite(\"out.jpg\", out)"}], "task": "code_generation", "index": 223, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_224", "instruction": "pythonを用いて、最近傍補間により画像を1.5倍に拡大せよ。\n\n最近傍補間(Nearest Neighbor)は画像の拡大時に最近傍にある画素をそのまま使う手法である。\nシンプルで処理速度が速いが、画質の劣化は著しい。\n\n次式で補間される。\nI' ... 拡大後の画像、 I ... 拡大前の画像、a ... 拡大率、[ ] ... 四捨五入\n\n\nI'(x,y) = I([x/a], [y/a])\n", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n# Nereset Neighbor interpolation\ndef nn_interpolate(img, ax=1, ay=1):\n\tH, W, C = img.shape\n\n\taH = int(ay * H)\n\taW = int(ax * W)\n\n\ty = np.arange(aH).repeat(aW).reshape(aW, -1)\n\tx = np.tile(np.arange(aW), (aH, 1))\n\ty = np.round(y / ay).astype(np.int)\n\tx = np.round(x / ax).astype(np.int)\n\n\tout = img[y,x]\n\n\tout = out.astype(np.uint8)\n\n\treturn out\n\n\n# Read image\nimg = cv2.imread(\"imori.jpg\").astype(np.float)\n\n# Nearest Neighbor\nout = nn_interpolate(img, ax=1.5, ay=1.5)\n\n# Save result\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)\ncv2.imwrite(\"out.jpg\", out)"}], "task": "code_generation", "index": 224, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_225", "instruction": "pythonを用いて、Bi-linear補間により画像を1.5倍に拡大せよ。\n\nBi-linear補間とは周辺の４画素に距離に応じた重みをつけることで補完する手法である。\n計算量が多いだけ処理時間がかかるが、画質の劣化を抑えることができる。\n\n1. 拡大画像の座標(x', y')を拡大率aで割り、floor(x'/a, y'/a)を求める。\n2. 元画像の(x'/a, y'/a)の周囲4画素、I(x,y), I(x+1,y), I(x,y+1), I(x+1, y+1)を求める\n\nI(x,y)    I(x+1,y) \n     * (x'/a,y'/a)\nI(x,y+1)  I(x+1,y+1)\n\n3. それぞれの画素と(x'/a, y'/a)との距離dを求め、重み付けする。 w = d / Sum d\n4. 次式によって拡大画像の画素(x',y')を求める。 \ndx = x'/a - x , dy = y'/a - y\n\nI'(x',y') = (1-dx)(1-dy)I(x,y) + dx(1-dy)I(x+1,y) + (1-dx)dyI(x,y+1) + dxdyI(x+1,y+1)\n", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n# Bi-Linear interpolation\ndef bl_interpolate(img, ax=1., ay=1.):\n\tH, W, C = img.shape\n\n\taH = int(ay * H)\n\taW = int(ax * W)\n\n\t# get position of resized image\n\ty = np.arange(aH).repeat(aW).reshape(aW, -1)\n\tx = np.tile(np.arange(aW), (aH, 1))\n\n\t# get position of original position\n\ty = (y / ay)\n\tx = (x / ax)\n\n\tix = np.floor(x).astype(np.int)\n\tiy = np.floor(y).astype(np.int)\n\n\tix = np.minimum(ix, W-2)\n\tiy = np.minimum(iy, H-2)\n\n\t# get distance \n\tdx = x - ix\n\tdy = y - iy\n\n\tdx = np.repeat(np.expand_dims(dx, axis=-1), 3, axis=-1)\n\tdy = np.repeat(np.expand_dims(dy, axis=-1), 3, axis=-1)\n\n\t# interpolation\n\tout = (1-dx) * (1-dy) * img[iy, ix] + dx * (1 - dy) * img[iy, ix+1] + (1 - dx) * dy * img[iy+1, ix] + dx * dy * img[iy+1, ix+1]\n\n\tout = np.clip(out, 0, 255)\n\tout = out.astype(np.uint8)\n\n\treturn out\n\n\n# Read image\nimg = cv2.imread(\"imori.jpg\").astype(np.float)\n\n# Bilinear interpolation\nout = bl_interpolate(img, ax=1.5, ay=1.5)\n\n# Save result\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)\ncv2.imwrite(\"out.jpg\", out)"}], "task": "code_generation", "index": 225, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_226", "instruction": "pythonを用いて、Bi-cubic補間により画像を1.5倍に拡大せよ。\n\nBi-cubic補間とはBi-linear補間の拡張であり、周辺の16画素から補間を行う。\n\nI(x-1,y-1)  I(x,y-1)  I(x+1,y-1)  I(x+2,y-1)\nI(x-1,y)    I(x,y)    I(x+1,y)    I(x+2,y)\nI(x-1,y+1)  I(x,y+1)  I(x+1,y+1)  I(x+2,y+1)\nI(x-1,y+2)  I(x,y+2)  I(x+1,y+2)  I(x+2,y+2)\n\nそれぞれの画素との距離は次式の様に決定される。\n\ndx1 = x'/a - (x-1) , dx2 = x'/a - x , dx3 = (x+1) - x'/a , dx4 = (x+2) - x'/a\ndy1 = y'/a - (y-1) , dy2 = y'/a - y , dy3 = (y+1) - y'/a , dy4 = (y+2) - y'/a\n\n重みは距離によって次の関数により決定される。\na は多くの場合-1をとる。だいたい図の青色のピクセルは距離|t|<=1、緑色が1<|t|<=2の重みとなる。\n\nh(t) = { (a+2)|t|^3 - (a+3)|t|^2 + 1    (when |t|<=1)\n         a|t|^3 - 5a|t|^2 + 8a|t| - 4a  (when 1<|t|<=2)\n         0                              (when 2<|t|) \n\nこれら画素と重みを用いて、次式で拡大画像の画素が計算される。\nそれぞれの画素と重みを掛けた和を重みの和で割る。\n\nI'(x', y') = (Sum{i=-1:2}{j=-1:2} I(x+i,y+j) * wxi * wyj) / Sum{i=-1:2}{j=-1:2} wxi * wyj\n", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n# Bi-cubic interpolation\ndef bc_interpolate(img, ax=1., ay=1.):\n\tH, W, C = img.shape\n\n\taH = int(ay * H)\n\taW = int(ax * W)\n\n\t# get positions of resized image\n\ty = np.arange(aH).repeat(aW).reshape(aW, -1)\n\tx = np.tile(np.arange(aW), (aH, 1))\n\ty = (y / ay)\n\tx = (x / ax)\n\n\t# get positions of original image\n\tix = np.floor(x).astype(np.int)\n\tiy = np.floor(y).astype(np.int)\n\n\tix = np.minimum(ix, W-1)\n\tiy = np.minimum(iy, H-1)\n\n\t# get distance of each position of original image\n\tdx2 = x - ix\n\tdy2 = y - iy\n\tdx1 = dx2 + 1\n\tdy1 = dy2 + 1\n\tdx3 = 1 - dx2\n\tdy3 = 1 - dy2\n\tdx4 = 1 + dx3\n\tdy4 = 1 + dy3\n\n\tdxs = [dx1, dx2, dx3, dx4]\n\tdys = [dy1, dy2, dy3, dy4]\n\n\t# bi-cubic weight\n\tdef weight(t):\n\t\ta = -1.\n\t\tat = np.abs(t)\n\t\tw = np.zeros_like(t)\n\t\tind = np.where(at <= 1)\n\t\tw[ind] = ((a+2) * np.power(at, 3) - (a+3) * np.power(at, 2) + 1)[ind]\n\t\tind = np.where((at > 1) & (at <= 2))\n\t\tw[ind] = (a*np.power(at, 3) - 5*a*np.power(at, 2) + 8*a*at - 4*a)[ind]\n\t\treturn w\n\n\tw_sum = np.zeros((aH, aW, C), dtype=np.float32)\n\tout = np.zeros((aH, aW, C), dtype=np.float32)\n\n\t# interpolate\n\tfor j in range(-1, 3):\n\t\tfor i in range(-1, 3):\n\t\t\tind_x = np.minimum(np.maximum(ix + i, 0), W-1)\n\t\t\tind_y = np.minimum(np.maximum(iy + j, 0), H-1)\n\n\t\t\twx = weight(dxs[i+1])\n\t\t\twy = weight(dys[j+1])\n\t\t\twx = np.repeat(np.expand_dims(wx, axis=-1), 3, axis=-1)\n\t\t\twy = np.repeat(np.expand_dims(wy, axis=-1), 3, axis=-1)\n\n\t\t\tw_sum += wx * wy\n\t\t\tout += wx * wy * img[ind_y, ind_x]\n\n\tout /= w_sum\n\tout = np.clip(out, 0, 255)\n\tout = out.astype(np.uint8)\n\n\treturn out\n\n\n# Read image\nimg = cv2.imread(\"imori.jpg\").astype(np.float32)\n\n# Bi-cubic interpolation\nout = bc_interpolate(img, ax=1.5, ay=1.5)\n\n# Save result\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)\ncv2.imwrite(\"out.jpg\", out)"}], "task": "code_generation", "index": 226, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_227", "instruction": "pythonを用いて、アフィン変換を利用して画像をx方向に+30、y方向に-30だけ平行移動させよ。\n\nアフィン変換とは3x3の行列を用いて画像の変換を行う操作である。\n\n変換は(1)平行移動(Q.28) (2)拡大縮小(Q.29) (3)回転(Q.30) (4)スキュー(Q.31) がある。\n\n元画像を(x,y)、変換後の画像を(x',y')とする。\n画像の拡大縮小は、次式で表される。\n\n[ x' ] = [a b][x]\n  y'      c d  y\n\n一方、平行移動は次式となる。\n\n[ x' ] = [x] + [tx]\n  y'      y  +  ty\n\n以上を一つの式にまとめると、次式になり、これがアフィン変換である。\n\n  x'       a b tx    x\n[ y' ] = [ c d ty ][ y ]\n  1        0 0  1    1\n\nしかし実装する時は、元画像に対して１ピクセルずつ行うと、処理後の画像で値が割り当てられない可能性がでてきてしまう。よって、処理後画像の各ピクセルに対してAffine変換の逆変換を行い、値をあ割り当てる元画像の座標を取得する必要がある。Affine変換の逆操作は次式となる。\n\n今回の平行移動では次式を用いる。tx, tyが平行移動のピクセルの移動距離となる。\n\n\n\n  x'       1 0 tx    x\n[ y' ] = [ 0 1 ty ][ y ]\n  1        0 0  1    1\n\n", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n# Affine\ndef affine(img, a, b, c, d, tx, ty):\n  \tH, W, C = img.shape\n\n\t# temporary image\n\timg = np.zeros((H+2, W+2, C), dtype=np.float32)\n\timg[1:H+1, 1:W+1] = _img\n\n\t# get new image shape\n\tH_new = np.round(H * d).astype(np.int)\n\tW_new = np.round(W * a).astype(np.int)\n\tout = np.zeros((H_new+1, W_new+1, C), dtype=np.float32)\n\n\t# get position of new image\n\tx_new = np.tile(np.arange(W_new), (H_new, 1))\n\ty_new = np.arange(H_new).repeat(W_new).reshape(H_new, -1)\n\n\t# get position of original image by affine\n\tadbc = a * d - b * c\n\tx = np.round((d * x_new  - b * y_new) / adbc).astype(np.int) - tx + 1\n\ty = np.round((-c * x_new + a * y_new) / adbc).astype(np.int) - ty + 1\n\n\tx = np.minimum(np.maximum(x, 0), W+1).astype(np.int)\n\ty = np.minimum(np.maximum(y, 0), H+1).astype(np.int)\n\n\t# assgin pixcel to new image\n\tout[y_new, x_new] = img[y, x]\n\n\tout = out[:H_new, :W_new]\n\tout = out.astype(np.uint8)\n\n\treturn out\n\n\n# Read image\nimg = cv2.imread(\"imori.jpg\").astype(np.float32)\n\n# Affine\nout = affine(img, a=1, b=0, c=0, d=1, tx=30, ty=-30)\n\n# Save result\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)\ncv2.imwrite(\"out.jpg\", out)"}], "task": "code_generation", "index": 227, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_228", "instruction": "pythonを用いて、アフィン変換を用いて、(1)x方向に1.3倍、y方向に0.8倍にリサイズせよ。\nまた、(2)  (1)の条件に加えて、x方向に+30、y方向に-30だけ平行移動を同時に実現せよ。", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n# Affine\ndef affine(img, a, b, c, d, tx, ty):\n  \tH, W, C = img.shape\n\n\t# temporary image\n\timg = np.zeros((H+2, W+2, C), dtype=np.float32)\n\timg[1:H+1, 1:W+1] = _img\n\n\t# get new image shape\n\tH_new = np.round(H * d).astype(np.int)\n\tW_new = np.round(W * a).astype(np.int)\n\tout = np.zeros((H_new+1, W_new+1, C), dtype=np.float32)\n\n\t# get position of new image\n\tx_new = np.tile(np.arange(W_new), (H_new, 1))\n\ty_new = np.arange(H_new).repeat(W_new).reshape(H_new, -1)\n\n\t# get position of original image by affine\n\tadbc = a * d - b * c\n\tx = np.round((d * x_new  - b * y_new) / adbc).astype(np.int) - tx + 1\n\ty = np.round((-c * x_new + a * y_new) / adbc).astype(np.int) - ty + 1\n\n\tx = np.minimum(np.maximum(x, 0), W+1).astype(np.int)\n\ty = np.minimum(np.maximum(y, 0), H+1).astype(np.int)\n\n\t# assgin pixcel to new image\n\tout[y_new, x_new] = img[y, x]\n\n\tout = out[:H_new, :W_new]\n\tout = out.astype(np.uint8)\n\n\treturn out\n\n\n# Read image\n_img = cv2.imread(\"imori.jpg\").astype(np.float32)\n\n# Affine\nout = affine(img, a=1.3, b=0, c=0, d=0.8, tx=30, ty=-30)\n\n\n# Save result\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)\ncv2.imwrite(\"out.jpg\", out)"}], "task": "code_generation", "index": 228, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_229", "instruction": "pythonを用いて、\n\n(1)アフィン変換を用いて、反時計方向に30度回転させよ。\n\n(2) アフィン変換を用いて、反時計方向に30度回転した画像で中心座標を固定することで、なるべく黒い領域がなくなるように画像を作成せよ。\n（ただし、単純なアフィン変換を行うと画像が切れてしまうので、工夫を要する。）\n\nアフィン変換において、反時計方向にA度回転させる時は、次式となる。\n\n  x'       cosA -sinA tx    x\n[ y' ] = [ sinA  cosA ty ][ y ]\n  1         0     0    1    1", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n# affine\ndef affine(img, a, b, c, d, tx, ty):\n\tH, W, C = _img.shape\n\n\t# temporary image\n\timg = np.zeros((H+2, W+2, C), dtype=np.float32)\n\timg[1:H+1, 1:W+1] = _img\n\n\t# get shape of new image\n\tH_new = np.round(H).astype(np.int)\n\tW_new = np.round(W).astype(np.int)\n\tout = np.zeros((H_new, W_new, C), dtype=np.float32)\n\n\t# get position of new image\n\tx_new = np.tile(np.arange(W_new), (H_new, 1))\n\ty_new = np.arange(H_new).repeat(W_new).reshape(H_new, -1)\n\n\t# get position of original image by affine\n\tadbc = a * d - b * c\n\tx = np.round((d * x_new  - b * y_new) / adbc).astype(np.int) - tx + 1\n\ty = np.round((-c * x_new + a * y_new) / adbc).astype(np.int) - ty + 1\n\n\t# adjust center by affine\n\tdcx = (x.max() + x.min()) // 2 - W // 2\n\tdcy = (y.max() + y.min()) // 2 - H // 2\n\n\tx -= dcx\n\ty -= dcy\n\n\tx = np.clip(x, 0, W + 1)\n\ty = np.clip(y, 0, H + 1)\n\n\t# assign pixcel\n\tout[y_new, x_new] = img[y, x]\n\tout = out.astype(np.uint8)\n\n\treturn out\n\n# Read image\n_img = cv2.imread(\"imori.jpg\").astype(np.float32)\n\n\n# Affine\nA = 30.\ntheta = - np.pi * A / 180.\n\nout = affine(img, a=np.cos(theta), b=-np.sin(theta), c=np.sin(theta), d=np.cos(theta),\n tx=0, ty=0)\n\n\n# Save result\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)\ncv2.imwrite(\"out.jpg\", out)"}], "task": "code_generation", "index": 229, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_230", "instruction": "pythonを用いて、\n\n(1)アフィン変換を用いて、出力(1)のようなX-sharing(dx = 30)画像を作成せよ。\n\n(2)アフィン変換を用いて、出力2のようなY-sharing(dy = 30)画像を作成せよ。\n\n(3)アフィン変換を用いて、出力3のような幾何変換した(dx = 30, dy = 30)画像を作成せよ。\n\nこのような画像はスキュー画像と呼ばれ、画像を斜め方向に伸ばした画像である。\n\n出力(1)の場合、x方向にdxだけ引き伸ばした画像はX-sharingと呼ばれる。\n\n出力(2)の場合、y方向にdyだけ引き伸ばした画像はY-sharingと呼ばれる。\n\nそれぞれ次式のアフィン変換で実現できる。\nただし、元画像のサイズがh x wとする。\n\n(1) X-sharing  \n   a = dx / h\n\n  x'       1 a tx    x \n[ y' ] = [ 0 1 ty ][ y ] \n  1        0 0  1    1  \n\n\n(2) Y-sharing\n a = dy / w\n\nx'       1 0 tx    x\n[ y' ] = [ a 1 ty ][ y ]\n1        0 0  1    1", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Affine\ndef affine(img, dx=30, dy=30):\n    # get shape\n    H, W, C = img.shape\n\n    # Affine hyper parameters\n    a = 1.\n    b = dx / H\n    c = dy / W\n    d = 1.\n    tx = 0.\n    ty = 0.\n\n    # prepare temporary\n    _img = np.zeros((H+2, W+2, C), dtype=np.float32)\n\n    # insert image to center of temporary\n    _img[1:H+1, 1:W+1] = img\n\n    # prepare affine image temporary\n    H_new = np.ceil(dy + H).astype(np.int)\n    W_new = np.ceil(dx + W).astype(np.int)\n    out = np.zeros((H_new, W_new, C), dtype=np.float32)\n\n    # preprare assigned index\n    x_new = np.tile(np.arange(W_new), (H_new, 1))\n    y_new = np.arange(H_new).repeat(W_new).reshape(H_new, -1)\n\n    # prepare inverse matrix for affine\n    adbc = a * d - b * c\n    x = np.round((d * x_new  - b * y_new) / adbc).astype(np.int) - tx + 1\n    y = np.round((-c * x_new + a * y_new) / adbc).astype(np.int) - ty + 1\n\n    x = np.minimum(np.maximum(x, 0), W+1).astype(np.int)\n    y = np.minimum(np.maximum(y, 0), H+1).astype(np.int)\n\n    # assign value from original to affine image\n    out[y_new, x_new] = _img[y, x]\n    out = out.astype(np.uint8)\n\n    return out\n\n\n# Read image\nimg = cv2.imread(\"imori.jpg\").astype(np.float32)\n\n# Affine\nout = affine(img, dx=30, dy=30)\n\n# Save result\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)\ncv2.imwrite(\"out.jpg\", out)"}], "task": "code_generation", "index": 230, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_231", "instruction": "pythonを用いて、二次元離散フーリエ変換(DFT)を実装し、imori.jpgをグレースケール化したものの周波数のパワースペクトルを表示せよ。\nまた、逆二次元離散フーリエ変換(IDFT)で画像を復元せよ。\n\n二次元離散フーリエ変換(DFT: Discrete Fourier Transformation)とはフーリエ変換の画像に対する処理方法である。\n\n通常のフーリエ変換はアナログ信号や音声などの連続値かつ一次元を対象に周波数成分を求める計算処理である。\n\n一方、ディジタル画像は[0,255]の離散値をとり、かつ画像はHxWの二次元表示であるので、二次元離散フーリエ変換が行われる。\n\n二次元離散フーリエ変換(DFT)は次式で計算される。\n\nK = 0:W, l = 0:H, 入力画像をI として\nG(k,l) = Sum_{y=0:H-1, x=0:W-1} I(x,y) exp( -2pi * j * (kx/W + ly/H)) / sqrt(H * W)\n\nK = [0, W-1], l = [0, H-1], 入力画像を I として\n\nここでは画像をグレースケール化してから二次元離散フーリエ変換を行え。\n\nパワースペクトルとは Gは複素数で表されるので、Gの絶対値を求めることである。\n今回のみ画像表示の時はパワースペクトルは[0,255]にスケーリングせよ。\n\n逆二次元離散フーリエ変換(IDFT: Inverse DFT)とは周波数成分Gから元の画像を復元する手法であり、次式で定義される。\n\nx = 0:W, y = 0:H  として\nI(x,y) = Sum_{l=0:H-1, k=0:W-1} G(k,l) exp( 2pi * j * (kx/W + ly/H)) / sqrt(H * W)\n\nx = [0, W-1], y = [0, H-1] として\n\n上が定義式ですがexp(j)は複素数の値をとってしまうので、実際にコードにするときはぜ下式のように絶対値を使います。\n\nシンプルに全部for文で回すと128^4の計算になるので、時間がかかってしまいます。numpyをうまく活用すれば計算コストを減らすことができます。（解答は128^2まで減らしました。）", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n# DFT hyper-parameters\nK, L = 128, 128\nchannel = 3\n\n\n# DFT\ndef dft(img):\n\tH, W, _ = img.shape\n\n\t# Prepare DFT coefficient\n\tG = np.zeros((L, K, channel), dtype=np.complex)\n\n\t# prepare processed index corresponding to original image positions\n\tx = np.tile(np.arange(W), (H, 1))\n\ty = np.arange(H).repeat(W).reshape(H, -1)\n\n\t# dft\n\tfor c in range(channel):\n\t\tfor l in range(L):\n\t\t\tfor k in range(K):\n\t\t\t\tG[l, k, c] = np.sum(img[..., c] * np.exp(-2j * np.pi * (x * k / K + y * l / L))) / np.sqrt(K * L)\n\t\t\t\t#for n in range(N):\n\t\t\t\t#    for m in range(M):\n\t\t\t\t#        v += gray[n, m] * np.exp(-2j * np.pi * (m * k / M + n * l / N))\n\t\t\t\t#G[l, k] = v / np.sqrt(M * N)\n\n\treturn G\n\n# IDFT\ndef idft(G):\n\t# prepare out image\n\tH, W, _ = G.shape\n\tout = np.zeros((H, W, channel), dtype=np.float32)\n\n\t# prepare processed index corresponding to original image positions\n\tx = np.tile(np.arange(W), (H, 1))\n\ty = np.arange(H).repeat(W).reshape(H, -1)\n\n\t# idft\n\tfor c in range(channel):\n\t\tfor l in range(H):\n\t\t\tfor k in range(W):\n\t\t\t\tout[l, k, c] = np.abs(np.sum(G[..., c] * np.exp(2j * np.pi * (x * k / W + y * l / H)))) / np.sqrt(W * H)\n\n\t# clipping\n\tout = np.clip(out, 0, 255)\n\tout = out.astype(np.uint8)\n\n\treturn out\n\n\n\n# Read image\nimg = cv2.imread(\"imori.jpg\").astype(np.float32)\n\n# DFT\nG = dft(img)\n\n# write poser spectal to image\nps = (np.abs(G) / np.abs(G).max() * 255).astype(np.uint8)\ncv2.imwrite(\"out_ps.jpg\", ps)\n\n# IDFT\nout = idft(G)\n\n# Save result\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)\ncv2.imwrite(\"out.jpg\", out)\n\n\n\n\"\"\"\nfimg = np.fft.fft2(gray)\n    \n# 第1象限と第3象限, 第2象限と第4象限を入れ替え\nfimg =  np.fft.fftshift(fimg)\nprint(fimg.shape)\n# パワースペクトルの計算\nmag = 20*np.log(np.abs(fimg))\n    \n# 入力画像とスペクトル画像をグラフ描画\nplt.subplot(121)\nplt.imshow(gray, cmap = 'gray')\nplt.subplot(122)\nplt.imshow(mag, cmap = 'gray')\nplt.show()\n\"\"\""}], "task": "code_generation", "index": 231, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_232", "instruction": "pythonを用いて、imori.jpgをグレースケール化したものをDFTし、ローパスフィルタを通してIDFTで画像を復元せよ。\n\nDFTによって得られた周波数成分は左上、右上、左下、右下に近いほど低周波数の成分を含んでいることになり、中心に近いほど高周波成分を示す。\n\n![](assets/lpf.png)\n\n画像における高周波成分とは色が変わっている部分（ノイズや輪郭など）を示し、低周波成分とは色があまり変わっていない部分（夕日のグラデーションなど）を表す。\nここでは、高周波成分をカットし、低周波成分のみを通すローパスフィルタを実装せよ。\n\nここでは低周波数の中心から高周波までの距離をrとすると0.5rまでの成分を通すとする。", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n# DFT hyper-parameters\nK, L = 128, 128\nchannel = 3\n\n# bgr -> gray\ndef bgr2gray(img):\n\tgray = 0.2126 * img[..., 2] + 0.7152 * img[..., 1] + 0.0722 * img[..., 0]\n\treturn gray\n\n\n# DFT\ndef dft(img):\n\t# Prepare DFT coefficient\n\tG = np.zeros((L, K, channel), dtype=np.complex)\n\n\t# prepare processed index corresponding to original image positions\n\tx = np.tile(np.arange(W), (H, 1))\n\ty = np.arange(H).repeat(W).reshape(H, -1)\n\n\t# dft\n\tfor c in range(channel):\n\t\tfor l in range(L):\n\t\t\tfor k in range(K):\n\t\t\t\tG[l, k, c] = np.sum(img[..., c] * np.exp(-2j * np.pi * (x * k / K + y * l / L))) / np.sqrt(K * L)\n\t\t\t\t#for n in range(N):\n\t\t\t\t#    for m in range(M):\n\t\t\t\t#        v += gray[n, m] * np.exp(-2j * np.pi * (m * k / M + n * l / N))\n\t\t\t\t#G[l, k] = v / np.sqrt(M * N)\n\n\treturn G\n\n# IDFT\ndef idft(G):\n\t# prepare out image\n\tH, W, _ = G.shape\n\tout = np.zeros((H, W, channel), dtype=np.float32)\n\n\t# prepare processed index corresponding to original image positions\n\tx = np.tile(np.arange(W), (H, 1))\n\ty = np.arange(H).repeat(W).reshape(H, -1)\n\n\t# idft\n\tfor c in range(channel):\n\t\tfor l in range(H):\n\t\t\tfor k in range(W):\n\t\t\t\tout[l, k, c] = np.abs(np.sum(G[..., c] * np.exp(2j * np.pi * (x * k / W + y * l / H)))) / np.sqrt(W * H)\n\n\t# clipping\n\tout = np.clip(out, 0, 255)\n\tout = out.astype(np.uint8)\n\n\treturn out\n\n\n# LPF\ndef lpf(G, ratio=0.5):\n\tH, W, _ = G.shape\t\n\n\t# transfer positions\n\t_G = np.zeros_like(G)\n\t_G[:H//2, :W//2] = G[H//2:, W//2:]\n\t_G[:H//2, W//2:] = G[H//2:, :W//2]\n\t_G[H//2:, :W//2] = G[:H//2, W//2:]\n\t_G[H//2:, W//2:] = G[:H//2, :W//2]\n\n\t# get distance from center (H / 2, W / 2)\n\tx = np.tile(np.arange(W), (H, 1))\n\ty = np.arange(H).repeat(W).reshape(H, -1)\n\n\t# make filter\n\t_x = x - W // 2\n\t_y = y - H // 2\n\tr = np.sqrt(_x ** 2 + _y ** 2)\n\tmask = np.ones((H, W), dtype=np.float32)\n\tmask[r > (W // 2 * ratio)] = 0\n\n\tmask = np.repeat(mask, channel).reshape(H, W, channel)\n\n\t# filtering\n\t_G *= mask\n\n\t# reverse original positions\n\tG[:H//2, :W//2] = _G[H//2:, W//2:]\n\tG[:H//2, W//2:] = _G[H//2:, :W//2]\n\tG[H//2:, :W//2] = _G[:H//2, W//2:]\n\tG[H//2:, W//2:] = _G[:H//2, :W//2]\n\n\treturn G\n\n\n# Read image\nimg = cv2.imread(\"imori.jpg\").astype(np.float32)\nH, W, C = img.shape\n\n# Gray scale\ngray = bgr2gray(img)\n\n# DFT\nG = dft(img)\n\n# LPF\nG = lpf(G)\n\n# IDFT\nout = idft(G)\n\n# Save result\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)\ncv2.imwrite(\"out.jpg\", out)"}], "task": "code_generation", "index": 232, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_233", "instruction": "pythonを用いて、imori.jpgをグレースケール化したものをDFTし、ハイパスフィルタを通してIDFTで画像を復元せよ。\n\nここでは、低周波成分をカットし、高周波成分のみを通すハイパスフィルタを実装せよ。\n\nここでは低周波数の中心から高周波までの距離をrとすると0.1rからの成分を通すとする。", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n# DFT hyper-parameters\nK, L = 128, 128\nchannel = 3\n\n# bgr -> gray\ndef bgr2gray(img):\n\tgray = 0.2126 * img[..., 2] + 0.7152 * img[..., 1] + 0.0722 * img[..., 0]\n\treturn gray\n\n\n# DFT\ndef dft(img):\n\t# Prepare DFT coefficient\n\tG = np.zeros((L, K, channel), dtype=np.complex)\n\n\t# prepare processed index corresponding to original image positions\n\tx = np.tile(np.arange(W), (H, 1))\n\ty = np.arange(H).repeat(W).reshape(H, -1)\n\n\t# dft\n\tfor c in range(channel):\n\t\tfor l in range(L):\n\t\t\tfor k in range(K):\n\t\t\t\tG[l, k, c] = np.sum(img[..., c] * np.exp(-2j * np.pi * (x * k / K + y * l / L))) / np.sqrt(K * L)\n\t\t\t\t#for n in range(N):\n\t\t\t\t#    for m in range(M):\n\t\t\t\t#        v += gray[n, m] * np.exp(-2j * np.pi * (m * k / M + n * l / N))\n\t\t\t\t#G[l, k] = v / np.sqrt(M * N)\n\n\treturn G\n\n# IDFT\ndef idft(G):\n\t# prepare out image\n\tH, W, _ = G.shape\n\tout = np.zeros((H, W, channel), dtype=np.float32)\n\n\t# prepare processed index corresponding to original image positions\n\tx = np.tile(np.arange(W), (H, 1))\n\ty = np.arange(H).repeat(W).reshape(H, -1)\n\n\t# idft\n\tfor c in range(channel):\n\t\tfor l in range(H):\n\t\t\tfor k in range(W):\n\t\t\t\tout[l, k, c] = np.abs(np.sum(G[..., c] * np.exp(2j * np.pi * (x * k / W + y * l / H)))) / np.sqrt(W * H)\n\n\t# clipping\n\tout = np.clip(out, 0, 255)\n\tout = out.astype(np.uint8)\n\n\treturn out\n\n\n# HPF\ndef hpf(G, ratio=0.1):\n\tH, W, _ = G.shape\t\n\n\t# transfer positions\n\t_G = np.zeros_like(G)\n\t_G[:H//2, :W//2] = G[H//2:, W//2:]\n\t_G[:H//2, W//2:] = G[H//2:, :W//2]\n\t_G[H//2:, :W//2] = G[:H//2, W//2:]\n\t_G[H//2:, W//2:] = G[:H//2, :W//2]\n\n\t# get distance from center (H / 2, W / 2)\n\tx = np.tile(np.arange(W), (H, 1))\n\ty = np.arange(H).repeat(W).reshape(H, -1)\n\n\t# make filter\n\t_x = x - W // 2\n\t_y = y - H // 2\n\tr = np.sqrt(_x ** 2 + _y ** 2)\n\tmask = np.ones((H, W), dtype=np.float32)\n\tmask[r < (W // 2 * ratio)] = 0\n\n\tmask = np.repeat(mask, channel).reshape(H, W, channel)\n\n\t# filtering\n\t_G *= mask\n\n\t# reverse original positions\n\tG[:H//2, :W//2] = _G[H//2:, W//2:]\n\tG[:H//2, W//2:] = _G[H//2:, :W//2]\n\tG[H//2:, :W//2] = _G[:H//2, W//2:]\n\tG[H//2:, W//2:] = _G[:H//2, :W//2]\n\n\treturn G\n\n\n# Read image\nimg = cv2.imread(\"imori.jpg\").astype(np.float32)\nH, W, C = img.shape\n\n# Gray scale\ngray = bgr2gray(img)\n\n# DFT\nG = dft(img)\n\n# HPF\nG = hpf(G)\n\n# IDFT\nout = idft(G)\n\n# Save result\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)\ncv2.imwrite(\"out.jpg\", out)"}], "task": "code_generation", "index": 233, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_234", "instruction": "pythonを用いて、imori.jpgをグレースケール化したものをDFTし、ハイパスフィルタを通してIDFTで画像を復元せよ。\n\nここでは、低周波成分と高周波成分の中間の周波数成分のみを通すハイパスフィルタを実装せよ。\n\nここでは低周波数の中心から高周波までの距離をrとすると0.1rから0.5rまでの成分を通すとする。", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n# DFT hyper-parameters\nK, L = 128, 128\nchannel = 3\n\n# bgr -> gray\ndef bgr2gray(img):\n\tgray = 0.2126 * img[..., 2] + 0.7152 * img[..., 1] + 0.0722 * img[..., 0]\n\treturn gray\n\n\n# DFT\ndef dft(img):\n\t# Prepare DFT coefficient\n\tG = np.zeros((L, K, channel), dtype=np.complex)\n\n\t# prepare processed index corresponding to original image positions\n\tx = np.tile(np.arange(W), (H, 1))\n\ty = np.arange(H).repeat(W).reshape(H, -1)\n\n\t# dft\n\tfor c in range(channel):\n\t\tfor l in range(L):\n\t\t\tfor k in range(K):\n\t\t\t\tG[l, k, c] = np.sum(img[..., c] * np.exp(-2j * np.pi * (x * k / K + y * l / L))) / np.sqrt(K * L)\n\t\t\t\t#for n in range(N):\n\t\t\t\t#    for m in range(M):\n\t\t\t\t#        v += gray[n, m] * np.exp(-2j * np.pi * (m * k / M + n * l / N))\n\t\t\t\t#G[l, k] = v / np.sqrt(M * N)\n\n\treturn G\n\n# IDFT\ndef idft(G):\n\t# prepare out image\n\tH, W, _ = G.shape\n\tout = np.zeros((H, W, channel), dtype=np.float32)\n\n\t# prepare processed index corresponding to original image positions\n\tx = np.tile(np.arange(W), (H, 1))\n\ty = np.arange(H).repeat(W).reshape(H, -1)\n\n\t# idft\n\tfor c in range(channel):\n\t\tfor l in range(H):\n\t\t\tfor k in range(W):\n\t\t\t\tout[l, k, c] = np.abs(np.sum(G[..., c] * np.exp(2j * np.pi * (x * k / W + y * l / H)))) / np.sqrt(W * H)\n\n\t# clipping\n\tout = np.clip(out, 0, 255)\n\tout = out.astype(np.uint8)\n\n\treturn out\n\n\n# BPF\ndef bpf(G, ratio1=0.1, ratio2=0.5):\n\tH, W, _ = G.shape\t\n\n\t# transfer positions\n\t_G = np.zeros_like(G)\n\t_G[:H//2, :W//2] = G[H//2:, W//2:]\n\t_G[:H//2, W//2:] = G[H//2:, :W//2]\n\t_G[H//2:, :W//2] = G[:H//2, W//2:]\n\t_G[H//2:, W//2:] = G[:H//2, :W//2]\n\n\t# get distance from center (H / 2, W / 2)\n\tx = np.tile(np.arange(W), (H, 1))\n\ty = np.arange(H).repeat(W).reshape(H, -1)\n\n\t# make filter\n\t_x = x - W // 2\n\t_y = y - H // 2\n\tr = np.sqrt(_x ** 2 + _y ** 2)\n\tmask = np.ones((H, W), dtype=np.float32)\n\tmask[(r < (W // 2 * ratio1)) | (r > (W // 2 * ratio2))] = 0\n\n\tmask = np.repeat(mask, channel).reshape(H, W, channel)\n\n\t# filtering\n\t_G *= mask\n\n\t# reverse original positions\n\tG[:H//2, :W//2] = _G[H//2:, W//2:]\n\tG[:H//2, W//2:] = _G[H//2:, :W//2]\n\tG[H//2:, :W//2] = _G[:H//2, W//2:]\n\tG[H//2:, W//2:] = _G[:H//2, :W//2]\n\n\treturn G\n\n\n# Read image\nimg = cv2.imread(\"imori.jpg\").astype(np.float32)\nH, W, C = img.shape\n\n# Gray scale\ngray = bgr2gray(img)\n\n# DFT\nG = dft(img)\n\n# BPF\nG = bpf(G, ratio1=0.1, ratio2=0.5)\n\n# IDFT\nout = idft(G)\n\n# Save result\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)\ncv2.imwrite(\"out.jpg\", out)"}], "task": "code_generation", "index": 234, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_235", "instruction": "pythonを用いて、imori.jpgをグレースケール化し離散コサイン変換を行い、逆離散コサイン変換を行え。\n\n離散コサイン変換(DCT: Discrete Cosine Transformation)とは、次式で定義される周波数変換の一つである。\n\nT = 8\nF(u,v) = 2 / T * C(u)C(v) * Sum_{y=0:T-1} Sum_{x=0:T-1} f(x,y) cos((2x+1)u*pi/2T) cos((2y+1)v*pi/2T)\n\n逆離散コサイン変換(IDCT: Inverse Discrete Cosine Transformation)とは離散コサイン変換の逆（復号）であり、次式で定義される。\nここでいう K は復元時にどれだけ解像度を良くするかを決定するパラメータである。\nK = Tの時は、DCT係数を全部使うのでIDCT後の解像度は最大になるが、Kが１や２などの時は復元に使う情報量（DCT係数）が減るので解像度が下がる。これを適度に設定することで、画像の容量を減らすことができる。\n\nT = 8\nK = 8\nf(x,y) = 2 / T * C(x)C(y) * Sum_{u=0:K-1} Sum_{v=0:K-1} F(u,v) cos((2x+1)u*pi/2T) cos((2y+1)v*pi/2T)\n\nここでは画像を8x8ずつの領域に分割して、各領域で以上のDCT, IDCTを繰り返すことで、jpeg符号に応用される。\n今回も同様に8x8の領域に分割して、DCT, IDCTを行え。", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# DCT hyoer-parameter\nT = 8\nK = 8\nchannel = 3\n\n# DCT weight\ndef w(x, y, u, v):\n    cu = 1.\n    cv = 1.\n    if u == 0:\n        cu /= np.sqrt(2)\n    if v == 0:\n        cv /= np.sqrt(2)\n    theta = np.pi / (2 * T)\n    return (( 2 * cu * cv / T) * np.cos((2*x+1)*u*theta) * np.cos((2*y+1)*v*theta))\n\n# DCT\ndef dct(img):\n    H, W, _ = img.shape\n\n    F = np.zeros((H, W, channel), dtype=np.float32)\n\n    for c in range(channel):\n        for yi in range(0, H, T):\n            for xi in range(0, W, T):\n                for v in range(T):\n                    for u in range(T):\n                        for y in range(T):\n                            for x in range(T):\n                                F[v+yi, u+xi, c] += img[y+yi, x+xi, c] * w(x,y,u,v)\n\n    return F\n\n\n# IDCT\ndef idct(F):\n    H, W, _ = F.shape\n\n    out = np.zeros((H, W, channel), dtype=np.float32)\n\n    for c in range(channel):\n        for yi in range(0, H, T):\n            for xi in range(0, W, T):\n                for y in range(T):\n                    for x in range(T):\n                        for v in range(K):\n                            for u in range(K):\n                                out[y+yi, x+xi, c] += F[v+yi, u+xi, c] * w(x,y,u,v)\n\n    out = np.clip(out, 0, 255)\n    out = np.round(out).astype(np.uint8)\n\n    return out\n\n\n\n# Read image\nimg = cv2.imread(\"imori.jpg\").astype(np.float32)\n\n# DCT\nF = dct(img)\n\n# IDCT\nout = idct(F)\n\n# Save result\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)\ncv2.imwrite(\"out.jpg\", out)"}], "task": "code_generation", "index": 235, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_236", "instruction": "pythonを用いて、IDCTで用いるDCT係数を8でなく、4にすると画像の劣化が生じる。\n入力画像とIDCT画像のPSNRを求めよ。また、IDCTによるビットレートを求めよ。\n\nPSNR(Peak Signal to Noise Ratio)とは信号対雑音比と呼ばれ、画像がどれだけ劣化したかを示す。\n\nPSNRが大きいほど、画像が劣化していないことを示し、次式で定義される。\nMAXは取りうる値の最大値で[0,255]の表示なら MAX=255　となる。\nまた、MSEはMean Squared Error(平均二乗誤差)と呼ばれ、二つの画像の差分の二乗の平均値を示す。\n\nPSNR = 10 * log10(MAX^2 / MSE)\nMSE = Sum_{y=0:H-1} Sum_{x=0:W-1} (I1(x,y) - I2(x,y))^2 / (HW)\n\nビットレートとは8x8でDCTを行い、IDCTでKxKの係数までを用いた時に次式で定義される。\n\nbitrate = 8 * K^2 / 8^2", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# DCT hyoer-parameter\nT = 8\nK = 4\nchannel = 3\n\n# DCT weight\ndef w(x, y, u, v):\n    cu = 1.\n    cv = 1.\n    if u == 0:\n        cu /= np.sqrt(2)\n    if v == 0:\n        cv /= np.sqrt(2)\n    theta = np.pi / (2 * T)\n    return (( 2 * cu * cv / T) * np.cos((2*x+1)*u*theta) * np.cos((2*y+1)*v*theta))\n\n# DCT\ndef dct(img):\n    H, W, _ = img.shape\n\n    F = np.zeros((H, W, channel), dtype=np.float32)\n\n    for c in range(channel):\n        for yi in range(0, H, T):\n            for xi in range(0, W, T):\n                for v in range(T):\n                    for u in range(T):\n                        for y in range(T):\n                            for x in range(T):\n                                F[v+yi, u+xi, c] += img[y+yi, x+xi, c] * w(x,y,u,v)\n\n    return F\n\n\n# IDCT\ndef idct(F):\n    H, W, _ = F.shape\n\n    out = np.zeros((H, W, channel), dtype=np.float32)\n\n    for c in range(channel):\n        for yi in range(0, H, T):\n            for xi in range(0, W, T):\n                for y in range(T):\n                    for x in range(T):\n                        for v in range(K):\n                            for u in range(K):\n                                out[y+yi, x+xi, c] += F[v+yi, u+xi, c] * w(x,y,u,v)\n\n    out = np.clip(out, 0, 255)\n    out = np.round(out).astype(np.uint8)\n\n    return out\n\n\n# MSE\ndef MSE(img1, img2):\n    H, W, _ = img1.shape\n    mse = np.sum((img1 - img2) ** 2) / (H * W * channel)\n    return mse\n\n# PSNR\ndef PSNR(mse, vmax=255):\n    return 10 * np.log10(vmax * vmax / mse)\n\n# bitrate\ndef BITRATE():\n    return 1. * T * K * K / T / T\n\n\n# Read image\nimg = cv2.imread(\"imori.jpg\").astype(np.float32)\n\n# DCT\nF = dct(img)\n\n# IDCT\nout = idct(F)\n\n# MSE\nmse = MSE(img, out)\n\n# PSNR\npsnr = PSNR(mse)\n\n# bitrate\nbitrate = BITRATE()\n\nprint(\"MSE:\", mse)\nprint(\"PSNR:\", psnr)\nprint(\"bitrate:\", bitrate)\n\n# Save result\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)\ncv2.imwrite(\"out.jpg\", out)"}], "task": "code_generation", "index": 236, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_237", "instruction": "pythonを用いて、DCT係数を量子化し、IDCTで復元せよ。また、その時の画像の容量を比べよ。\n\nDCT係数を量子化することはjpeg画像にする符号化で用いられる手法である。\n\n量子化とは、値を予め決定された区分毎に値を大まかに丸め込む作業であり、floorやceil, roundなどが似た計算である。\n\nJPEG画像ではDCT係数を下記で表される量子化テーブルに則って量子化する。\nこの量子化テーブルはjpeg団体の仕様書から取った。\n量子化では8x8の係数をQで割り、四捨五入する。その後Qを掛けることで行われる。\nIDCTでは係数は全て用いるものとする。\n\nQ = np.array(((16, 11, 10, 16, 24, 40, 51, 61),\n              (12, 12, 14, 19, 26, 58, 60, 55),\n              (14, 13, 16, 24, 40, 57, 69, 56),\n              (14, 17, 22, 29, 51, 87, 80, 62),\n              (18, 22, 37, 56, 68, 109, 103, 77),\n              (24, 35, 55, 64, 81, 104, 113, 92),\n              (49, 64, 78, 87, 103, 121, 120, 101),\n              (72, 92, 95, 98, 112, 100, 103, 99)), dtype=np.float32)\n\n\n量子化を行うと画像の容量が減っていることから、データ量が削減されたことが伺える。", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# DCT hyoer-parameter\nT = 8\nK = 4\nchannel = 3\n\n# DCT weight\ndef DCT_w(x, y, u, v):\n    cu = 1.\n    cv = 1.\n    if u == 0:\n        cu /= np.sqrt(2)\n    if v == 0:\n        cv /= np.sqrt(2)\n    theta = np.pi / (2 * T)\n    return (( 2 * cu * cv / T) * np.cos((2*x+1)*u*theta) * np.cos((2*y+1)*v*theta))\n\n# DCT\ndef dct(img):\n    H, W, _ = img.shape\n\n    F = np.zeros((H, W, channel), dtype=np.float32)\n\n    for c in range(channel):\n        for yi in range(0, H, T):\n            for xi in range(0, W, T):\n                for v in range(T):\n                    for u in range(T):\n                        for y in range(T):\n                            for x in range(T):\n                                F[v+yi, u+xi, c] += img[y+yi, x+xi, c] * DCT_w(x,y,u,v)\n\n    return F\n\n\n# IDCT\ndef idct(F):\n    H, W, _ = F.shape\n\n    out = np.zeros((H, W, channel), dtype=np.float32)\n\n    for c in range(channel):\n        for yi in range(0, H, T):\n            for xi in range(0, W, T):\n                for y in range(T):\n                    for x in range(T):\n                        for v in range(K):\n                            for u in range(K):\n                                out[y+yi, x+xi, c] += F[v+yi, u+xi, c] * DCT_w(x,y,u,v)\n\n    out = np.clip(out, 0, 255)\n    out = np.round(out).astype(np.uint8)\n\n    return out\n\n# Quantization\ndef quantization(F):\n    H, W, _ = F.shape\n\n    Q = np.array(((16, 11, 10, 16, 24, 40, 51, 61),\n                (12, 12, 14, 19, 26, 58, 60, 55),\n                (14, 13, 16, 24, 40, 57, 69, 56),\n                (14, 17, 22, 29, 51, 87, 80, 62),\n                (18, 22, 37, 56, 68, 109, 103, 77),\n                (24, 35, 55, 64, 81, 104, 113, 92),\n                (49, 64, 78, 87, 103, 121, 120, 101),\n                (72, 92, 95, 98, 112, 100, 103, 99)), dtype=np.float32)\n\n    for ys in range(0, H, T):\n        for xs in range(0, W, T):\n            for c in range(channel):\n                F[ys: ys + T, xs: xs + T, c] =  np.round(F[ys: ys + T, xs: xs + T, c] / Q) * Q\n\n    return F\n\n\n\n# MSE\ndef MSE(img1, img2):\n    H, W, _ = img1.shape\n    mse = np.sum((img1 - img2) ** 2) / (H * W * channel)\n    return mse\n\n# PSNR\ndef PSNR(mse, vmax=255):\n    return 10 * np.log10(vmax * vmax / mse)\n\n# bitrate\ndef BITRATE():\n    return 1. * T * K * K / T / T\n\n\n# Read image\nimg = cv2.imread(\"imori.jpg\").astype(np.float32)\n\n# DCT\nF = dct(img)\n\n# quantization\nF = quantization(F)\n\n# IDCT\nout = idct(F)\n\n# MSE\nmse = MSE(img, out)\n\n# PSNR\npsnr = PSNR(mse)\n\n# bitrate\nbitrate = BITRATE()\n\nprint(\"MSE:\", mse)\nprint(\"PSNR:\", psnr)\nprint(\"bitrate:\", bitrate)\n\n# Save result\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)\ncv2.imwrite(\"out.jpg\", out)"}], "task": "code_generation", "index": 237, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_238", "instruction": "pythonを用いて、YCbCr表色形において、Yを0.7倍してコントラストを暗くせよ。\n\nYCbCr表色系とは、画像を明るさを表すY、輝度と青レベルの差Cb、輝度と赤レベルの差Crに分解する表現方法である。\n\nこれはJPEG変換で用いられる。\n\nRGBからYCbCrへの変換は次式。\n\nY = 0.299 * R + 0.5870 * G + 0.114 * B\nCb = -0.1687 * R - 0.3313 * G + 0.5 * B + 128\nCr = 0.5 * R - 0.4187 * G - 0.0813 * B + 128\n\nYCbCrからRGBへの変換は次式。\n\nR = Y + (Cr - 128) * 1.402\nG = Y - (Cb - 128) * 0.3441 - (Cr - 128) * 0.7139\nB = Y + (Cb - 128) * 1.7718", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nchannel = 3\n\n# BGR -> Y Cb Cr\ndef BGR2YCbCr(img):\n  H, W, _ = img.shape\n\n  ycbcr = np.zeros([H, W, 3], dtype=np.float32)\n\n  ycbcr[..., 0] = 0.2990 * img[..., 2] + 0.5870 * img[..., 1] + 0.1140 * img[..., 0]\n  ycbcr[..., 1] = -0.1687 * img[..., 2] - 0.3313 * img[..., 1] + 0.5 * img[..., 0] + 128.\n  ycbcr[..., 2] = 0.5 * img[..., 2] - 0.4187 * img[..., 1] - 0.0813 * img[..., 0] + 128.\n\n  return ycbcr\n\n# Y Cb Cr -> BGR\ndef YCbCr2BGR(ycbcr):\n  H, W, _ = ycbcr.shape\n\n  out = np.zeros([H, W, channel], dtype=np.float32)\n  out[..., 2] = ycbcr[..., 0] + (ycbcr[..., 2] - 128.) * 1.4020\n  out[..., 1] = ycbcr[..., 0] - (ycbcr[..., 1] - 128.) * 0.3441 - (ycbcr[..., 2] - 128.) * 0.7139\n  out[..., 0] = ycbcr[..., 0] + (ycbcr[..., 1] - 128.) * 1.7718\n\n  out = np.clip(out, 0, 255)\n  out = out.astype(np.uint8)\n\n  return out\n\n\n# Read image\nimg = cv2.imread(\"imori.jpg\").astype(np.float32)\n\n# bgr -> Y Cb Cr\nycbcr = BGR2YCbCr(img)\n\n# process\nycbcr[..., 0] *= 0.7\n\n# YCbCr > RGB\nout = YCbCr2BGR(ycbcr)\n\n# Save result\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)\ncv2.imwrite(\"out.jpg\", out)"}], "task": "code_generation", "index": 238, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_239", "instruction": "pythonを用いて、YCbCr表色系にし、DCT後、Yを量子化テーブルQ1、CbとCrをQ2で量子化し、IDCTで画像を復元せよ。また、画像の容量を比較せよ。\n\nアルゴリズムは、\n1. RGB を YCbCrに変換\n2. YCbCrをDCT\n3. DCTしたものを量子化\n4. 量子化したものをIDCT\n5. IDCTしたYCbCrをRGBに変換\n\nこれはJPEGで実際に使われるデータ量削減の手法であり、Q1,Q2はJPEGの仕様書に則って次式で定義される。\n\nQ1 = np.array(((16, 11, 10, 16, 24, 40, 51, 61),\n               (12, 12, 14, 19, 26, 58, 60, 55),\n               (14, 13, 16, 24, 40, 57, 69, 56),\n               (14, 17, 22, 29, 51, 87, 80, 62),\n               (18, 22, 37, 56, 68, 109, 103, 77),\n               (24, 35, 55, 64, 81, 104, 113, 92),\n               (49, 64, 78, 87, 103, 121, 120, 101),\n               (72, 92, 95, 98, 112, 100, 103, 99)), dtype=np.float32)\n\nQ2 = np.array(((17, 18, 24, 47, 99, 99, 99, 99),\n               (18, 21, 26, 66, 99, 99, 99, 99),\n               (24, 26, 56, 99, 99, 99, 99, 99),\n               (47, 66, 99, 99, 99, 99, 99, 99),\n               (99, 99, 99, 99, 99, 99, 99, 99),\n               (99, 99, 99, 99, 99, 99, 99, 99),\n               (99, 99, 99, 99, 99, 99, 99, 99),\n               (99, 99, 99, 99, 99, 99, 99, 99)), dtype=np.float32)", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# DCT hyoer-parameter\nT = 8\nK = 8\nchannel = 3\n\n\n# BGR -> Y Cb Cr\ndef BGR2YCbCr(img):\n  H, W, _ = img.shape\n\n  ycbcr = np.zeros([H, W, 3], dtype=np.float32)\n\n  ycbcr[..., 0] = 0.2990 * img[..., 2] + 0.5870 * img[..., 1] + 0.1140 * img[..., 0]\n  ycbcr[..., 1] = -0.1687 * img[..., 2] - 0.3313 * img[..., 1] + 0.5 * img[..., 0] + 128.\n  ycbcr[..., 2] = 0.5 * img[..., 2] - 0.4187 * img[..., 1] - 0.0813 * img[..., 0] + 128.\n\n  return ycbcr\n\n# Y Cb Cr -> BGR\ndef YCbCr2BGR(ycbcr):\n  H, W, _ = ycbcr.shape\n\n  out = np.zeros([H, W, channel], dtype=np.float32)\n  out[..., 2] = ycbcr[..., 0] + (ycbcr[..., 2] - 128.) * 1.4020\n  out[..., 1] = ycbcr[..., 0] - (ycbcr[..., 1] - 128.) * 0.3441 - (ycbcr[..., 2] - 128.) * 0.7139\n  out[..., 0] = ycbcr[..., 0] + (ycbcr[..., 1] - 128.) * 1.7718\n\n  out = np.clip(out, 0, 255)\n  out = out.astype(np.uint8)\n\n  return out\n\n\n# DCT weight\ndef DCT_w(x, y, u, v):\n    cu = 1.\n    cv = 1.\n    if u == 0:\n        cu /= np.sqrt(2)\n    if v == 0:\n        cv /= np.sqrt(2)\n    theta = np.pi / (2 * T)\n    return (( 2 * cu * cv / T) * np.cos((2*x+1)*u*theta) * np.cos((2*y+1)*v*theta))\n\n# DCT\ndef dct(img):\n    H, W, _ = img.shape\n\n    F = np.zeros((H, W, channel), dtype=np.float32)\n\n    for c in range(channel):\n        for yi in range(0, H, T):\n            for xi in range(0, W, T):\n                for v in range(T):\n                    for u in range(T):\n                        for y in range(T):\n                            for x in range(T):\n                                F[v+yi, u+xi, c] += img[y+yi, x+xi, c] * DCT_w(x,y,u,v)\n\n    return F\n\n\n# IDCT\ndef idct(F):\n    H, W, _ = F.shape\n\n    out = np.zeros((H, W, channel), dtype=np.float32)\n\n    for c in range(channel):\n        for yi in range(0, H, T):\n            for xi in range(0, W, T):\n                for y in range(T):\n                    for x in range(T):\n                        for v in range(K):\n                            for u in range(K):\n                                out[y+yi, x+xi, c] += F[v+yi, u+xi, c] * DCT_w(x,y,u,v)\n\n    out = np.clip(out, 0, 255)\n    out = np.round(out).astype(np.uint8)\n\n    return out\n\n# Quantization\ndef quantization(F):\n    H, W, _ = F.shape\n\n    Q = np.array(((16, 11, 10, 16, 24, 40, 51, 61),\n                (12, 12, 14, 19, 26, 58, 60, 55),\n                (14, 13, 16, 24, 40, 57, 69, 56),\n                (14, 17, 22, 29, 51, 87, 80, 62),\n                (18, 22, 37, 56, 68, 109, 103, 77),\n                (24, 35, 55, 64, 81, 104, 113, 92),\n                (49, 64, 78, 87, 103, 121, 120, 101),\n                (72, 92, 95, 98, 112, 100, 103, 99)), dtype=np.float32)\n\n    for ys in range(0, H, T):\n        for xs in range(0, W, T):\n            for c in range(channel):\n                F[ys: ys + T, xs: xs + T, c] =  np.round(F[ys: ys + T, xs: xs + T, c] / Q) * Q\n\n    return F\n\n\n# JPEG without Hufman coding\ndef JPEG(img):\n    # BGR -> Y Cb Cr\n    ycbcr = BGR2YCbCr(img)\n\n    # DCT\n    F = dct(ycbcr)\n\n    # quantization\n    F = quantization(F)\n\n    # IDCT\n    ycbcr = idct(F)\n\n    # Y Cb Cr -> BGR\n    out = YCbCr2BGR(ycbcr)\n\n    return out\n\n\n# MSE\ndef MSE(img1, img2):\n    H, W, _ = img1.shape\n    mse = np.sum((img1 - img2) ** 2) / (H * W * channel)\n    return mse\n\n# PSNR\ndef PSNR(mse, vmax=255):\n    return 10 * np.log10(vmax * vmax / mse)\n\n# bitrate\ndef BITRATE():\n    return 1. * T * K * K / T / T\n\n\n# Read image\nimg = cv2.imread(\"imori.jpg\").astype(np.float32)\n\n# JPEG\nout = JPEG(img)\n\n# MSE\nmse = MSE(img, out)\n\n# PSNR\npsnr = PSNR(mse)\n\n# bitrate\nbitrate = BITRATE()\n\nprint(\"MSE:\", mse)\nprint(\"PSNR:\", psnr)\nprint(\"bitrate:\", bitrate)\n\n# Save result\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)\ncv2.imwrite(\"out.jpg\", out)"}], "task": "code_generation", "index": 239, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_240", "instruction": "Canny法は、\n1. ガウシアンフィルタを掛ける\n2. x, y方向のSobelフィルタを掛け、それらからエッジ強度とエッジ勾配を求める\n3. エッジ勾配の値から、Non-maximum suppression によりエッジの細線化を行う\n4. ヒステリシスによる閾値処理を行う\n\n以上により、画像からエッジ部分を抜き出す手法である。\n\npythonを用いて、1と2の処理を実装しなさい。\n\n処理手順は、\n1. 画像をグレースケール化する\n2. ガウシアンフィルタ(5x5, s=1.4)をかける\n3. x方向、y方向のsobelフィルタを掛け、画像の勾配画像fx, fyを求め、勾配強度と勾配角度を次式で求める。\n\n勾配強度 edge = sqrt(fx^2 + fy^2)\n勾配角度 angle = arctan(fy / fx)\n\n4. 勾配角度を次式に沿って、量子化する。\n\nただし、angleはradianから角度(degree)にして、-22.5から157.5の範囲をとるように値が修正してから、以下の計算を行う。\n\nangle = {   0  (if -22.5 < angle <= 22.5)\n           45  (if 22.5 < angle <= 67.5)\n           90  (if 67.5 < angle <= 112.5)\n          135  (if 112.5 < angle <= 157.5)\n\nただし、フィルタリングをパディングする際は、numpy.pad()を用いて、エッジの値でパディングせよ。", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef Canny_step1(img):\n\n\t# Gray scale\n\tdef BGR2GRAY(img):\n\t\tb = img[:, :, 0].copy()\n\t\tg = img[:, :, 1].copy()\n\t\tr = img[:, :, 2].copy()\n\n\t\t# Gray scale\n\t\tout = 0.2126 * r + 0.7152 * g + 0.0722 * b\n\t\tout = out.astype(np.uint8)\n\n\t\treturn out\n\n\n\t# Gaussian filter for grayscale\n\tdef gaussian_filter(img, K_size=3, sigma=1.3):\n\n\t\tif len(img.shape) == 3:\n\t\t\tH, W, C = img.shape\n\t\t\tgray = False\n\t\telse:\n\t\t\timg = np.expand_dims(img, axis=-1)\n\t\t\tH, W, C = img.shape\n\t\t\tgray = True\n\n\t\t## Zero padding\n\t\tpad = K_size // 2\n\t\tout = np.zeros([H + pad * 2, W + pad * 2, C], dtype=np.float)\n\t\tout[pad: pad + H, pad: pad + W] = img.copy().astype(np.float)\n\n\t\t## prepare Kernel\n\t\tK = np.zeros((K_size, K_size), dtype=np.float)\n\t\tfor x in range(-pad, -pad + K_size):\n\t\t\tfor y in range(-pad, -pad + K_size):\n\t\t\t\tK[y + pad, x + pad] = np.exp( - (x ** 2 + y ** 2) / (2 * (sigma ** 2)))\n\t\tK /= (2 * np.pi * sigma * sigma)\n\t\tK /= K.sum()\n\n\t\ttmp = out.copy()\n\n\t\t# filtering\n\t\tfor y in range(H):\n\t\t\tfor x in range(W):\n\t\t\t\tfor c in range(C):\n\t\t\t\t\tout[pad + y, pad + x, c] = np.sum(K * tmp[y : y + K_size, x : x + K_size, c]) \n\t\t\t\t\t\n\t\tout = np.clip(out, 0, 255)\n\t\tout = out[pad : pad + H, pad : pad + W]\n\t\t#out = out.astype(np.uint8)\n\n\t\tif gray:\n\t\t\tout = out[..., 0]\n\n\t\treturn out\n\n\n\t# sobel filter\n\tdef sobel_filter(img, K_size=3):\n\t\tif len(img.shape) == 3:\n\t\t\tH, W, C = img.shape\n\t\telse:\n\t\t\t#img = np.expand_dims(img, axis=-1)\n\t\t\tH, W = img.shape\n\n\t\t# Zero padding\n\t\tpad = K_size // 2\n\t\tout = np.zeros((H + pad * 2, W + pad * 2), dtype=np.float)\n\t\tout[pad : pad + H, pad : pad + W] = img.copy().astype(np.float)\n\t\ttmp = out.copy()\n\n\t\tout_v = out.copy()\n\t\tout_h = out.copy()\n\n\t\t## Sobel vertical\n\t\tKv = [[1., 2., 1.],[0., 0., 0.], [-1., -2., -1.]]\n\t\t## Sobel horizontal\n\t\tKh = [[1., 0., -1.],[2., 0., -2.],[1., 0., -1.]]\n\n\t\t# filtering\n\t\tfor y in range(H):\n\t\t\tfor x in range(W):\n\t\t\t\tout_v[pad + y, pad + x] = np.sum(Kv * (tmp[y : y + K_size, x : x + K_size]))\n\t\t\t\tout_h[pad + y, pad + x] = np.sum(Kh * (tmp[y : y + K_size, x : x + K_size]))\n\n\t\tout_v = np.clip(out_v, 0, 255)\n\t\tout_h = np.clip(out_h, 0, 255)\n\n\t\tout_v = out_v[pad : pad + H, pad : pad + W].astype(np.uint8)\n\t\tout_h = out_h[pad : pad + H, pad : pad + W].astype(np.uint8)\n\n\t\treturn out_v, out_h\n\n\n\tdef get_edge_angle(fx, fy):\n\t\t# get edge strength\n\t\tedge = np.sqrt(np.power(fx, 2) + np.power(fy, 2))\n\t\tfx = np.maximum(fx, 1e-5)\n\n\t\t# get edge angle\n\t\tangle = np.arctan(fy / fx)\n\n\t\treturn edge, angle\n\n\n\tdef angle_quantization(angle):\n\t\tangle = angle / np.pi * 180\n\t\tangle[angle < -22.5] = 180 + angle[angle < -22.5]\n\t\t_angle = np.zeros_like(angle, dtype=np.uint8)\n\t\t_angle[np.where(angle <= 22.5)] = 0\n\t\t_angle[np.where((angle > 22.5) & (angle <= 67.5))] = 45\n\t\t_angle[np.where((angle > 67.5) & (angle <= 112.5))] = 90\n\t\t_angle[np.where((angle > 112.5) & (angle <= 157.5))] = 135\n\n\t\treturn _angle\n\n\t# grayscale\n\tgray = BGR2GRAY(img)\n\n\t# gaussian filtering\n\tgaussian = gaussian_filter(gray, K_size=5, sigma=1.4)\n\n\t# sobel filtering\n\tfy, fx = sobel_filter(gaussian, K_size=3)\n\n\t# get edge strength, angle\n\tedge, angle = get_edge_angle(fx, fy)\n\n\t# angle quantization\n\tangle = angle_quantization(angle)\n\n\treturn edge, angle\n\n\n# Read image\nimg = cv2.imread(\"imori.jpg\").astype(np.float32)\n\n# Canny (step1)\nedge, angle = Canny_step1(img)\n\nedge = edge.astype(np.uint8)\nangle = angle.astype(np.uint8)\n\n# Save result\ncv2.imwrite(\"out.jpg\", edge)\ncv2.imshow(\"result\", edge)\ncv2.imwrite(\"out2.jpg\", angle)\ncv2.imshow(\"result2\", angle)\ncv2.waitKey(0)\ncv2.destroyAllWindows()"}], "task": "code_generation", "index": 240, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_241", "instruction": "Canny法は、\n1. ガウシアンフィルタを掛ける\n2. x, y方向のSobelフィルタを掛け、それらからエッジ強度とエッジ勾配を求める\n3. エッジ勾配の値から、Non-maximum suppression によりエッジの細線化を行う\n4. ヒステリシスによる閾値処理を行う\n\n以上により、画像からエッジ部分を抜き出す手法である。\n\npythonを用いて3の処理を実装しないさい。\n\n処理1、2で求めた勾配角度から、Non-maximum suppressionを行い、エッジ線を細くする（細線化）。\n\nNon-maximum suppression(NMS)とは非最大値以外を除去する作業の総称である。（他のタスクでもこの名前はよく出る）\n\nここでは、注目している箇所の勾配角度の法線方向の隣接ピクセルの３つの勾配強度を比較して、最大値ならそのまま値をいじらずに、最大値でなければ強度を0にする、\n\nつまり、勾配強度edge(x,y)に注目している際に、勾配角度angle(x,y)によって次式のようにedge(x,y)を変更する。\n\nif angle(x,y)  = 0\n if edge(x,y), edge(x-1,y), edge(x+1,y)で edge(x,y)が最大じゃない\n  then edge(x,y) = 0\nif angle(x,y)  = 45\n if edge(x,y), edge(x-1,y+1), edge(x+1,y-1)で edge(x,y)が最大じゃない\n  then edge(x,y) = 0\nif angle(x,y)  = 90\n if edge(x,y), edge(x,y-1), edge(x,y+1)で edge(x,y)が最大じゃない\n  then edge(x,y) = 0\nif angle(x,y)  = 135\n if edge(x,y), edge(x-1,y-1), edge(x+1,y+1)で edge(x,y)が最大じゃない\n  then edge(x,y) = 0", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef Canny_step2(img):\n\n\t# Gray scale\n\tdef BGR2GRAY(img):\n\t\tb = img[:, :, 0].copy()\n\t\tg = img[:, :, 1].copy()\n\t\tr = img[:, :, 2].copy()\n\n\t\t# Gray scale\n\t\tout = 0.2126 * r + 0.7152 * g + 0.0722 * b\n\t\tout = out.astype(np.uint8)\n\n\t\treturn out\n\n\n\t# Gaussian filter for grayscale\n\tdef gaussian_filter(img, K_size=3, sigma=1.3):\n\n\t\tif len(img.shape) == 3:\n\t\t\tH, W, C = img.shape\n\t\telse:\n\t\t\timg = np.expand_dims(img, axis=-1)\n\t\t\tH, W, C = img.shape\n\n\t\t## Zero padding\n\t\tpad = K_size // 2\n\t\tout = np.zeros([H + pad * 2, W + pad * 2, C], dtype=np.float)\n\t\tout[pad: pad + H, pad: pad + W] = img.copy().astype(np.float)\n\n\t\t## prepare Kernel\n\t\tK = np.zeros((K_size, K_size), dtype=np.float)\n\t\tfor x in range(-pad, -pad + K_size):\n\t\t\tfor y in range(-pad, -pad + K_size):\n\t\t\t\tK[y + pad, x + pad] = np.exp( - (x ** 2 + y ** 2) / (2 * (sigma ** 2)))\n\t\t#K /= (sigma * np.sqrt(2 * np.pi))\n\t\tK /= (2 * np.pi * sigma * sigma)\n\t\tK /= K.sum()\n\n\t\ttmp = out.copy()\n\n\t\t# filtering\n\t\tfor y in range(H):\n\t\t\tfor x in range(W):\n\t\t\t\tfor c in range(C):\n\t\t\t\t\tout[pad + y, pad + x, c] = np.sum(K * tmp[y : y + K_size, x : x + K_size, c])\n\n\t\tout = np.clip(out, 0, 255)\n\t\tout = out[pad : pad + H, pad : pad + W]\n\t\tout = out.astype(np.uint8)\n\t\tout = out[..., 0]\n\n\t\treturn out\n\n\n\t# sobel filter\n\tdef sobel_filter(img, K_size=3):\n\t\tif len(img.shape) == 3:\n\t\t\tH, W, C = img.shape\n\t\telse:\n\t\t\tH, W = img.shape\n\n\t\t# Zero padding\n\t\tpad = K_size // 2\n\t\tout = np.zeros((H + pad * 2, W + pad * 2), dtype=np.float)\n\t\tout[pad: pad + H, pad: pad + W] = img.copy().astype(np.float)\n\t\ttmp = out.copy()\n\n\t\tout_v = out.copy()\n\t\tout_h = out.copy()\n\n\t\t## Sobel vertical\n\t\tKv = [[1., 2., 1.],[0., 0., 0.], [-1., -2., -1.]]\n\t\t## Sobel horizontal\n\t\tKh = [[1., 0., -1.],[2., 0., -2.],[1., 0., -1.]]\n\n\t\t# filtering\n\t\tfor y in range(H):\n\t\t\tfor x in range(W):\n\t\t\t\tout_v[pad + y, pad + x] = np.sum(Kv * (tmp[y: y + K_size, x: x + K_size]))\n\t\t\t\tout_h[pad + y, pad + x] = np.sum(Kh * (tmp[y: y + K_size, x: x + K_size]))\n\n\t\tout_v = np.clip(out_v, 0, 255)\n\t\tout_h = np.clip(out_h, 0, 255)\n\n\t\tout_v = out_v[pad: pad + H, pad: pad + W].astype(np.uint8)\n\t\tout_h = out_h[pad: pad + H, pad: pad + W].astype(np.uint8)\n\n\t\treturn out_v, out_h\n\n\n\tdef get_edge_angle(fx, fy):\n\t\t# get edge strength\n\t\tedge = np.sqrt(np.power(fx.astype(np.float32), 2) + np.power(fy.astype(np.float32), 2))\n\t\tedge = np.clip(edge, 0, 255)\n\n\t\tfx = np.maximum(fx, 1e-5)\n\t\t#fx[np.abs(fx) <= 1e-5] = 1e-5\n\n\t\t# get edge angle\n\t\tangle = np.arctan(fy / fx)\n\n\t\treturn edge, angle\n\n\n\tdef angle_quantization(angle):\n\t\tangle = angle / np.pi * 180\n\t\tangle[angle < -22.5] = 180 + angle[angle < -22.5]\n\t\t_angle = np.zeros_like(angle, dtype=np.uint8)\n\t\t_angle[np.where(angle <= 22.5)] = 0\n\t\t_angle[np.where((angle > 22.5) & (angle <= 67.5))] = 45\n\t\t_angle[np.where((angle > 67.5) & (angle <= 112.5))] = 90\n\t\t_angle[np.where((angle > 112.5) & (angle <= 157.5))] = 135\n\n\t\treturn _angle\n\n\n\tdef non_maximum_suppression(angle, edge):\n\t\tH, W = angle.shape\n\t\t_edge = edge.copy()\n\n\t\tfor y in range(H):\n\t\t\tfor x in range(W):\n\t\t\t\t\tif angle[y, x] == 0:\n\t\t\t\t\t\t\tdx1, dy1, dx2, dy2 = -1, 0, 1, 0\n\t\t\t\t\telif angle[y, x] == 45:\n\t\t\t\t\t\t\tdx1, dy1, dx2, dy2 = -1, 1, 1, -1\n\t\t\t\t\telif angle[y, x] == 90:\n\t\t\t\t\t\t\tdx1, dy1, dx2, dy2 = 0, -1, 0, 1\n\t\t\t\t\telif angle[y, x] == 135:\n\t\t\t\t\t\t\tdx1, dy1, dx2, dy2 = -1, -1, 1, 1\n\t\t\t\t\tif x == 0:\n\t\t\t\t\t\t\tdx1 = max(dx1, 0)\n\t\t\t\t\t\t\tdx2 = max(dx2, 0)\n\t\t\t\t\tif x == W-1:\n\t\t\t\t\t\t\tdx1 = min(dx1, 0)\n\t\t\t\t\t\t\tdx2 = min(dx2, 0)\n\t\t\t\t\tif y == 0:\n\t\t\t\t\t\t\tdy1 = max(dy1, 0)\n\t\t\t\t\t\t\tdy2 = max(dy2, 0)\n\t\t\t\t\tif y == H-1:\n\t\t\t\t\t\t\tdy1 = min(dy1, 0)\n\t\t\t\t\t\t\tdy2 = min(dy2, 0)\n\t\t\t\t\tif max(max(edge[y, x], edge[y + dy1, x + dx1]), edge[y + dy2, x + dx2]) != edge[y, x]:\n\t\t\t\t\t\t\t_edge[y, x] = 0\n\n\t\treturn _edge\n\n\t# grayscale\n\tgray = BGR2GRAY(img)\n\n\t# gaussian filtering\n\tgaussian = gaussian_filter(gray, K_size=5, sigma=1.4)\n\n\t# sobel filtering\n\tfy, fx = sobel_filter(gaussian, K_size=3)\n\n\t# get edge strength, angle\n\tedge, angle = get_edge_angle(fx, fy)\n\n\t# angle quantization\n\tangle = angle_quantization(angle)\n\n\t# non maximum suppression\n\tedge = non_maximum_suppression(angle, edge)\n\n\treturn edge, angle\n\n\n# Read image\nimg = cv2.imread(\"imori.jpg\").astype(np.float32)\n\n# Canny (step2)\nedge, angle = Canny_step2(img)\n\nedge = edge.astype(np.uint8)\nangle = angle.astype(np.uint8)\n\n# Save result\ncv2.imwrite(\"out.jpg\", edge)\ncv2.imshow(\"result\", edge)\ncv2.imwrite(\"out2.jpg\", angle)\ncv2.imshow(\"result2\", angle)\ncv2.waitKey(0)\ncv2.destroyAllWindows()"}], "task": "code_generation", "index": 241, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_242", "instruction": "Canny法は、\n1. ガウシアンフィルタを掛ける\n2. x, y方向のSobelフィルタを掛け、それらからエッジ強度とエッジ勾配を求める\n3. エッジ勾配の値から、Non-maximum suppression によりエッジの細線化を行う\n4. ヒステリシスによる閾値処理を行う\n\n以上により、画像からエッジ部分を抜き出す手法である。\n\npythonを用いて、4の処理を実装する。\n\nここでは、閾値により勾配強度の二値化を行うがCanny法では二つの閾値(HT: high thoresholdとLT: low threshold)を用いる。\n\nはじめに、\n1. 勾配強度edge(x,y)がHT以上の場合はedge(x,y)=255\n2. LT以下のedge(x,y)=0\n3.  LT < edge(x,y) < HTの時、周り８ピクセルの勾配強度でHTより大きい値が存在すれば、edge(x,y)=255\n\nここでは、HT=50, LT=20とする。ちなみに閾値の値は結果を見ながら判断するしかない。\n\n以上のアルゴリズムによって、Canny法が行われる。", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef Canny(img):\n\n\t# Gray scale\n\tdef BGR2GRAY(img):\n\t\tb = img[:, :, 0].copy()\n\t\tg = img[:, :, 1].copy()\n\t\tr = img[:, :, 2].copy()\n\n\t\t# Gray scale\n\t\tout = 0.2126 * r + 0.7152 * g + 0.0722 * b\n\t\tout = out.astype(np.uint8)\n\n\t\treturn out\n\n\n\t# Gaussian filter for grayscale\n\tdef gaussian_filter(img, K_size=3, sigma=1.3):\n\n\t\tif len(img.shape) == 3:\n\t\t\tH, W, C = img.shape\n\t\t\tgray = False\n\t\telse:\n\t\t\timg = np.expand_dims(img, axis=-1)\n\t\t\tH, W, C = img.shape\n\t\t\tgray = True\n\n\t\t## Zero padding\n\t\tpad = K_size // 2\n\t\tout = np.zeros([H + pad * 2, W + pad * 2, C], dtype=np.float)\n\t\tout[pad : pad + H, pad : pad + W] = img.copy().astype(np.float)\n\n\t\t## prepare Kernel\n\t\tK = np.zeros((K_size, K_size), dtype=np.float)\n\t\tfor x in range(-pad, -pad + K_size):\n\t\t\tfor y in range(-pad, -pad + K_size):\n\t\t\t\tK[y + pad, x + pad] = np.exp( - (x ** 2 + y ** 2) / (2 * sigma * sigma))\n\t\t#K /= (sigma * np.sqrt(2 * np.pi))\n\t\tK /= (2 * np.pi * sigma * sigma)\n\t\tK /= K.sum()\n\n\t\ttmp = out.copy()\n\n\t\t# filtering\n\t\tfor y in range(H):\n\t\t\tfor x in range(W):\n\t\t\t\tfor c in range(C):\n\t\t\t\t\tout[pad + y, pad + x, c] = np.sum(K * tmp[y : y + K_size, x : x + K_size, c])\n\n\t\tout = np.clip(out, 0, 255)\n\t\tout = out[pad : pad + H, pad : pad + W]\n\t\tout = out.astype(np.uint8)\n\n\t\tif gray:\n\t\t\tout = out[..., 0]\n\n\t\treturn out\n\n\n\t# sobel filter\n\tdef sobel_filter(img, K_size=3):\n\t\tif len(img.shape) == 3:\n\t\t\tH, W, C = img.shape\n\t\telse:\n\t\t\tH, W = img.shape\n\n\t\t# Zero padding\n\t\tpad = K_size // 2\n\t\tout = np.zeros((H + pad * 2, W + pad * 2), dtype=np.float)\n\t\tout[pad : pad + H, pad : pad + W] = img.copy().astype(np.float)\n\t\ttmp = out.copy()\n\n\t\tout_v = out.copy()\n\t\tout_h = out.copy()\n\n\t\t## Sobel vertical\n\t\tKv = [[1., 2., 1.],[0., 0., 0.], [-1., -2., -1.]]\n\t\t## Sobel horizontal\n\t\tKh = [[1., 0., -1.],[2., 0., -2.],[1., 0., -1.]]\n\n\t\t# filtering\n\t\tfor y in range(H):\n\t\t\tfor x in range(W):\n\t\t\t\tout_v[pad + y, pad + x] = np.sum(Kv * (tmp[y : y + K_size, x : x + K_size]))\n\t\t\t\tout_h[pad + y, pad + x] = np.sum(Kh * (tmp[y : y + K_size, x : x + K_size]))\n\n\t\tout_v = np.clip(out_v, 0, 255)\n\t\tout_h = np.clip(out_h, 0, 255)\n\n\t\tout_v = out_v[pad : pad + H, pad : pad + W]\n\t\tout_v = out_v.astype(np.uint8)\n\t\tout_h = out_h[pad : pad + H, pad : pad + W]\n\t\tout_h = out_h.astype(np.uint8)\n\n\t\treturn out_v, out_h\n\n\n\tdef get_edge_angle(fx, fy):\n\t\t# get edge strength\n\t\tedge = np.sqrt(np.power(fx.astype(np.float32), 2) + np.power(fy.astype(np.float32), 2))\n\t\tedge = np.clip(edge, 0, 255)\n\n\t\tfx = np.maximum(fx, 1e-10)\n\t\t#fx[np.abs(fx) <= 1e-5] = 1e-5\n\n\t\t# get edge angle\n\t\tangle = np.arctan(fy / fx)\n\n\t\treturn edge, angle\n\n\n\tdef angle_quantization(angle):\n\t\tangle = angle / np.pi * 180\n\t\tangle[angle < -22.5] = 180 + angle[angle < -22.5]\n\t\t_angle = np.zeros_like(angle, dtype=np.uint8)\n\t\t_angle[np.where(angle <= 22.5)] = 0\n\t\t_angle[np.where((angle > 22.5) & (angle <= 67.5))] = 45\n\t\t_angle[np.where((angle > 67.5) & (angle <= 112.5))] = 90\n\t\t_angle[np.where((angle > 112.5) & (angle <= 157.5))] = 135\n\n\t\treturn _angle\n\n\n\tdef non_maximum_suppression(angle, edge):\n\t\tH, W = angle.shape\n\t\t_edge = edge.copy()\n\t\t\n\t\tfor y in range(H):\n\t\t\tfor x in range(W):\n\t\t\t\t\tif angle[y, x] == 0:\n\t\t\t\t\t\t\tdx1, dy1, dx2, dy2 = -1, 0, 1, 0\n\t\t\t\t\telif angle[y, x] == 45:\n\t\t\t\t\t\t\tdx1, dy1, dx2, dy2 = -1, 1, 1, -1\n\t\t\t\t\telif angle[y, x] == 90:\n\t\t\t\t\t\t\tdx1, dy1, dx2, dy2 = 0, -1, 0, 1\n\t\t\t\t\telif angle[y, x] == 135:\n\t\t\t\t\t\t\tdx1, dy1, dx2, dy2 = -1, -1, 1, 1\n\t\t\t\t\tif x == 0:\n\t\t\t\t\t\t\tdx1 = max(dx1, 0)\n\t\t\t\t\t\t\tdx2 = max(dx2, 0)\n\t\t\t\t\tif x == W-1:\n\t\t\t\t\t\t\tdx1 = min(dx1, 0)\n\t\t\t\t\t\t\tdx2 = min(dx2, 0)\n\t\t\t\t\tif y == 0:\n\t\t\t\t\t\t\tdy1 = max(dy1, 0)\n\t\t\t\t\t\t\tdy2 = max(dy2, 0)\n\t\t\t\t\tif y == H-1:\n\t\t\t\t\t\t\tdy1 = min(dy1, 0)\n\t\t\t\t\t\t\tdy2 = min(dy2, 0)\n\t\t\t\t\tif max(max(edge[y, x], edge[y + dy1, x + dx1]), edge[y + dy2, x + dx2]) != edge[y, x]:\n\t\t\t\t\t\t\t_edge[y, x] = 0\n\n\t\treturn _edge\n\n\tdef hysterisis(edge, HT=100, LT=30):\n\t\tH, W = edge.shape\n\n\t\t# Histeresis threshold\n\t\tedge[edge >= HT] = 255\n\t\tedge[edge <= LT] = 0\n\n\t\t_edge = np.zeros((H + 2, W + 2), dtype=np.float32)\n\t\t_edge[1 : H + 1, 1 : W + 1] = edge\n\n\t\t## 8 - Nearest neighbor\n\t\tnn = np.array(((1., 1., 1.), (1., 0., 1.), (1., 1., 1.)), dtype=np.float32)\n\n\t\tfor y in range(1, H+2):\n\t\t\t\tfor x in range(1, W+2):\n\t\t\t\t\t\tif _edge[y, x] < LT or _edge[y, x] > HT:\n\t\t\t\t\t\t\t\tcontinue\n\t\t\t\t\t\tif np.max(_edge[y-1:y+2, x-1:x+2] * nn) >= HT:\n\t\t\t\t\t\t\t\t_edge[y, x] = 255\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\t_edge[y, x] = 0\n\n\t\tedge = _edge[1:H+1, 1:W+1]\n\t\t\t\t\t\t\t\t\n\t\treturn edge\n\n\t# grayscale\n\tgray = BGR2GRAY(img)\n\n\t# gaussian filtering\n\tgaussian = gaussian_filter(gray, K_size=5, sigma=1.4)\n\n\t# sobel filtering\n\tfy, fx = sobel_filter(gaussian, K_size=3)\n\n\t# get edge strength, angle\n\tedge, angle = get_edge_angle(fx, fy)\n\n\t# angle quantization\n\tangle = angle_quantization(angle)\n\n\t# non maximum suppression\n\tedge = non_maximum_suppression(angle, edge)\n\n\t# hysterisis threshold\n\tout = hysterisis(edge, 50, 20)\n\n\treturn out\n\n\n# Read image\nimg = cv2.imread(\"imori.jpg\").astype(np.float32)\n\n# Canny\nedge = Canny(img)\n\nout = edge.astype(np.uint8)\n\n# Save result\ncv2.imwrite(\"out.jpg\", out)\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)\ncv2.destroyAllWindows()"}], "task": "code_generation", "index": 242, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_243", "instruction": "Hough変換とは、座標を直交座標から極座標に変換することにより数式に沿って直線や円など一定の形状を検出する手法である。\nある直線状の点では極座標に変換すると一定のr, tにおいて交わる。\nその点が検出すべき直線を表すパラメータであり、このパラメータを逆変換すると直線の方程式を求めることができる。\n\n方法としては、\n1. エッジ画像(ここではCannyの出力)からエッジのピクセルにおいてHough変換を行う。\n2. Hough変換後の値のヒストグラムをとり、極大点を選ぶ。\n3. 極大点のr, tの値をHough逆変換して検出した直線のパラメータを得る。\n\nとなる。\n\npythonを用いて1のHough変換を行いヒストグラムを作成しなさい。\n\nアルゴリズムは、\n\n1. 画像の対角線の長さrmaxを求める。\n2. エッジ箇所(x,y)において、t = 0-179で一度ずつtを変えながら、次式によりHough変換を行う。\n\n\nrho = x * cos(t) + y * sin(t)\n\n3. 「ボーティング（投票）」 [2 x rmax, 180]のサイズの表を用意し、1で得たtable(rho, t) に1を足す。２で求めたrhoは[-rmax, rmax]の範囲を取るので、ボーティングの時には注意！\n\nボーティングでは、一定の箇所に集中する。\n\n今回はtorino.jpgを用いて、ボーディングした表を図示せよ。\nCannyのパラメータは, gaussian filter(5x5, s=1.4), HT = 100, LT = 30で使用せよ。", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef Canny(img):\n\n\t# Gray scale\n\tdef BGR2GRAY(img):\n\t\tb = img[:, :, 0].copy()\n\t\tg = img[:, :, 1].copy()\n\t\tr = img[:, :, 2].copy()\n\n\t\t# Gray scale\n\t\tout = 0.2126 * r + 0.7152 * g + 0.0722 * b\n\t\tout = out.astype(np.uint8)\n\n\t\treturn out\n\n\n\t# Gaussian filter for grayscale\n\tdef gaussian_filter(img, K_size=3, sigma=1.3):\n\n\t\tif len(img.shape) == 3:\n\t\t\tH, W, C = img.shape\n\t\t\tgray = False\n\t\telse:\n\t\t\timg = np.expand_dims(img, axis=-1)\n\t\t\tH, W, C = img.shape\n\t\t\tgray = True\n\n\t\t## Zero padding\n\t\tpad = K_size // 2\n\t\tout = np.zeros([H + pad * 2, W + pad * 2, C], dtype=np.float)\n\t\tout[pad : pad + H, pad : pad + W] = img.copy().astype(np.float)\n\n\t\t## prepare Kernel\n\t\tK = np.zeros((K_size, K_size), dtype=np.float)\n\t\tfor x in range(-pad, -pad + K_size):\n\t\t\tfor y in range(-pad, -pad + K_size):\n\t\t\t\tK[y + pad, x + pad] = np.exp( - (x ** 2 + y ** 2) / (2 * sigma * sigma))\n\t\t#K /= (sigma * np.sqrt(2 * np.pi))\n\t\tK /= (2 * np.pi * sigma * sigma)\n\t\tK /= K.sum()\n\n\t\ttmp = out.copy()\n\n\t\t# filtering\n\t\tfor y in range(H):\n\t\t\tfor x in range(W):\n\t\t\t\tfor c in range(C):\n\t\t\t\t\tout[pad + y, pad + x, c] = np.sum(K * tmp[y : y + K_size, x : x + K_size, c])\n\n\t\tout = np.clip(out, 0, 255)\n\t\tout = out[pad : pad + H, pad : pad + W]\n\t\tout = out.astype(np.uint8)\n\n\t\tif gray:\n\t\t\tout = out[..., 0]\n\n\t\treturn out\n\n\n\t# sobel filter\n\tdef sobel_filter(img, K_size=3):\n\t\tif len(img.shape) == 3:\n\t\t\tH, W, C = img.shape\n\t\telse:\n\t\t\tH, W = img.shape\n\n\t\t# Zero padding\n\t\tpad = K_size // 2\n\t\tout = np.zeros((H + pad * 2, W + pad * 2), dtype=np.float)\n\t\tout[pad : pad + H, pad : pad + W] = img.copy().astype(np.float)\n\t\ttmp = out.copy()\n\n\t\tout_v = out.copy()\n\t\tout_h = out.copy()\n\n\t\t## Sobel vertical\n\t\tKv = [[1., 2., 1.],[0., 0., 0.], [-1., -2., -1.]]\n\t\t## Sobel horizontal\n\t\tKh = [[1., 0., -1.],[2., 0., -2.],[1., 0., -1.]]\n\n\t\t# filtering\n\t\tfor y in range(H):\n\t\t\tfor x in range(W):\n\t\t\t\tout_v[pad + y, pad + x] = np.sum(Kv * (tmp[y : y + K_size, x : x + K_size]))\n\t\t\t\tout_h[pad + y, pad + x] = np.sum(Kh * (tmp[y : y + K_size, x : x + K_size]))\n\n\t\tout_v = np.clip(out_v, 0, 255)\n\t\tout_h = np.clip(out_h, 0, 255)\n\n\t\tout_v = out_v[pad : pad + H, pad : pad + W]\n\t\tout_v = out_v.astype(np.uint8)\n\t\tout_h = out_h[pad : pad + H, pad : pad + W]\n\t\tout_h = out_h.astype(np.uint8)\n\n\t\treturn out_v, out_h\n\n\n\tdef get_edge_angle(fx, fy):\n\t\t# get edge strength\n\t\tedge = np.sqrt(np.power(fx.astype(np.float32), 2) + np.power(fy.astype(np.float32), 2))\n\t\tedge = np.clip(edge, 0, 255)\n\n\t\tfx = np.maximum(fx, 1e-10)\n\t\t#fx[np.abs(fx) <= 1e-5] = 1e-5\n\n\t\t# get edge angle\n\t\tangle = np.arctan(fy / fx)\n\n\t\treturn edge, angle\n\n\n\tdef angle_quantization(angle):\n\t\tangle = angle / np.pi * 180\n\t\tangle[angle < -22.5] = 180 + angle[angle < -22.5]\n\t\t_angle = np.zeros_like(angle, dtype=np.uint8)\n\t\t_angle[np.where(angle <= 22.5)] = 0\n\t\t_angle[np.where((angle > 22.5) & (angle <= 67.5))] = 45\n\t\t_angle[np.where((angle > 67.5) & (angle <= 112.5))] = 90\n\t\t_angle[np.where((angle > 112.5) & (angle <= 157.5))] = 135\n\n\t\treturn _angle\n\n\n\tdef non_maximum_suppression(angle, edge):\n\t\tH, W = angle.shape\n\t\t_edge = edge.copy()\n\t\t\n\t\tfor y in range(H):\n\t\t\tfor x in range(W):\n\t\t\t\t\tif angle[y, x] == 0:\n\t\t\t\t\t\t\tdx1, dy1, dx2, dy2 = -1, 0, 1, 0\n\t\t\t\t\telif angle[y, x] == 45:\n\t\t\t\t\t\t\tdx1, dy1, dx2, dy2 = -1, 1, 1, -1\n\t\t\t\t\telif angle[y, x] == 90:\n\t\t\t\t\t\t\tdx1, dy1, dx2, dy2 = 0, -1, 0, 1\n\t\t\t\t\telif angle[y, x] == 135:\n\t\t\t\t\t\t\tdx1, dy1, dx2, dy2 = -1, -1, 1, 1\n\t\t\t\t\tif x == 0:\n\t\t\t\t\t\t\tdx1 = max(dx1, 0)\n\t\t\t\t\t\t\tdx2 = max(dx2, 0)\n\t\t\t\t\tif x == W-1:\n\t\t\t\t\t\t\tdx1 = min(dx1, 0)\n\t\t\t\t\t\t\tdx2 = min(dx2, 0)\n\t\t\t\t\tif y == 0:\n\t\t\t\t\t\t\tdy1 = max(dy1, 0)\n\t\t\t\t\t\t\tdy2 = max(dy2, 0)\n\t\t\t\t\tif y == H-1:\n\t\t\t\t\t\t\tdy1 = min(dy1, 0)\n\t\t\t\t\t\t\tdy2 = min(dy2, 0)\n\t\t\t\t\tif max(max(edge[y, x], edge[y + dy1, x + dx1]), edge[y + dy2, x + dx2]) != edge[y, x]:\n\t\t\t\t\t\t\t_edge[y, x] = 0\n\n\t\treturn _edge\n\n\tdef hysterisis(edge, HT=100, LT=30):\n\t\tH, W = edge.shape\n\n\t\t# Histeresis threshold\n\t\tedge[edge >= HT] = 255\n\t\tedge[edge <= LT] = 0\n\n\t\t_edge = np.zeros((H + 2, W + 2), dtype=np.float32)\n\t\t_edge[1 : H + 1, 1 : W + 1] = edge\n\n\t\t## 8 - Nearest neighbor\n\t\tnn = np.array(((1., 1., 1.), (1., 0., 1.), (1., 1., 1.)), dtype=np.float32)\n\n\t\tfor y in range(1, H+2):\n\t\t\t\tfor x in range(1, W+2):\n\t\t\t\t\t\tif _edge[y, x] < LT or _edge[y, x] > HT:\n\t\t\t\t\t\t\t\tcontinue\n\t\t\t\t\t\tif np.max(_edge[y-1:y+2, x-1:x+2] * nn) >= HT:\n\t\t\t\t\t\t\t\t_edge[y, x] = 255\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\t_edge[y, x] = 0\n\n\t\tedge = _edge[1:H+1, 1:W+1]\n\t\t\t\t\t\t\t\t\n\t\treturn edge\n\n\t# grayscale\n\tgray = BGR2GRAY(img)\n\n\t# gaussian filtering\n\tgaussian = gaussian_filter(gray, K_size=5, sigma=1.4)\n\n\t# sobel filtering\n\tfy, fx = sobel_filter(gaussian, K_size=3)\n\n\t# get edge strength, angle\n\tedge, angle = get_edge_angle(fx, fy)\n\n\t# angle quantization\n\tangle = angle_quantization(angle)\n\n\t# non maximum suppression\n\tedge = non_maximum_suppression(angle, edge)\n\n\t# hysterisis threshold\n\tout = hysterisis(edge, 100, 30)\n\n\treturn out\n\n\ndef Hough_Line_step1(edge):\n\t## Voting\n\tdef voting(edge):\n\t\tH, W = edge.shape\n\t\tdrho = 1\n\t\tdtheta = 1\n\n\t\t# get rho max length\n\t\trho_max = np.ceil(np.sqrt(H ** 2 + W ** 2)).astype(np.int)\n\n\t\t# hough table\n\t\though = np.zeros((rho_max * 2, 180), dtype=np.int)\n\n\t\t# get index of edge\n\t\tind = np.where(edge == 255)\n\n\t\t## hough transformation\n\t\tfor y, x in zip(ind[0], ind[1]):\n\t\t\t\tfor theta in range(0, 180, dtheta):\n\t\t\t\t\t\t# get polar coordinat4s\n\t\t\t\t\t\tt = np.pi / 180 * theta\n\t\t\t\t\t\trho = int(x * np.cos(t) + y * np.sin(t))\n\n\t\t\t\t\t\t# vote\n\t\t\t\t\t\though[rho + rho_max, theta] += 1\n\t\t\t\t\t\t\t\n\t\tout = hough.astype(np.uint8)\n\n\t\treturn out\n\n\t# voting\n\tout = voting(edge)\n\n\treturn out\n\n\n# Read image\nimg = cv2.imread(\"thorino.jpg\").astype(np.float32)\n\n# Canny\nedge = Canny(img)\n\n# Hough\nout = Hough_Line_step1(edge)\n\nout = out.astype(np.uint8)\n\n# Save result\ncv2.imwrite(\"out.jpg\", out)\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)\ncv2.destroyAllWindows()"}], "task": "code_generation", "index": 243, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_244", "instruction": "Hough変換とは、座標を直交座標から極座標に変換することにより数式に沿って直線や円など一定の形状を検出する手法である。\nある直線状の点では極座標に変換すると一定のr, tにおいて交わる。\nその点が検出すべき直線を表すパラメータであり、このパラメータを逆変換すると直線の方程式を求めることができる。\n\n方法としては、\n1. エッジ画像(ここではCannyの出力)からエッジのピクセルにおいてHough変換を行う。\n2. Hough変換後の値のヒストグラムをとり、極大点を選ぶ。\n3. 極大点のr, tの値をHough逆変換して検出した直線のパラメータを得る。\n\nとなる。\n\npythonを用いて、2の処理を実装しなさい。\n\n1の処理で得られた表では、ある一定の箇所付近に多く投票される。\nここでは、その付近の極大値を抜き出す操作を行え。\n\n今回はボーディングが多い箇所を上位20個抜き出し、図示せよ。(C++の解答は上位30個にしてます。なんか20だと同じような結果にらなかったので、、)\n\nNMSのアルゴリズムは、\n1. 表において、周囲8マス(8近傍)より注目ピクセルの得票数が多ければそのまま。\n2. 注目ピクセルの値が少なければ0にする。", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef Canny(img):\n\n\t# Gray scale\n\tdef BGR2GRAY(img):\n\t\tb = img[:, :, 0].copy()\n\t\tg = img[:, :, 1].copy()\n\t\tr = img[:, :, 2].copy()\n\n\t\t# Gray scale\n\t\tout = 0.2126 * r + 0.7152 * g + 0.0722 * b\n\t\tout = out.astype(np.uint8)\n\n\t\treturn out\n\n\n\t# Gaussian filter for grayscale\n\tdef gaussian_filter(img, K_size=3, sigma=1.3):\n\n\t\tif len(img.shape) == 3:\n\t\t\tH, W, C = img.shape\n\t\t\tgray = False\n\t\telse:\n\t\t\timg = np.expand_dims(img, axis=-1)\n\t\t\tH, W, C = img.shape\n\t\t\tgray = True\n\n\t\t## Zero padding\n\t\tpad = K_size // 2\n\t\tout = np.zeros([H + pad * 2, W + pad * 2, C], dtype=np.float)\n\t\tout[pad : pad + H, pad : pad + W] = img.copy().astype(np.float)\n\n\t\t## prepare Kernel\n\t\tK = np.zeros((K_size, K_size), dtype=np.float)\n\t\tfor x in range(-pad, -pad + K_size):\n\t\t\tfor y in range(-pad, -pad + K_size):\n\t\t\t\tK[y + pad, x + pad] = np.exp( - (x ** 2 + y ** 2) / (2 * sigma * sigma))\n\t\t#K /= (sigma * np.sqrt(2 * np.pi))\n\t\tK /= (2 * np.pi * sigma * sigma)\n\t\tK /= K.sum()\n\n\t\ttmp = out.copy()\n\n\t\t# filtering\n\t\tfor y in range(H):\n\t\t\tfor x in range(W):\n\t\t\t\tfor c in range(C):\n\t\t\t\t\tout[pad + y, pad + x, c] = np.sum(K * tmp[y : y + K_size, x : x + K_size, c])\n\n\t\tout = np.clip(out, 0, 255)\n\t\tout = out[pad : pad + H, pad : pad + W]\n\t\tout = out.astype(np.uint8)\n\n\t\tif gray:\n\t\t\tout = out[..., 0]\n\n\t\treturn out\n\n\n\t# sobel filter\n\tdef sobel_filter(img, K_size=3):\n\t\tif len(img.shape) == 3:\n\t\t\tH, W, C = img.shape\n\t\telse:\n\t\t\tH, W = img.shape\n\n\t\t# Zero padding\n\t\tpad = K_size // 2\n\t\tout = np.zeros((H + pad * 2, W + pad * 2), dtype=np.float)\n\t\tout[pad : pad + H, pad : pad + W] = img.copy().astype(np.float)\n\t\ttmp = out.copy()\n\n\t\tout_v = out.copy()\n\t\tout_h = out.copy()\n\n\t\t## Sobel vertical\n\t\tKv = [[1., 2., 1.],[0., 0., 0.], [-1., -2., -1.]]\n\t\t## Sobel horizontal\n\t\tKh = [[1., 0., -1.],[2., 0., -2.],[1., 0., -1.]]\n\n\t\t# filtering\n\t\tfor y in range(H):\n\t\t\tfor x in range(W):\n\t\t\t\tout_v[pad + y, pad + x] = np.sum(Kv * (tmp[y : y + K_size, x : x + K_size]))\n\t\t\t\tout_h[pad + y, pad + x] = np.sum(Kh * (tmp[y : y + K_size, x : x + K_size]))\n\n\t\tout_v = np.clip(out_v, 0, 255)\n\t\tout_h = np.clip(out_h, 0, 255)\n\n\t\tout_v = out_v[pad : pad + H, pad : pad + W]\n\t\tout_v = out_v.astype(np.uint8)\n\t\tout_h = out_h[pad : pad + H, pad : pad + W]\n\t\tout_h = out_h.astype(np.uint8)\n\n\t\treturn out_v, out_h\n\n\n\tdef get_edge_angle(fx, fy):\n\t\t# get edge strength\n\t\tedge = np.sqrt(np.power(fx.astype(np.float32), 2) + np.power(fy.astype(np.float32), 2))\n\t\tedge = np.clip(edge, 0, 255)\n\n\t\tfx = np.maximum(fx, 1e-10)\n\t\t#fx[np.abs(fx) <= 1e-5] = 1e-5\n\n\t\t# get edge angle\n\t\tangle = np.arctan(fy / fx)\n\n\t\treturn edge, angle\n\n\n\tdef angle_quantization(angle):\n\t\tangle = angle / np.pi * 180\n\t\tangle[angle < -22.5] = 180 + angle[angle < -22.5]\n\t\t_angle = np.zeros_like(angle, dtype=np.uint8)\n\t\t_angle[np.where(angle <= 22.5)] = 0\n\t\t_angle[np.where((angle > 22.5) & (angle <= 67.5))] = 45\n\t\t_angle[np.where((angle > 67.5) & (angle <= 112.5))] = 90\n\t\t_angle[np.where((angle > 112.5) & (angle <= 157.5))] = 135\n\n\t\treturn _angle\n\n\n\tdef non_maximum_suppression(angle, edge):\n\t\tH, W = angle.shape\n\t\t_edge = edge.copy()\n\t\t\n\t\tfor y in range(H):\n\t\t\tfor x in range(W):\n\t\t\t\t\tif angle[y, x] == 0:\n\t\t\t\t\t\t\tdx1, dy1, dx2, dy2 = -1, 0, 1, 0\n\t\t\t\t\telif angle[y, x] == 45:\n\t\t\t\t\t\t\tdx1, dy1, dx2, dy2 = -1, 1, 1, -1\n\t\t\t\t\telif angle[y, x] == 90:\n\t\t\t\t\t\t\tdx1, dy1, dx2, dy2 = 0, -1, 0, 1\n\t\t\t\t\telif angle[y, x] == 135:\n\t\t\t\t\t\t\tdx1, dy1, dx2, dy2 = -1, -1, 1, 1\n\t\t\t\t\tif x == 0:\n\t\t\t\t\t\t\tdx1 = max(dx1, 0)\n\t\t\t\t\t\t\tdx2 = max(dx2, 0)\n\t\t\t\t\tif x == W-1:\n\t\t\t\t\t\t\tdx1 = min(dx1, 0)\n\t\t\t\t\t\t\tdx2 = min(dx2, 0)\n\t\t\t\t\tif y == 0:\n\t\t\t\t\t\t\tdy1 = max(dy1, 0)\n\t\t\t\t\t\t\tdy2 = max(dy2, 0)\n\t\t\t\t\tif y == H-1:\n\t\t\t\t\t\t\tdy1 = min(dy1, 0)\n\t\t\t\t\t\t\tdy2 = min(dy2, 0)\n\t\t\t\t\tif max(max(edge[y, x], edge[y + dy1, x + dx1]), edge[y + dy2, x + dx2]) != edge[y, x]:\n\t\t\t\t\t\t\t_edge[y, x] = 0\n\n\t\treturn _edge\n\n\tdef hysterisis(edge, HT=100, LT=30):\n\t\tH, W = edge.shape\n\n\t\t# Histeresis threshold\n\t\tedge[edge >= HT] = 255\n\t\tedge[edge <= LT] = 0\n\n\t\t_edge = np.zeros((H + 2, W + 2), dtype=np.float32)\n\t\t_edge[1 : H + 1, 1 : W + 1] = edge\n\n\t\t## 8 - Nearest neighbor\n\t\tnn = np.array(((1., 1., 1.), (1., 0., 1.), (1., 1., 1.)), dtype=np.float32)\n\n\t\tfor y in range(1, H+2):\n\t\t\t\tfor x in range(1, W+2):\n\t\t\t\t\t\tif _edge[y, x] < LT or _edge[y, x] > HT:\n\t\t\t\t\t\t\t\tcontinue\n\t\t\t\t\t\tif np.max(_edge[y-1:y+2, x-1:x+2] * nn) >= HT:\n\t\t\t\t\t\t\t\t_edge[y, x] = 255\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\t_edge[y, x] = 0\n\n\t\tedge = _edge[1:H+1, 1:W+1]\n\t\t\t\t\t\t\t\t\n\t\treturn edge\n\n\t# grayscale\n\tgray = BGR2GRAY(img)\n\n\t# gaussian filtering\n\tgaussian = gaussian_filter(gray, K_size=5, sigma=1.4)\n\n\t# sobel filtering\n\tfy, fx = sobel_filter(gaussian, K_size=3)\n\n\t# get edge strength, angle\n\tedge, angle = get_edge_angle(fx, fy)\n\n\t# angle quantization\n\tangle = angle_quantization(angle)\n\n\t# non maximum suppression\n\tedge = non_maximum_suppression(angle, edge)\n\n\t# hysterisis threshold\n\tout = hysterisis(edge, 100, 30)\n\n\treturn out\n\n\ndef Hough_Line_step2(edge):\n\t## Voting\n\tdef voting(edge):\n\t\tH, W = edge.shape\n\t\t\n\t\tdrho = 1\n\t\tdtheta = 1\n\n\t\t# get rho max length\n\t\trho_max = np.ceil(np.sqrt(H ** 2 + W ** 2)).astype(np.int)\n\n\t\t# hough table\n\t\though = np.zeros((rho_max * 2, 180), dtype=np.int)\n\n\t\t# get index of edge\n\t\tind = np.where(edge == 255)\n\n\t\t## hough transformation\n\t\tfor y, x in zip(ind[0], ind[1]):\n\t\t\t\tfor theta in range(0, 180, dtheta):\n\t\t\t\t\t\t# get polar coordinat4s\n\t\t\t\t\t\tt = np.pi / 180 * theta\n\t\t\t\t\t\trho = int(x * np.cos(t) + y * np.sin(t))\n\n\t\t\t\t\t\t# vote\n\t\t\t\t\t\though[rho + rho_max, theta] += 1\n\t\t\t\t\t\t\t\n\t\tout = hough.astype(np.uint8)\n\n\t\treturn out\n\n\t# non maximum suppression\n\tdef non_maximum_suppression(hough):\n\t\trho_max, _ = hough.shape\n\n\t\t## non maximum suppression\n\t\tfor y in range(rho_max):\n\t\t\tfor x in range(180):\n\t\t\t\t# get 8 nearest neighbor\n\t\t\t\tx1 = max(x-1, 0)\n\t\t\t\tx2 = min(x+2, 180)\n\t\t\t\ty1 = max(y-1, 0)\n\t\t\t\ty2 = min(y+2, rho_max-1)\n\t\t\t\tif np.max(hough[y1:y2, x1:x2]) == hough[y,x] and hough[y, x] != 0:\n\t\t\t\t\tpass\n\t\t\t\t\t#hough[y,x] = 255\n\t\t\t\telse:\n\t\t\t\t\though[y,x] = 0\n\n\t\t# for hough visualization\n\t\t# get top-10 x index of hough table\n\t\tind_x = np.argsort(hough.ravel())[::-1][:20]\n\t\t# get y index\n\t\tind_y = ind_x.copy()\n\t\tthetas = ind_x % 180\n\t\trhos = ind_y // 180\n\t\t_hough = np.zeros_like(hough, dtype=np.int)\n\t\t_hough[rhos, thetas] = 255\n\n\t\treturn _hough\n\n\t# voting\n\though = voting(edge)\n\n\t# non maximum suppression\n\tout = non_maximum_suppression(hough)\n\n\treturn out\n\n\n# Read image\nimg = cv2.imread(\"thorino.jpg\").astype(np.float32)\n\n# Canny\nedge = Canny(img)\n\n# Hough\nout = Hough_Line_step2(edge)\n\nout = out.astype(np.uint8)\n\n# Save result\ncv2.imwrite(\"out.jpg\", out)\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)\ncv2.destroyAllWindows()"}], "task": "code_generation", "index": 244, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_245", "instruction": "Hough変換とは、座標を直交座標から極座標に変換することにより数式に沿って直線や円など一定の形状を検出する手法である。\nある直線状の点では極座標に変換すると一定のr, tにおいて交わる。\nその点が検出すべき直線を表すパラメータであり、このパラメータを逆変換すると直線の方程式を求めることができる。\n\n方法としては、\n1. エッジ画像(ここではCannyの出力)からエッジのピクセルにおいてHough変換を行う。\n2. Hough変換後の値のヒストグラムをとり、極大点を選ぶ。\n3. 極大点のr, tの値をHough逆変換して検出した直線のパラメータを得る。\n\nとなる。\n\npythonを用いて、2の処理で得られた極大値をHough逆変換をして直線を描画しなさい。これで、Hough変換による直線検出が完了する。\n\nアルゴリズムは、\n1. 極大点(r, t)を次式で逆変換する。\n\n\ny = - cos(t) / sin(t) * x + r / sin(t)\nx = - sin(t) / cos(t) * y + r / cos(t)\n\n\n2. 1の逆変換を極大点ごとにy = 0 - H-1, x = 0 - W-1 で行い、入力画像に検出した直線を描画せよ。\nただし、描画するのは赤線(R,G,B) = (255, 0, 0)とする。", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef Canny(img):\n\n\t# Gray scale\n\tdef BGR2GRAY(img):\n\t\tb = img[:, :, 0].copy()\n\t\tg = img[:, :, 1].copy()\n\t\tr = img[:, :, 2].copy()\n\n\t\t# Gray scale\n\t\tout = 0.2126 * r + 0.7152 * g + 0.0722 * b\n\t\tout = out.astype(np.uint8)\n\n\t\treturn out\n\n\n\t# Gaussian filter for grayscale\n\tdef gaussian_filter(img, K_size=3, sigma=1.3):\n\n\t\tif len(img.shape) == 3:\n\t\t\tH, W, C = img.shape\n\t\t\tgray = False\n\t\telse:\n\t\t\timg = np.expand_dims(img, axis=-1)\n\t\t\tH, W, C = img.shape\n\t\t\tgray = True\n\n\t\t## Zero padding\n\t\tpad = K_size // 2\n\t\tout = np.zeros([H + pad * 2, W + pad * 2, C], dtype=np.float)\n\t\tout[pad : pad + H, pad : pad + W] = img.copy().astype(np.float)\n\n\t\t## prepare Kernel\n\t\tK = np.zeros((K_size, K_size), dtype=np.float)\n\t\tfor x in range(-pad, -pad + K_size):\n\t\t\tfor y in range(-pad, -pad + K_size):\n\t\t\t\tK[y + pad, x + pad] = np.exp( - (x ** 2 + y ** 2) / (2 * sigma * sigma))\n\t\t#K /= (sigma * np.sqrt(2 * np.pi))\n\t\tK /= (2 * np.pi * sigma * sigma)\n\t\tK /= K.sum()\n\n\t\ttmp = out.copy()\n\n\t\t# filtering\n\t\tfor y in range(H):\n\t\t\tfor x in range(W):\n\t\t\t\tfor c in range(C):\n\t\t\t\t\tout[pad + y, pad + x, c] = np.sum(K * tmp[y : y + K_size, x : x + K_size, c])\n\n\t\tout = np.clip(out, 0, 255)\n\t\tout = out[pad : pad + H, pad : pad + W]\n\t\tout = out.astype(np.uint8)\n\n\t\tif gray:\n\t\t\tout = out[..., 0]\n\n\t\treturn out\n\n\n\t# sobel filter\n\tdef sobel_filter(img, K_size=3):\n\t\tif len(img.shape) == 3:\n\t\t\tH, W, C = img.shape\n\t\telse:\n\t\t\tH, W = img.shape\n\n\t\t# Zero padding\n\t\tpad = K_size // 2\n\t\tout = np.zeros((H + pad * 2, W + pad * 2), dtype=np.float)\n\t\tout[pad : pad + H, pad : pad + W] = img.copy().astype(np.float)\n\t\ttmp = out.copy()\n\n\t\tout_v = out.copy()\n\t\tout_h = out.copy()\n\n\t\t## Sobel vertical\n\t\tKv = [[1., 2., 1.],[0., 0., 0.], [-1., -2., -1.]]\n\t\t## Sobel horizontal\n\t\tKh = [[1., 0., -1.],[2., 0., -2.],[1., 0., -1.]]\n\n\t\t# filtering\n\t\tfor y in range(H):\n\t\t\tfor x in range(W):\n\t\t\t\tout_v[pad + y, pad + x] = np.sum(Kv * (tmp[y : y + K_size, x : x + K_size]))\n\t\t\t\tout_h[pad + y, pad + x] = np.sum(Kh * (tmp[y : y + K_size, x : x + K_size]))\n\n\t\tout_v = np.clip(out_v, 0, 255)\n\t\tout_h = np.clip(out_h, 0, 255)\n\n\t\tout_v = out_v[pad : pad + H, pad : pad + W]\n\t\tout_v = out_v.astype(np.uint8)\n\t\tout_h = out_h[pad : pad + H, pad : pad + W]\n\t\tout_h = out_h.astype(np.uint8)\n\n\t\treturn out_v, out_h\n\n\n\tdef get_edge_angle(fx, fy):\n\t\t# get edge strength\n\t\tedge = np.sqrt(np.power(fx.astype(np.float32), 2) + np.power(fy.astype(np.float32), 2))\n\t\tedge = np.clip(edge, 0, 255)\n\n\t\tfx = np.maximum(fx, 1e-10)\n\t\t#fx[np.abs(fx) <= 1e-5] = 1e-5\n\n\t\t# get edge angle\n\t\tangle = np.arctan(fy / fx)\n\n\t\treturn edge, angle\n\n\n\tdef angle_quantization(angle):\n\t\tangle = angle / np.pi * 180\n\t\tangle[angle < -22.5] = 180 + angle[angle < -22.5]\n\t\t_angle = np.zeros_like(angle, dtype=np.uint8)\n\t\t_angle[np.where(angle <= 22.5)] = 0\n\t\t_angle[np.where((angle > 22.5) & (angle <= 67.5))] = 45\n\t\t_angle[np.where((angle > 67.5) & (angle <= 112.5))] = 90\n\t\t_angle[np.where((angle > 112.5) & (angle <= 157.5))] = 135\n\n\t\treturn _angle\n\n\n\tdef non_maximum_suppression(angle, edge):\n\t\tH, W = angle.shape\n\t\t_edge = edge.copy()\n\t\t\n\t\tfor y in range(H):\n\t\t\tfor x in range(W):\n\t\t\t\t\tif angle[y, x] == 0:\n\t\t\t\t\t\t\tdx1, dy1, dx2, dy2 = -1, 0, 1, 0\n\t\t\t\t\telif angle[y, x] == 45:\n\t\t\t\t\t\t\tdx1, dy1, dx2, dy2 = -1, 1, 1, -1\n\t\t\t\t\telif angle[y, x] == 90:\n\t\t\t\t\t\t\tdx1, dy1, dx2, dy2 = 0, -1, 0, 1\n\t\t\t\t\telif angle[y, x] == 135:\n\t\t\t\t\t\t\tdx1, dy1, dx2, dy2 = -1, -1, 1, 1\n\t\t\t\t\tif x == 0:\n\t\t\t\t\t\t\tdx1 = max(dx1, 0)\n\t\t\t\t\t\t\tdx2 = max(dx2, 0)\n\t\t\t\t\tif x == W-1:\n\t\t\t\t\t\t\tdx1 = min(dx1, 0)\n\t\t\t\t\t\t\tdx2 = min(dx2, 0)\n\t\t\t\t\tif y == 0:\n\t\t\t\t\t\t\tdy1 = max(dy1, 0)\n\t\t\t\t\t\t\tdy2 = max(dy2, 0)\n\t\t\t\t\tif y == H-1:\n\t\t\t\t\t\t\tdy1 = min(dy1, 0)\n\t\t\t\t\t\t\tdy2 = min(dy2, 0)\n\t\t\t\t\tif max(max(edge[y, x], edge[y + dy1, x + dx1]), edge[y + dy2, x + dx2]) != edge[y, x]:\n\t\t\t\t\t\t\t_edge[y, x] = 0\n\n\t\treturn _edge\n\n\tdef hysterisis(edge, HT=100, LT=30):\n\t\tH, W = edge.shape\n\n\t\t# Histeresis threshold\n\t\tedge[edge >= HT] = 255\n\t\tedge[edge <= LT] = 0\n\n\t\t_edge = np.zeros((H + 2, W + 2), dtype=np.float32)\n\t\t_edge[1 : H + 1, 1 : W + 1] = edge\n\n\t\t## 8 - Nearest neighbor\n\t\tnn = np.array(((1., 1., 1.), (1., 0., 1.), (1., 1., 1.)), dtype=np.float32)\n\n\t\tfor y in range(1, H+2):\n\t\t\t\tfor x in range(1, W+2):\n\t\t\t\t\t\tif _edge[y, x] < LT or _edge[y, x] > HT:\n\t\t\t\t\t\t\t\tcontinue\n\t\t\t\t\t\tif np.max(_edge[y-1:y+2, x-1:x+2] * nn) >= HT:\n\t\t\t\t\t\t\t\t_edge[y, x] = 255\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\t_edge[y, x] = 0\n\n\t\tedge = _edge[1:H+1, 1:W+1]\n\t\t\t\t\t\t\t\t\n\t\treturn edge\n\n\t# grayscale\n\tgray = BGR2GRAY(img)\n\n\t# gaussian filtering\n\tgaussian = gaussian_filter(gray, K_size=5, sigma=1.4)\n\n\t# sobel filtering\n\tfy, fx = sobel_filter(gaussian, K_size=3)\n\n\t# get edge strength, angle\n\tedge, angle = get_edge_angle(fx, fy)\n\n\t# angle quantization\n\tangle = angle_quantization(angle)\n\n\t# non maximum suppression\n\tedge = non_maximum_suppression(angle, edge)\n\n\t# hysterisis threshold\n\tout = hysterisis(edge, 100, 30)\n\n\treturn out\n\n\ndef Hough_Line(edge, img):\n\t## Voting\n\tdef voting(edge):\n\t\tH, W = edge.shape\n\t\t\n\t\tdrho = 1\n\t\tdtheta = 1\n\n\t\t# get rho max length\n\t\trho_max = np.ceil(np.sqrt(H ** 2 + W ** 2)).astype(np.int)\n\n\t\t# hough table\n\t\though = np.zeros((rho_max * 2, 180), dtype=np.int)\n\n\t\t# get index of edge\n\t\tind = np.where(edge == 255)\n\n\t\t## hough transformation\n\t\tfor y, x in zip(ind[0], ind[1]):\n\t\t\t\tfor theta in range(0, 180, dtheta):\n\t\t\t\t\t\t# get polar coordinat4s\n\t\t\t\t\t\tt = np.pi / 180 * theta\n\t\t\t\t\t\trho = int(x * np.cos(t) + y * np.sin(t))\n\n\t\t\t\t\t\t# vote\n\t\t\t\t\t\though[rho + rho_max, theta] += 1\n\t\t\t\t\t\t\t\n\t\tout = hough.astype(np.uint8)\n\n\t\treturn out\n\n\t# non maximum suppression\n\tdef non_maximum_suppression(hough):\n\t\trho_max, _ = hough.shape\n\n\t\t## non maximum suppression\n\t\tfor y in range(rho_max):\n\t\t\tfor x in range(180):\n\t\t\t\t# get 8 nearest neighbor\n\t\t\t\tx1 = max(x-1, 0)\n\t\t\t\tx2 = min(x+2, 180)\n\t\t\t\ty1 = max(y-1, 0)\n\t\t\t\ty2 = min(y+2, rho_max-1)\n\t\t\t\tif np.max(hough[y1:y2, x1:x2]) == hough[y,x] and hough[y, x] != 0:\n\t\t\t\t\tpass\n\t\t\t\t\t#hough[y,x] = 255\n\t\t\t\telse:\n\t\t\t\t\though[y,x] = 0\n\n\t\treturn hough\n\n\tdef inverse_hough(hough, img):\n\t\tH, W, _ = img.shape\n\t\trho_max, _ = hough.shape\n\n\t\tout = img.copy()\n\n\t\t# get x, y index of hough table\n\t\tind_x = np.argsort(hough.ravel())[::-1][:20]\n\t\tind_y = ind_x.copy()\n\t\tthetas = ind_x % 180\n\t\trhos = ind_y // 180 - rho_max / 2\n\n\t\t# each theta and rho\n\t\tfor theta, rho in zip(thetas, rhos):\n\t\t\t# theta[radian] -> angle[degree]\n\t\t\tt = np.pi / 180. * theta\n\n\t\t\t# hough -> (x,y)\n\t\t\tfor x in range(W):\n\t\t\t\tif np.sin(t) != 0:\n\t\t\t\t\ty = - (np.cos(t) / np.sin(t)) * x + (rho) / np.sin(t)\n\t\t\t\t\ty = int(y)\n\t\t\t\t\tif y >= H or y < 0:\n\t\t\t\t\t\tcontinue\n\t\t\t\t\tout[y, x] = [0, 0, 255]\n\t\t\tfor y in range(H):\n\t\t\t\tif np.cos(t) != 0:\n\t\t\t\t\tx = - (np.sin(t) / np.cos(t)) * y + (rho) / np.cos(t)\n\t\t\t\t\tx = int(x)\n\t\t\t\t\tif x >= W or x < 0:\n\t\t\t\t\t\tcontinue\n\t\t\t\t\tout[y, x] = [0, 0, 255]\n\t\t\t\t\n\t\tout = out.astype(np.uint8)\n\n\t\treturn out\n\n\n\t# voting\n\though = voting(edge)\n\n\t# non maximum suppression\n\though = non_maximum_suppression(hough)\n\n\t# inverse hough\n\tout = inverse_hough(hough, img)\n\n\treturn out\n\n\n# Read image\nimg = cv2.imread(\"thorino.jpg\").astype(np.float32)\n\n# Canny\nedge = Canny(img)\n\n# Hough\nout = Hough_Line(edge, img)\n\nout = out.astype(np.uint8)\n\n# Save result\ncv2.imwrite(\"out.jpg\", out)\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)\ncv2.destroyAllWindows()"}], "task": "code_generation", "index": 245, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_246", "instruction": "pythonを用いて、imori.jpgを大津の二値化したものに、モルフォロジー処理による膨張を2回行え。\n\nモルフォロジー処理とは二値化画像の白(255)マス部分を4近傍(上下左右1マス)に膨張、または1マスだけ収縮させる処理をいう。\n\nこの膨張と収縮を何度も繰り返すことで1マスだけに存在する白マスを消したり(Q.49. オープニング処理)、本来つながってほしい白マスを結合させたりできる(Q.50. クロージング処理)。\n\nモルフォロジー処理の膨張(Dilation)アルゴリズムは、\n注目画素I(x, y)=0で、I(x, y-1), I(x-1, y), I(x+1, y), I(x, y+1)のどれか一つが255なら、I(x, y) = 255 とする。\n\nつまり、上の処理を2回行えば2マス分膨張できることになる。\n\nモルフォロジー処理の実装は例えば、[[0,1,0], [1,0,1], [0,1,0]] のフィルタを掛けた和が255以上なら膨張である、と考えることもできる。", "instances": [{"input": "", "output": "mport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n# Gray scale\ndef BGR2GRAY(img):\n\tb = img[:, :, 0].copy()\n\tg = img[:, :, 1].copy()\n\tr = img[:, :, 2].copy()\n\n\t# Gray scale\n\tout = 0.2126 * r + 0.7152 * g + 0.0722 * b\n\tout = out.astype(np.uint8)\n\n\treturn out\n\n# Otsu Binalization\ndef otsu_binarization(img, th=128):\n\tH, W = img.shape\n\tout = img.copy()\n\n\tmax_sigma = 0\n\tmax_t = 0\n\n\t# determine threshold\n\tfor _t in range(1, 255):\n\t\tv0 = out[np.where(out < _t)]\n\t\tm0 = np.mean(v0) if len(v0) > 0 else 0.\n\t\tw0 = len(v0) / (H * W)\n\t\tv1 = out[np.where(out >= _t)]\n\t\tm1 = np.mean(v1) if len(v1) > 0 else 0.\n\t\tw1 = len(v1) / (H * W)\n\t\tsigma = w0 * w1 * ((m0 - m1) ** 2)\n\t\tif sigma > max_sigma:\n\t\t\tmax_sigma = sigma\n\t\t\tmax_t = _t\n\n\t# Binarization\n\tprint(\"threshold >>\", max_t)\n\tth = max_t\n\tout[out < th] = 0\n\tout[out >= th] = 255\n\n\treturn out\n\n\n# Morphology Erode\ndef Morphology_Erode(img, Dil_time=1):\n\tH, W = img.shape\n\n\t# kernel\n\tMF = np.array(((0, 1, 0),\n\t\t\t\t(1, 0, 1),\n\t\t\t\t(0, 1, 0)), dtype=np.int)\n\n\t# each dilate time\n\tout = img.copy()\n\tfor i in range(Dil_time):\n\t\ttmp = np.pad(out, (1, 1), 'edge')\n\t\tfor y in range(1, H+1):\n\t\t\tfor x in range(1, W+1):\n\t\t\t\tif np.sum(MF * tmp[y-1:y+2, x-1:x+2]) >= 255:\n\t\t\t\t\tout[y-1, x-1] = 255\n\n\treturn out\n\n# Read image\nimg = cv2.imread(\"imori.jpg\").astype(np.float32)\n\n\n# Grayscale\ngray = BGR2GRAY(img)\n\n# Otsu's binarization\notsu = otsu_binarization(gray)\n\n# Morphology - dilate\nout = Morphology_Erode(otsu, Dil_time=2)\n\n# Save result\ncv2.imwrite(\"out.jpg\", out)\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)\ncv2.destroyAllWindows()"}], "task": "code_generation", "index": 246, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_247", "instruction": "pythonを用いて、imori.jpgを大津の二値化したものに、モルフォロジー処理による収縮を2回行え。\n\nモルフォロジー処理の収縮(Erosion)アルゴリズムは、\n注目画素I(x, y)=255で、I(x, y-1), I(x-1, y), I(x+1, y), I(x, y+1)のどれか一つでも0なら、I(x, y) = 0 とする。\n\n収縮処理は例えば、[[0,1,0], [1,0,1], [0,1,0]] のフィルタを掛けた和が255*4未満なら収縮である、と考えることもできる。", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Gray scale\ndef BGR2GRAY(img):\n\tb = img[:, :, 0].copy()\n\tg = img[:, :, 1].copy()\n\tr = img[:, :, 2].copy()\n\n\t# Gray scale\n\tout = 0.2126 * r + 0.7152 * g + 0.0722 * b\n\tout = out.astype(np.uint8)\n\n\treturn out\n\n# Otsu Binalization\ndef otsu_binarization(img, th=128):\n\tH, W = img.shape\n\tout = img.copy()\n\n\tmax_sigma = 0\n\tmax_t = 0\n\n\t# determine threshold\n\tfor _t in range(1, 255):\n\t\tv0 = out[np.where(out < _t)]\n\t\tm0 = np.mean(v0) if len(v0) > 0 else 0.\n\t\tw0 = len(v0) / (H * W)\n\t\tv1 = out[np.where(out >= _t)]\n\t\tm1 = np.mean(v1) if len(v1) > 0 else 0.\n\t\tw1 = len(v1) / (H * W)\n\t\tsigma = w0 * w1 * ((m0 - m1) ** 2)\n\t\tif sigma > max_sigma:\n\t\t\tmax_sigma = sigma\n\t\t\tmax_t = _t\n\n\t# Binarization\n\tprint(\"threshold >>\", max_t)\n\tth = max_t\n\tout[out < th] = 0\n\tout[out >= th] = 255\n\n\treturn out\n\n\n# Morphology Dilate\ndef Morphology_Dilate(img, Erode_time=1):\n\tH, W = img.shape\n\tout = img.copy()\n\n\t# kernel\n\tMF = np.array(((0, 1, 0),\n\t\t\t\t(1, 0, 1),\n\t\t\t\t(0, 1, 0)), dtype=np.int)\n\n\t# each erode\n\tfor i in range(Erode_time):\n\t\ttmp = np.pad(out, (1, 1), 'edge')\n\t\t# erode\n\t\tfor y in range(1, H+1):\n\t\t\tfor x in range(1, W+1):\n\t\t\t\tif np.sum(MF * tmp[y-1:y+2, x-1:x+2]) < 255*4:\n\t\t\t\t\tout[y-1, x-1] = 0\n\n\treturn out\n\n\n# Read image\nimg = cv2.imread(\"imori.jpg\").astype(np.float32)\n\n\n# Grayscale\ngray = BGR2GRAY(img)\n\n# Otsu's binarization\notsu = otsu_binarization(gray)\n\n# Morphology - dilate\nout = Morphology_Dilate(otsu, Erode_time=2)\n\n\n# Save result\ncv2.imwrite(\"out.jpg\", out)\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)\ncv2.destroyAllWindows()"}], "task": "code_generation", "index": 247, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_248", "instruction": "pythonを用いて、大津の二値化後に、オープニング処理(N=1)を行え。\n\nオープニング処理とは、モルフォロジー処理の収縮をN回行った後に膨張をN回行う処理である。\n\nオープニング処理により、一つだけ余分に存在する画素などを削除できる。", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Gray scale\ndef BGR2GRAY(img):\n\tb = img[:, :, 0].copy()\n\tg = img[:, :, 1].copy()\n\tr = img[:, :, 2].copy()\n\n\t# Gray scale\n\tout = 0.2126 * r + 0.7152 * g + 0.0722 * b\n\tout = out.astype(np.uint8)\n\n\treturn out\n\n# Otsu Binalization\ndef otsu_binarization(img, th=128):\n\tH, W = img.shape\n\tout = img.copy()\n\n\tmax_sigma = 0\n\tmax_t = 0\n\n\t# determine threshold\n\tfor _t in range(1, 255):\n\t\tv0 = out[np.where(out < _t)]\n\t\tm0 = np.mean(v0) if len(v0) > 0 else 0.\n\t\tw0 = len(v0) / (H * W)\n\t\tv1 = out[np.where(out >= _t)]\n\t\tm1 = np.mean(v1) if len(v1) > 0 else 0.\n\t\tw1 = len(v1) / (H * W)\n\t\tsigma = w0 * w1 * ((m0 - m1) ** 2)\n\t\tif sigma > max_sigma:\n\t\t\tmax_sigma = sigma\n\t\t\tmax_t = _t\n\n\t# Binarization\n\tprint(\"threshold >>\", max_t)\n\tth = max_t\n\tout[out < th] = 0\n\tout[out >= th] = 255\n\n\treturn out\n\n\n# Morphology Dilate\ndef Morphology_Dilate(img, Erode_time=1):\n\tH, W = img.shape\n\tout = img.copy()\n\n\t# kernel\n\tMF = np.array(((0, 1, 0),\n\t\t\t\t(1, 0, 1),\n\t\t\t\t(0, 1, 0)), dtype=np.int)\n\n\t# each erode\n\tfor i in range(Erode_time):\n\t\ttmp = np.pad(out, (1, 1), 'edge')\n\t\t# erode\n\t\tfor y in range(1, H+1):\n\t\t\tfor x in range(1, W+1):\n\t\t\t\tif np.sum(MF * tmp[y-1:y+2, x-1:x+2]) < 255*4:\n\t\t\t\t\tout[y-1, x-1] = 0\n\n\treturn out\n\n\n# Morphology Erode\ndef Morphology_Erode(img, Dil_time=1):\n\tH, W = img.shape\n\n\t# kernel\n\tMF = np.array(((0, 1, 0),\n\t\t\t\t(1, 0, 1),\n\t\t\t\t(0, 1, 0)), dtype=np.int)\n\n\t# each dilate time\n\tout = img.copy()\n\tfor i in range(Dil_time):\n\t\ttmp = np.pad(out, (1, 1), 'edge')\n\t\tfor y in range(1, H+1):\n\t\t\tfor x in range(1, W+1):\n\t\t\t\tif np.sum(MF * tmp[y-1:y+2, x-1:x+2]) >= 255:\n\t\t\t\t\tout[y-1, x-1] = 255\n\n\treturn out\n\n\n# Opening morphology\ndef Morphology_Opening(img, time=1):\n\tout = Morphology_Dilate(img, Erode_time=time)\n\tout = Morphology_Erode(out, Dil_time=time)\n\treturn out\n\n\n# Read image\nimg = cv2.imread(\"imori.jpg\").astype(np.float32)\n\n\n# Grayscale\ngray = BGR2GRAY(img)\n\n# Otsu's binarization\notsu = otsu_binarization(gray)\n\n# Morphology - opening\nout = Morphology_Opening(otsu, time=1)\n\n# Save result\ncv2.imwrite(\"out.jpg\", out)\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)\ncv2.destroyAllWindows()"}], "task": "code_generation", "index": 248, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_249", "instruction": "pythonを用いて、Canny検出した後に、クロージング処理(N=1)を行え。\n\nクロージング処理とは、モルフォロジー処理の膨張をN回行った後に収縮をN回行う処理である。\n\nクロージング処理により、途中で途切れた画素を結合することができる。", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n# Canny\ndef Canny(img):\n\n\t# Gray scale\n\tdef BGR2GRAY(img):\n\t\tb = img[:, :, 0].copy()\n\t\tg = img[:, :, 1].copy()\n\t\tr = img[:, :, 2].copy()\n\n\t\t# Gray scale\n\t\tout = 0.2126 * r + 0.7152 * g + 0.0722 * b\n\t\tout = out.astype(np.uint8)\n\n\t\treturn out\n\n\n\t# Gaussian filter for grayscale\n\tdef gaussian_filter(img, K_size=3, sigma=1.3):\n\n\t\tif len(img.shape) == 3:\n\t\t\tH, W, C = img.shape\n\t\t\tgray = False\n\t\telse:\n\t\t\timg = np.expand_dims(img, axis=-1)\n\t\t\tH, W, C = img.shape\n\t\t\tgray = True\n\n\t\t## Zero padding\n\t\tpad = K_size // 2\n\t\tout = np.zeros([H + pad * 2, W + pad * 2, C], dtype=np.float)\n\t\tout[pad : pad + H, pad : pad + W] = img.copy().astype(np.float)\n\n\t\t## prepare Kernel\n\t\tK = np.zeros((K_size, K_size), dtype=np.float)\n\t\tfor x in range(-pad, -pad + K_size):\n\t\t\tfor y in range(-pad, -pad + K_size):\n\t\t\t\tK[y + pad, x + pad] = np.exp( - (x ** 2 + y ** 2) / (2 * sigma * sigma))\n\t\t#K /= (sigma * np.sqrt(2 * np.pi))\n\t\tK /= (2 * np.pi * sigma * sigma)\n\t\tK /= K.sum()\n\n\t\ttmp = out.copy()\n\n\t\t# filtering\n\t\tfor y in range(H):\n\t\t\tfor x in range(W):\n\t\t\t\tfor c in range(C):\n\t\t\t\t\tout[pad + y, pad + x, c] = np.sum(K * tmp[y : y + K_size, x : x + K_size, c])\n\n\t\tout = np.clip(out, 0, 255)\n\t\tout = out[pad : pad + H, pad : pad + W]\n\t\tout = out.astype(np.uint8)\n\n\t\tif gray:\n\t\t\tout = out[..., 0]\n\n\t\treturn out\n\n\n\t# sobel filter\n\tdef sobel_filter(img, K_size=3):\n\t\tif len(img.shape) == 3:\n\t\t\tH, W, C = img.shape\n\t\telse:\n\t\t\tH, W = img.shape\n\n\t\t# Zero padding\n\t\tpad = K_size // 2\n\t\tout = np.zeros((H + pad * 2, W + pad * 2), dtype=np.float)\n\t\tout[pad : pad + H, pad : pad + W] = img.copy().astype(np.float)\n\t\ttmp = out.copy()\n\n\t\tout_v = out.copy()\n\t\tout_h = out.copy()\n\n\t\t## Sobel vertical\n\t\tKv = [[1., 2., 1.],[0., 0., 0.], [-1., -2., -1.]]\n\t\t## Sobel horizontal\n\t\tKh = [[1., 0., -1.],[2., 0., -2.],[1., 0., -1.]]\n\n\t\t# filtering\n\t\tfor y in range(H):\n\t\t\tfor x in range(W):\n\t\t\t\tout_v[pad + y, pad + x] = np.sum(Kv * (tmp[y : y + K_size, x : x + K_size]))\n\t\t\t\tout_h[pad + y, pad + x] = np.sum(Kh * (tmp[y : y + K_size, x : x + K_size]))\n\n\t\tout_v = np.clip(out_v, 0, 255)\n\t\tout_h = np.clip(out_h, 0, 255)\n\n\t\tout_v = out_v[pad : pad + H, pad : pad + W]\n\t\tout_v = out_v.astype(np.uint8)\n\t\tout_h = out_h[pad : pad + H, pad : pad + W]\n\t\tout_h = out_h.astype(np.uint8)\n\n\t\treturn out_v, out_h\n\n\n\tdef get_edge_angle(fx, fy):\n\t\t# get edge strength\n\t\tedge = np.sqrt(np.power(fx.astype(np.float32), 2) + np.power(fy.astype(np.float32), 2))\n\t\tedge = np.clip(edge, 0, 255)\n\n\t\tfx = np.maximum(fx, 1e-10)\n\t\t#fx[np.abs(fx) <= 1e-5] = 1e-5\n\n\t\t# get edge angle\n\t\tangle = np.arctan(fy / fx)\n\n\t\treturn edge, angle\n\n\n\tdef angle_quantization(angle):\n\t\tangle = angle / np.pi * 180\n\t\tangle[angle < -22.5] = 180 + angle[angle < -22.5]\n\t\t_angle = np.zeros_like(angle, dtype=np.uint8)\n\t\t_angle[np.where(angle <= 22.5)] = 0\n\t\t_angle[np.where((angle > 22.5) & (angle <= 67.5))] = 45\n\t\t_angle[np.where((angle > 67.5) & (angle <= 112.5))] = 90\n\t\t_angle[np.where((angle > 112.5) & (angle <= 157.5))] = 135\n\n\t\treturn _angle\n\n\n\tdef non_maximum_suppression(angle, edge):\n\t\tH, W = angle.shape\n\t\t_edge = edge.copy()\n\t\t\n\t\tfor y in range(H):\n\t\t\tfor x in range(W):\n\t\t\t\t\tif angle[y, x] == 0:\n\t\t\t\t\t\t\tdx1, dy1, dx2, dy2 = -1, 0, 1, 0\n\t\t\t\t\telif angle[y, x] == 45:\n\t\t\t\t\t\t\tdx1, dy1, dx2, dy2 = -1, 1, 1, -1\n\t\t\t\t\telif angle[y, x] == 90:\n\t\t\t\t\t\t\tdx1, dy1, dx2, dy2 = 0, -1, 0, 1\n\t\t\t\t\telif angle[y, x] == 135:\n\t\t\t\t\t\t\tdx1, dy1, dx2, dy2 = -1, -1, 1, 1\n\t\t\t\t\tif x == 0:\n\t\t\t\t\t\t\tdx1 = max(dx1, 0)\n\t\t\t\t\t\t\tdx2 = max(dx2, 0)\n\t\t\t\t\tif x == W-1:\n\t\t\t\t\t\t\tdx1 = min(dx1, 0)\n\t\t\t\t\t\t\tdx2 = min(dx2, 0)\n\t\t\t\t\tif y == 0:\n\t\t\t\t\t\t\tdy1 = max(dy1, 0)\n\t\t\t\t\t\t\tdy2 = max(dy2, 0)\n\t\t\t\t\tif y == H-1:\n\t\t\t\t\t\t\tdy1 = min(dy1, 0)\n\t\t\t\t\t\t\tdy2 = min(dy2, 0)\n\t\t\t\t\tif max(max(edge[y, x], edge[y + dy1, x + dx1]), edge[y + dy2, x + dx2]) != edge[y, x]:\n\t\t\t\t\t\t\t_edge[y, x] = 0\n\n\t\treturn _edge\n\n\tdef hysterisis(edge, HT=100, LT=30):\n\t\tH, W = edge.shape\n\n\t\t# Histeresis threshold\n\t\tedge[edge >= HT] = 255\n\t\tedge[edge <= LT] = 0\n\n\t\t_edge = np.zeros((H + 2, W + 2), dtype=np.float32)\n\t\t_edge[1 : H + 1, 1 : W + 1] = edge\n\n\t\t## 8 - Nearest neighbor\n\t\tnn = np.array(((1., 1., 1.), (1., 0., 1.), (1., 1., 1.)), dtype=np.float32)\n\n\t\tfor y in range(1, H+2):\n\t\t\t\tfor x in range(1, W+2):\n\t\t\t\t\t\tif _edge[y, x] < LT or _edge[y, x] > HT:\n\t\t\t\t\t\t\t\tcontinue\n\t\t\t\t\t\tif np.max(_edge[y-1:y+2, x-1:x+2] * nn) >= HT:\n\t\t\t\t\t\t\t\t_edge[y, x] = 255\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\t_edge[y, x] = 0\n\n\t\tedge = _edge[1:H+1, 1:W+1]\n\t\t\t\t\t\t\t\t\n\t\treturn edge\n\n\t# grayscale\n\tgray = BGR2GRAY(img)\n\n\t# gaussian filtering\n\tgaussian = gaussian_filter(gray, K_size=5, sigma=1.4)\n\n\t# sobel filtering\n\tfy, fx = sobel_filter(gaussian, K_size=3)\n\n\t# get edge strength, angle\n\tedge, angle = get_edge_angle(fx, fy)\n\n\t# angle quantization\n\tangle = angle_quantization(angle)\n\n\t# non maximum suppression\n\tedge = non_maximum_suppression(angle, edge)\n\n\t# hysterisis threshold\n\tout = hysterisis(edge, 50, 20)\n\n\treturn out\n\n\n# Morphology Dilate\ndef Morphology_Dilate(img, Erode_time=1):\n\tH, W = img.shape\n\tout = img.copy()\n\n\t# kernel\n\tMF = np.array(((0, 1, 0),\n\t\t\t\t(1, 0, 1),\n\t\t\t\t(0, 1, 0)), dtype=np.int)\n\n\t# each erode\n\tfor i in range(Erode_time):\n\t\ttmp = np.pad(out, (1, 1), 'edge')\n\t\t# erode\n\t\tfor y in range(1, H+1):\n\t\t\tfor x in range(1, W+1):\n\t\t\t\tif np.sum(MF * tmp[y-1:y+2, x-1:x+2]) < 255*4:\n\t\t\t\t\tout[y-1, x-1] = 0\n\n\treturn out\n\n\n# Morphology Erode\ndef Morphology_Erode(img, Dil_time=1):\n\tH, W = img.shape\n\n\t# kernel\n\tMF = np.array(((0, 1, 0),\n\t\t\t\t(1, 0, 1),\n\t\t\t\t(0, 1, 0)), dtype=np.int)\n\n\t# each dilate time\n\tout = img.copy()\n\tfor i in range(Dil_time):\n\t\ttmp = np.pad(out, (1, 1), 'edge')\n\t\tfor y in range(1, H+1):\n\t\t\tfor x in range(1, W+1):\n\t\t\t\tif np.sum(MF * tmp[y-1:y+2, x-1:x+2]) >= 255:\n\t\t\t\t\tout[y-1, x-1] = 255\n\n\treturn out\n\n# Morphology Closing\ndef Morphology_Closing(img, time=1):\n\tout = Morphology_Erode(img, Dil_time=time)\n\tout = Morphology_Dilate(out, Erode_time=time)\n\treturn out\n\n\n# Read image\nimg = cv2.imread(\"imori.jpg\").astype(np.float32)\n\n# Canny\ncanny = Canny(img)\n\n# Morphology - opening\nout = Morphology_Closing(canny, time=1)\n\n# Save result\ncv2.imwrite(\"out.jpg\", out)\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)\ncv2.destroyAllWindows()"}], "task": "code_generation", "index": 249, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_250", "instruction": "pythonを用いて、大津の二値化を行った後、モルフォロジー勾配を求めよ。\n\nモルフォロジー勾配とはモルフォロジー膨張の画像と収縮の画像の差分をとることで、物体の境界線を抽出する手法である。\n\nここではモルフォロジー処理のN=1とする。", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n# Gray scale\ndef BGR2GRAY(img):\n\tb = img[:, :, 0].copy()\n\tg = img[:, :, 1].copy()\n\tr = img[:, :, 2].copy()\n\n\t# Gray scale\n\tout = 0.2126 * r + 0.7152 * g + 0.0722 * b\n\tout = out.astype(np.uint8)\n\n\treturn out\n\n# Otsu Binalization\ndef otsu_binarization(img, th=128):\n\tH, W = img.shape\n\tout = img.copy()\n\n\tmax_sigma = 0\n\tmax_t = 0\n\n\t# determine threshold\n\tfor _t in range(1, 255):\n\t\tv0 = out[np.where(out < _t)]\n\t\tm0 = np.mean(v0) if len(v0) > 0 else 0.\n\t\tw0 = len(v0) / (H * W)\n\t\tv1 = out[np.where(out >= _t)]\n\t\tm1 = np.mean(v1) if len(v1) > 0 else 0.\n\t\tw1 = len(v1) / (H * W)\n\t\tsigma = w0 * w1 * ((m0 - m1) ** 2)\n\t\tif sigma > max_sigma:\n\t\t\tmax_sigma = sigma\n\t\t\tmax_t = _t\n\n\t# Binarization\n\tprint(\"threshold >>\", max_t)\n\tth = max_t\n\tout[out < th] = 0\n\tout[out >= th] = 255\n\n\treturn out\n\n\n# Erosion\ndef Erode(img, Erode_time=1):\n\tH, W = img.shape\n\tout = img.copy()\n\n\t# kernel\n\tMF = np.array(((0, 1, 0),\n\t\t\t\t(1, 0, 1),\n\t\t\t\t(0, 1, 0)), dtype=np.int)\n\n\t# each erode\n\tfor i in range(Erode_time):\n\t\ttmp = np.pad(out, (1, 1), 'edge')\n\t\t# erode\n\t\tfor y in range(1, H+1):\n\t\t\tfor x in range(1, W+1):\n\t\t\t\tif np.sum(MF * tmp[y-1:y+2, x-1:x+2]) < 255*4:\n\t\t\t\t\tout[y-1, x-1] = 0\n\n\treturn out\n\n\n# Dilation\ndef Dilate(img, Dil_time=1):\n\tH, W = img.shape\n\n\t# kernel\n\tMF = np.array(((0, 1, 0),\n\t\t\t\t(1, 0, 1),\n\t\t\t\t(0, 1, 0)), dtype=np.int)\n\n\t# each dilate time\n\tout = img.copy()\n\tfor i in range(Dil_time):\n\t\ttmp = np.pad(out, (1, 1), 'edge')\n\t\tfor y in range(1, H+1):\n\t\t\tfor x in range(1, W+1):\n\t\t\t\tif np.sum(MF * tmp[y-1:y+2, x-1:x+2]) >= 255:\n\t\t\t\t\tout[y-1, x-1] = 255\n\n\treturn out\n\n\n# Read image\nimg = cv2.imread(\"imori.jpg\").astype(np.float32)\n\n# Grayscale\ngray = BGR2GRAY(img)\n\n# Otsu's binarization\notsu = otsu_binarization(gray)\n\n# Erode image\neroded = Erode(otsu)\n\n# Delate image\ndilated = Dilate(otsu)\n\n# Morphology\nout = np.abs(eroded - dilated) * 255\n                \n# Save result\ncv2.imwrite(\"out.jpg\", out)\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)\ncv2.destroyAllWindows()"}], "task": "code_generation", "index": 250, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_251", "instruction": "pythonを用いて、大津の二値化を行った後、トップハット変換を行え。\n\nトップハット変換とは元画像からオープニング処理を行った画像を差し引いた画像であり、細い線状のものやノイズなどを抽出できると言われる。\n\nここでは、大津の二値化画像からオープニング処理画像(N=3)を差し引いて求めよ。", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Gray scale\ndef BGR2GRAY(img):\n\tb = img[:, :, 0].copy()\n\tg = img[:, :, 1].copy()\n\tr = img[:, :, 2].copy()\n\n\t# Gray scale\n\tout = 0.2126 * r + 0.7152 * g + 0.0722 * b\n\tout = out.astype(np.uint8)\n\n\treturn out\n\n# Otsu Binalization\ndef otsu_binarization(img, th=128):\n\tH, W = img.shape\n\tout = img.copy()\n\n\tmax_sigma = 0\n\tmax_t = 0\n\n\t# determine threshold\n\tfor _t in range(1, 255):\n\t\tv0 = out[np.where(out < _t)]\n\t\tm0 = np.mean(v0) if len(v0) > 0 else 0.\n\t\tw0 = len(v0) / (H * W)\n\t\tv1 = out[np.where(out >= _t)]\n\t\tm1 = np.mean(v1) if len(v1) > 0 else 0.\n\t\tw1 = len(v1) / (H * W)\n\t\tsigma = w0 * w1 * ((m0 - m1) ** 2)\n\t\tif sigma > max_sigma:\n\t\t\tmax_sigma = sigma\n\t\t\tmax_t = _t\n\n\t# Binarization\n\tprint(\"threshold >>\", max_t)\n\tth = max_t\n\tout[out < th] = 0\n\tout[out >= th] = 255\n\n\treturn out\n\n\n# Erosion\ndef Erode(img, Erode_time=1):\n\tH, W = img.shape\n\tout = img.copy()\n\n\t# kernel\n\tMF = np.array(((0, 1, 0),\n\t\t\t\t(1, 0, 1),\n\t\t\t\t(0, 1, 0)), dtype=np.int)\n\n\t# each erode\n\tfor i in range(Erode_time):\n\t\ttmp = np.pad(out, (1, 1), 'edge')\n\t\t# erode\n\t\tfor y in range(1, H+1):\n\t\t\tfor x in range(1, W+1):\n\t\t\t\tif np.sum(MF * tmp[y-1:y+2, x-1:x+2]) < 255*4:\n\t\t\t\t\tout[y-1, x-1] = 0\n\n\treturn out\n\n\n# Dilation\ndef Dilate(img, Dil_time=1):\n\tH, W = img.shape\n\n\t# kernel\n\tMF = np.array(((0, 1, 0),\n\t\t\t\t(1, 0, 1),\n\t\t\t\t(0, 1, 0)), dtype=np.int)\n\n\t# each dilate time\n\tout = img.copy()\n\tfor i in range(Dil_time):\n\t\ttmp = np.pad(out, (1, 1), 'edge')\n\t\tfor y in range(1, H+1):\n\t\t\tfor x in range(1, W+1):\n\t\t\t\tif np.sum(MF * tmp[y-1:y+2, x-1:x+2]) >= 255:\n\t\t\t\t\tout[y-1, x-1] = 255\n\n\treturn out\n\n# Opening morphology\ndef Morphology_Opening(img, time=1):\n    dil = Dilate(img, Dil_time=time)\n    erode = Erode(dil, Erode_time=time)\n    return erode\n\n# Read image\nimg = cv2.imread(\"imori.jpg\").astype(np.float32)\n\n# Grayscale\ngray = BGR2GRAY(img)\n\n# Otsu's binarization\notsu = otsu_binarization(gray)\n\n# Opening process\nopened = Morphology_Opening(otsu, time=3)\n\n# Tophat\nout = np.abs(otsu - opened) * 255\n                \n# Save result\ncv2.imwrite(\"out.jpg\", out)\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)\ncv2.destroyAllWindows()"}], "task": "code_generation", "index": 251, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_252", "instruction": "pythonを用いて、大津の二値化を行った後、ブラックハット変換を行え。\n\nブラックハット変換とはクロージング画像から元画像を差し引いた画像であり、これもトップ変換同様に細い線状やノイズを抽出できると言われる。\n\nここでは、クロージング処理画像(N=3)から大津の二値化画像を差し引いて求めよ。", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n# Gray scale\ndef BGR2GRAY(img):\n\tb = img[:, :, 0].copy()\n\tg = img[:, :, 1].copy()\n\tr = img[:, :, 2].copy()\n\n\t# Gray scale\n\tout = 0.2126 * r + 0.7152 * g + 0.0722 * b\n\tout = out.astype(np.uint8)\n\n\treturn out\n\n# Otsu Binalization\ndef otsu_binarization(img, th=128):\n\tH, W = img.shape\n\tout = img.copy()\n\n\tmax_sigma = 0\n\tmax_t = 0\n\n\t# determine threshold\n\tfor _t in range(1, 255):\n\t\tv0 = out[np.where(out < _t)]\n\t\tm0 = np.mean(v0) if len(v0) > 0 else 0.\n\t\tw0 = len(v0) / (H * W)\n\t\tv1 = out[np.where(out >= _t)]\n\t\tm1 = np.mean(v1) if len(v1) > 0 else 0.\n\t\tw1 = len(v1) / (H * W)\n\t\tsigma = w0 * w1 * ((m0 - m1) ** 2)\n\t\tif sigma > max_sigma:\n\t\t\tmax_sigma = sigma\n\t\t\tmax_t = _t\n\n\t# Binarization\n\tprint(\"threshold >>\", max_t)\n\tth = max_t\n\tout[out < th] = 0\n\tout[out >= th] = 255\n\n\treturn out\n\n\n# Erosion\ndef Erode(img, Erode_time=1):\n\tH, W = img.shape\n\tout = img.copy()\n\n\t# kernel\n\tMF = np.array(((0, 1, 0),\n\t\t\t\t(1, 0, 1),\n\t\t\t\t(0, 1, 0)), dtype=np.int)\n\n\t# each erode\n\tfor i in range(Erode_time):\n\t\ttmp = np.pad(out, (1, 1), 'edge')\n\t\t# erode\n\t\tfor y in range(1, H+1):\n\t\t\tfor x in range(1, W+1):\n\t\t\t\tif np.sum(MF * tmp[y-1:y+2, x-1:x+2]) < 255*4:\n\t\t\t\t\tout[y-1, x-1] = 0\n\n\treturn out\n\n\n# Dilation\ndef Dilate(img, Dil_time=1):\n\tH, W = img.shape\n\n\t# kernel\n\tMF = np.array(((0, 1, 0),\n\t\t\t\t(1, 0, 1),\n\t\t\t\t(0, 1, 0)), dtype=np.int)\n\n\t# each dilate time\n\tout = img.copy()\n\tfor i in range(Dil_time):\n\t\ttmp = np.pad(out, (1, 1), 'edge')\n\t\tfor y in range(1, H+1):\n\t\t\tfor x in range(1, W+1):\n\t\t\t\tif np.sum(MF * tmp[y-1:y+2, x-1:x+2]) >= 255:\n\t\t\t\t\tout[y-1, x-1] = 255\n\n\treturn out\n\n# Closing morphology\ndef Morphology_Closing(img, time=1):\n    erode = Erode(img, Erode_time=time)\n    dil = Dilate(erode, Dil_time=time)\n    return erode\n\n\n\n# Read image\nimg = cv2.imread(\"imori.jpg\").astype(np.float32)\n\n# Grayscale\ngray = BGR2GRAY(img)\n\n# Otsu's binarization\notsu = otsu_binarization(gray)\n\n# Opening process\nopened = Morphology_Closing(otsu, time=3)\n\n# Tophat\nout = np.abs(opened - otsu) * 255\n                \n# Save result\ncv2.imwrite(\"out.jpg\", out)\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)\ncv2.destroyAllWindows()"}], "task": "code_generation", "index": 252, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_253", "instruction": "pythonを用いて、かつテンプレートマッチングのSSDを用いて、imori_part.jpgがimori.jpgのどこに位置するかをimori.jpgの赤の矩形で図示せよ。\n\nテンプレートマッチングとは、テンプレート画像と全体画像の一部分で類似度が高い位置を探す手法であり、物体検出などで使われる。今では物体検出はCNNで行われるが、テンプレートマッチングは最も基本処理となる。\n\nアルゴリズムとしては、画像I (H x W)、テンプレート画像T (h x w)とすると、\n\n1. 画像Iにおいて、for ( j = 0, H-h)  for ( i = 0, W-w)と1ピクセルずつずらしながら画像Aの一部分I(i:i+w, j:j+h)とテンプレート画像の類似度Sを計算する。\n2. Sが最大もしくは最小の位置がマッチング位置となる。\n\nSの選び方は主にSSD, SAD(Q.55), NCC(Q.56), ZNCC(Q.57)などがあり、それぞれ最大値をとるか最小値をとるか異なる。\n\nここではSSD(Sum of Squared Difference)を用いる。\nSSDとは画素値の差分の二乗値の和を類似度にする手法であり、Sが最小の位置がマッチング位置となる。", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n# template matching\ndef Template_matching(img, template):\n    # get original image shape\n    H, W, C = img.shape\n\n    # get template image shape\n    Ht, Wt, Ct = template.shape\n\n    # Templete matching\n    # prepare x, y index\n    i, j = -1, -1\n    # prepare evaluate value\n    v = 255 * H * W * C\n\n    for y in range(H - Ht):\n        for x in range(W - Wt):\n            # get SSD value\n            _v = np.sum((img[y : y + Ht, x : x + Wt] - template) ** 2)\n\n            # if SSD is min\n            if _v < v:\n                v = _v\n                i, j = x, y\n\n    out = img.copy()\n    # draw rectangle\n    cv2.rectangle(out, pt1=(i, j), pt2=(i+Wt, j+Ht), color=(0,0,255), thickness=1)\n    out = out.astype(np.uint8)\n\n    return out\n\n# Read image\nimg = cv2.imread(\"imori.jpg\").astype(np.float32)\n\n# Read templete image\ntemplate = cv2.imread(\"imori_part.jpg\").astype(np.float32)\n\n# Template matching\nout = Template_matching(img, template)\n\n                \n# Save result\ncv2.imwrite(\"out.jpg\", out)\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)\ncv2.destroyAllWindows()"}], "task": "code_generation", "index": 253, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_254", "instruction": "pythonを用いて、かつテンプレートマッチングのSADを用いて、imori_part.jpgがimori.jpgのどこに位置するかをimori.jpgの赤の矩形で図示せよ。\n\nSAD(Sum of Absolute Difference)とは画素値の差分の絶対値の和を類似度にする手法であり、Sが最小の位置がマッチング位置となる。\n\nS = Sum_{x=0:w, y=0:h} |I(i+x, j+y) - T(x, y)|", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# template matching\ndef Template_matching(img, template):\n    # get original image shape\n    H, W, C = img.shape\n\n    # get template image shape\n    Ht, Wt, Ct = template.shape\n\n    # Templete matching\n    # prepare x, y index\n    i, j = -1, -1\n    # prepare evaluate value\n    v = 255 * H * W * C\n\n    for y in range(H - Ht):\n        for x in range(W - Wt):\n            # get SAD value\n            _v = np.sum(np.abs(img[y : y + Ht, x : x + Wt] - template))\n\n            # if SAD is min\n            if _v < v:\n                v = _v\n                i, j = x, y\n\n    out = img.copy()\n    # draw rectangle\n    cv2.rectangle(out, pt1=(i, j), pt2=(i+Wt, j+Ht), color=(0,0,255), thickness=1)\n    out = out.astype(np.uint8)\n\n    return out\n\n# Read image\nimg = cv2.imread(\"imori.jpg\").astype(np.float32)\n\n# Read templete image\ntemplate = cv2.imread(\"imori_part.jpg\").astype(np.float32)\n\n# Template matching\nout = Template_matching(img, template)\n\n                \n# Save result\ncv2.imwrite(\"out.jpg\", out)\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)\ncv2.destroyAllWindows()"}], "task": "code_generation", "index": 254, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_255", "instruction": "pythonを用いて、かつテンプレートマッチングのNCCを用いて、imori_part.jpgがimori.jpgのどこに位置するかをimori.jpgの赤の矩形で図示せよ。\n\nNCC(Normalized Cross Correlation)とは正規化相互相関を類似度にする手法であり、Sが最大の位置がマッチング位置となる。\n\n     Sum_{x=0:w, y=0:h} I(i+x, j+y) * T(x, y)\nS = -----------------------------------------------------------------------------\n    Sqrt(Sum_{x=0:w, y=0:h} I(i+x, j+y)^2) * Sqrt(Sum_{x=0:w, y=0:h} T(x, y)^2)\n\nこのSは、-1<=S<=1をとる。\nNCCは照明変化に強いと言われる。", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# template matching\ndef Template_matching(img, template):\n    # get original image shape\n    H, W, C = img.shape\n\n    # get template image shape\n    Ht, Wt, Ct = template.shape\n\n    # Templete matching\n    # prepare x, y index\n    i, j = -1, -1\n    # prepare evaluate value\n    v = -1\n\n    for y in range(H - Ht):\n        for x in range(W - Wt):\n            # get NCC value\n            # get numerator of NCC\n            _v = np.sum(img[y : y + Ht, x : x + Wt] * template)\n            # devided numerator\n            _v /= (np.sqrt(np.sum(img[y : y + Ht, x : x + Wt] ** 2)) * np.sqrt(np.sum(template ** 2)))\n\n            # if NCC is max\n            if _v > v:\n                v = _v\n                i, j = x, y\n\n    out = img.copy()\n    # draw rectangle\n    cv2.rectangle(out, pt1=(i, j), pt2=(i+Wt, j+Ht), color=(0,0,255), thickness=1)\n    out = out.astype(np.uint8)\n\n    return out\n\n# Read image\nimg = cv2.imread(\"imori.jpg\").astype(np.float32)\n\n# Read templete image\ntemplate = cv2.imread(\"imori_part.jpg\").astype(np.float32)\n\n# Template matching\nout = Template_matching(img, template)\n                \n# Save result\ncv2.imwrite(\"out.jpg\", out)\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)\ncv2.destroyAllWindows()"}], "task": "code_generation", "index": 255, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_256", "instruction": "pythonを用いて、かつテンプレートマッチングのZNCCを用いて、imori_part.jpgがimori.jpgのどこに位置するかをimori.jpgの赤の矩形で図示せよ。\n\nZNCC(Zero means Normalized Cross Correlation)とは零平均正規化相互相関を類似度にする手法であり、Sが最大の位置がマッチング位置となる。\n\n画像Iの平均値をmi、画像Tの平均値をmtとすると、Sは次式で計算される。（ただし、平均値はRGB成分ごとに減算する）\n\n\n       Sum_{x=0:w, y=0:h} (I(i+x, j+y)-mi) * (T(x, y)-mt)\nS = --------------------------------------------------------------------------------------\n    Sqrt(Sum_{x=0:w, y=0:h} (I(i+x, j+y)-mi)^2) * Sqrt(Sum_{x=0:w, y=0:h} (T(x, y)-mt)^2)\n\n\nこのSは、-1<=S<=1をとる。\nZNCCは平均値を引くことでNCCよりも照明変化に強いと言われる。", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n# template matching\ndef Template_matching(img, template):\n    # get original image shape\n    H, W, C = img.shape\n\n    # subtract mean BGR\n    _img = img - np.mean(img, axis=(0, 1))\n\n    # get template image shape\n    Ht, Wt, Ct = template.shape\n\n    # subtract mean BGR\n    _template = template - np.mean(img, axis=(0, 1))\n\n    # Templete matching\n    # prepare x, y index\n    i, j = -1, -1\n    # prepare evaluate value\n    v = -1\n\n    for y in range(H - Ht):\n        for x in range(W - Wt):\n            # get ZNCC value\n            # get numerator of ZNCC\n            _v = np.sum(_img[y : y + Ht, x : x + Wt] * _template)\n            # devided numerator\n            _v /= (np.sqrt(np.sum(_img[y : y + Ht, x : x + Wt] ** 2)) * np.sqrt(np.sum(template ** 2)))\n\n            # if ZNCC is max\n            if _v > v:\n                v = _v\n                i, j = x, y\n\n    out = img.copy()\n    # draw rectangle\n    cv2.rectangle(out, pt1=(i, j), pt2=(i+Wt, j+Ht), color=(0,0,255), thickness=1)\n    out = out.astype(np.uint8)\n\n    return out\n\n# Read image\nimg = cv2.imread(\"imori.jpg\").astype(np.float32)\n\n# Read templete image\ntemplate = cv2.imread(\"imori_part.jpg\").astype(np.float32)\n\n# Template matching\nout = Template_matching(img, template)\n\n\n# Save result\ncv2.imwrite(\"out.jpg\", out)\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)\ncv2.destroyAllWindows()"}], "task": "code_generation", "index": 256, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_257", "instruction": "pythonを用いて、seg.pngをラベリングせよ。\n\nラベリングとは隣接したピクセルに同じラベルを割り当てる作業である。\n\nつまり、\n\n黒　黒　黒　黒\n黒　白　白　黒\n黒　白　黒　黒\n黒　黒　黒　黒\n\nこのように隣り合った白ピクセルは同じラベルを割り当てる。\n\nこのようにピクセルの塊にラベリングしたものはConnected Componentとも呼ばれる。\n\nここでは４近傍に注目してラベリングを行う。\nまた、ここではルックアップテーブルというものを使用する。\n\nルックアップテーブルとは\n\n|   Source   |    Distination   | \n|     1      |         1        |\n|     2      |         2        |\n|     3      |         1        |\n\nというような表になっており、Source=1に割り当てた画素には最終的にラベル1を割り当てる、Source =3に割り当てた画素には最終的にラベル1を割り当てることを示す表である。\n\nアルゴリズムは\n1. 左上からラスタスキャンを行う。\n2. 注目画素i(x,y)が黒画素なら何も行わない。白画素なら、上画素i(x,y-1)と左画素i(x-1,y)に注目し、どちらも0だった場合、最後に割り当てたラベル+1を割り当てる。\n3. どちらか一方以上が0でない場合（つまりすでにラベルが割り合っている場合）、上と左に割り当てられたラベルの中で最小の方(0以外)をi(x,y)に割り当てる。ここで、上か左で用いなかったラベルに対応するルックアップテーブルをここで割り当てた番号に変える。\n4. 最後、ルックアップテーブルを見て、Sourceに対応する画素に当たる部分をDistinationの値に変換する。\n\n以上により隣接ピクセル同士に同じラベルを割り当てる。\n4近傍としているが、ラスタスキャンのため、上画素と左画素の２画素に注目すればいい。", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# labeling 4 nearest neighbor\ndef labeling_4nn(img):\n    # get image shape\n    H, W, C = img.shape\n\n    # prepare label tempolary image\n    label = np.zeros((H, W), dtype=np.int)\n    label[img[..., 0]>0] = 1\n\n    # look up table\n    LUT = [0 for _ in range(H*W)]\n\n    n = 1\n\n    for y in range(H):\n        for x in range(W):\n            # skip black pixel\n            if label[y, x] == 0:\n                continue\n            \n            # get above pixel\n            c3 = label[max(y-1,0), x]\n\n            # get left pixel\n            c5 = label[y, max(x-1,0)]\n\n            # if not labeled\n            if c3 < 2 and c5 < 2:\n                # labeling\n                n += 1\n                label[y, x] = n\n            else:\n                # replace min label index\n                _vs = [c3, c5]\n                vs = [a for a in _vs if a > 1]\n                v = min(vs)\n                label[y, x] = v\n                \n                minv = v\n                for _v in vs:\n                    if LUT[_v] != 0:\n                        minv = min(minv, LUT[_v])\n                for _v in vs:\n                    LUT[_v] = minv\n                    \n    count = 1\n\n    # integrate index of look up table\n    for l in range(2, n+1):\n        flag = True\n        for i in range(n+1):\n            if LUT[i] == l:\n                if flag:\n                    count += 1\n                    flag = False\n                LUT[i] = count\n\n    # draw color\n    COLORS = [[0, 0, 255], [0, 255, 0], [255, 0, 0], [255, 255, 0]]\n    out = np.zeros((H, W, C), dtype=np.uint8)\n\n    for i, lut in enumerate(LUT[2:]):\n        out[label == (i+2)] = COLORS[lut-2]\n\n    return out\n    \n\n# Read image\nimg = cv2.imread(\"seg.png\").astype(np.float32)\n\n# labeling 4 nearest neighbor\nout = labeling_4nn(img)\n\n# Save result\ncv2.imwrite(\"out.png\", out)\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)\ncv2.destroyAllWindows()"}], "task": "code_generation", "index": 257, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_258", "instruction": "pythonを用いて、8近傍のラベリングを行え。ラベリングとは隣接したピクセルに同じラベルを割り当てる作業である。\n\nつまり、\n\n黒　黒　黒　黒\n黒　白　白　黒\n黒　白　黒　黒\n黒　黒　黒　黒\n\nこのように隣り合った白ピクセルは同じラベルを割り当てる。\n\nこのようにピクセルの塊にラベリングしたものはConnected Componentとも呼ばれる。\nここではルックアップテーブルというものを使用する。\n\nルックアップテーブルとは\n\n|   Source   |    Distination   | \n|     1      |         1        |\n|     2      |         2        |\n|     3      |         1        |\n\nというような表になっており、Source=1に割り当てた画素には最終的にラベル1を割り当てる、Source =3に割り当てた画素には最終的にラベル1を割り当てることを示す表である。\n\nアルゴリズムは\n1. 左上からラスタスキャンを行う。\n2. 注目画素i(x,y)が黒画素なら何も行わない。白画素なら、上画素i(x,y-1)と左画素i(x-1,y)に注目し、どちらも0だった場合、最後に割り当てたラベル+1を割り当てる。\n3. どちらか一方以上が0でない場合（つまりすでにラベルが割り合っている場合）、上と左に割り当てられたラベルの中で最小の方(0以外)をi(x,y)に割り当てる。ここで、上か左で用いなかったラベルに対応するルックアップテーブルをここで割り当てた番号に変える。\n4. 最後、ルックアップテーブルを見て、Sourceに対応する画素に当たる部分をDistinationの値に変換する。\n\n以上により隣接ピクセル同士に同じラベルを割り当てる。\n\n8近傍とは、i(x-1,y-1), i(x, y-1), i(x+1,y-1), i(x-1,y)の4画素に注目すればよい。", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n# labeling 8 nearest neighbor\ndef labeling_8nn(img):\n    # get image shape\n    H, W, C = img.shape\n\n    # prepare labeling image\n    label = np.zeros((H, W), dtype=np.int)\n    label[img[..., 0]>0] = 1\n\n    # look up table\n    LUT = [0 for _ in range(H*W)]\n\n    n = 1\n\n    for y in range(H):\n        for x in range(W):\n            if label[y, x] == 0:\n                continue\n            # get right top pixel\n            c2 = label[max(y-1,0), min(x+1, W-1)]\n            # get top pixel\n            c3 = label[max(y-1,0), x]\n            # get left top pixel\n            c4 = label[max(y-1,0), max(x-1,0)]\n            # get left pixel\n            c5 = label[y, max(x-1,0)]\n\n            # if all pixel is non labeled\n            if c3 < 2 and c5 < 2 and c2 < 2 and c4 < 2:\n                n += 1\n                label[y, x] = n\n            else:\n                # get labeled index\n                _vs = [c3, c5, c2, c4]\n                vs = [a for a in _vs if a > 1]\n                v = min(vs)\n                label[y, x] = v\n\n                minv = v\n                for _v in vs:\n                    if LUT[_v] != 0:\n                        minv = min(minv, LUT[_v])\n                for _v in vs:\n                    LUT[_v] = minv\n                    \n    count = 1\n\n    # integrate labeled index of look up table\n    for l in range(2, n+1):\n        flag = True\n        for i in range(n+1):\n            if LUT[i] == l:\n                if flag:\n                    count += 1\n                    flag = False\n                LUT[i] = count\n\n    # draw color\n    COLORS = [[0, 0, 255], [0, 255, 0], [255, 0, 0], [255, 255, 0]]\n    out = np.zeros((H, W, C), dtype=np.uint8)\n\n    for i, lut in enumerate(LUT[2:]):\n        out[label == (i+2)] = COLORS[lut-2]\n\n    return out\n    \n\n# Read image\nimg = cv2.imread(\"seg.png\").astype(np.float32)\n\n# labeling 8 nearest neighbor\nout = labeling_8nn(img)\n\n# Save result\ncv2.imwrite(\"out.png\", out)\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)\ncv2.destroyAllWindows()"}], "task": "code_generation", "index": 258, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_259", "instruction": "pythonを用いて、アルファブレンドにより、imori.jpgとthorino.jpgを6:4の割合で画像を合成せよ。\n\nアルファブレンドとは透明度（アルファ値）を設定することにより画像の透明度を設定する方法である。\nOpenCVでは透明度のパラメータはないが、PILなどのライブラリでは存在する。\nここではその透明度を手動で設定する。\n\n二つの画像を重ね合わせたい時などに、この手法は有効である。\n\nimg1とimg2を1:1の割合で重ね合わせたい時は、次式となる。\nalphaの値を変えることで重ねる時の重みを変えることができる。\n\nalpha = 0.5\nout = img1 * alpha + img2 * (1 - alpha)", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n# alpha blend\ndef alpha_blend(img1, img2, alpha):\n\t# blend\n\tout = img * alpha + img2 * (1 - alpha)\n\tout = out.astype(np.uint8)\n\treturn out\n\n# Read image\nimg = cv2.imread(\"imori.jpg\").astype(np.float32)\n\n# Read blend target image\nimg2 = cv2.imread(\"thorino.jpg\").astype(np.float32)\n\nout = alpha_blend(img, img2, alpha=0.6)\n    \n# Save result\ncv2.imwrite(\"out.jpg\", out)\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)\ncv2.destroyAllWindows()"}], "task": "code_generation", "index": 259, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_260", "instruction": "pythonを用いて、renketsu.pngを4-連結数により、色分けせよ。\n\n4-連結数とは近傍との画素の状態を見る値である。\n通常、近傍は注目画素x0(x,y)が0でない場合に対して、次のように定義される。\n\nx4(x-1,y-1) x3(x,y-1) x2(x+1,y-1)\nx5(x-1,y)   x0(x,y)   x1(x+1,y)\nx6(x-1,y+1) x7(x,y+1) x8(x+1,y+1)\n\nここで4連結数とは、次式で計算される。\n\nS = (x1 - x1 x2 x3) + (x3 - x3 x4 x5) + (x5 - x5 x6 x7) + (x7 - x7 x8 x1) \n\nS = [0,4]の範囲をとり、\n- S = 0 は内部点\n- S = 1 は端点\n- S = 2 は連結点\n- S = 3 は分岐点\n- S = 4 は交差点\nを示す。", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Connect 4\ndef connect_4(img):\n    # get shape\n    H, W, C = img.shape\n\n    # prepare temporary image\n    tmp = np.zeros((H, W), dtype=np.int)\n\n    # binarize\n    tmp[img[..., 0] > 0] = 1\n\n    # prepare out image\n    out = np.zeros((H, W, 3), dtype=np.uint8)\n\n    # each pixel\n    for y in range(H):\n        for x in range(W):\n            if tmp[y, x] < 1:\n                continue\n\n            S = 0\n            S += (tmp[y,min(x+1,W-1)] - tmp[y,min(x+1,W-1)] * tmp[max(y-1,0),min(x+1,W-1)] * tmp[max(y-1,0),x])\n            S += (tmp[max(y-1,0),x] - tmp[max(y-1,0),x] * tmp[max(y-1,0),max(x-1,0)] * tmp[y,max(x-1,0)])\n            S += (tmp[y,max(x-1,0)] - tmp[y,max(x-1,0)] * tmp[min(y+1,H-1),max(x-1,0)] * tmp[min(y+1,H-1),x])\n            S += (tmp[min(y+1,H-1),x] - tmp[min(y+1,H-1),x] * tmp[min(y+1,H-1),min(x+1,W-1)] * tmp[y,min(x+1,W-1)])\n            \n            if S == 0:\n                out[y,x] = [0, 0, 255]\n            elif S == 1:\n                out[y,x] = [0, 255, 0]\n            elif S == 2:\n                out[y,x] = [255, 0, 0]\n            elif S == 3:\n                out[y,x] = [255, 255, 0]\n            elif S == 4:\n                out[y,x] = [255, 0, 255]\n                    \n    out = out.astype(np.uint8)\n\n    return out\n\n\n\n# Read image\nimg = cv2.imread(\"renketsu.png\").astype(np.float32)\n\n# connect 4\nout = connect_4(img)\n\n# Save result\ncv2.imwrite(\"out.png\", out)\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)\ncv2.destroyAllWindows()"}], "task": "code_generation", "index": 260, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_261", "instruction": "pythonを用いて、renketsu.pngを8-連結数により、色分けせよ。\n\n8連結数とは\n\nS = (x1 - x1 x2 x3) + (x3 - x3 x4 x5) + (x5 - x5 x6 x7) + (x7 - x7 x8 x1) \n\nにおいて各x¥*の値の0と1を反転させた値を用いる。", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# connect 8\ndef connect_8(img):\n    # get shape\n    H, W, C = img.shape\n\n    # prepare temporary\n    _tmp = np.zeros((H, W), dtype=np.int)\n\n    # get binarize\n    _tmp[img[..., 0] > 0] = 1\n\n    # inverse for connect 8\n    tmp = 1 - _tmp\n\n    # prepare image\n    out = np.zeros((H, W, 3), dtype=np.uint8)\n\n    # each pixel\n    for y in range(H):\n        for x in range(W):\n            if _tmp[y, x] < 1:\n                continue\n\n            S = 0\n            S += (tmp[y,min(x+1,W-1)] - tmp[y,min(x+1,W-1)] * tmp[max(y-1,0),min(x+1,W-1)] * tmp[max(y-1,0),x])\n            S += (tmp[max(y-1,0),x] - tmp[max(y-1,0),x] * tmp[max(y-1,0),max(x-1,0)] * tmp[y,max(x-1,0)])\n            S += (tmp[y,max(x-1,0)] - tmp[y,max(x-1,0)] * tmp[min(y+1,H-1),max(x-1,0)] * tmp[min(y+1,H-1),x])\n            S += (tmp[min(y+1,H-1),x] - tmp[min(y+1,H-1),x] * tmp[min(y+1,H-1),min(x+1,W-1)] * tmp[y,min(x+1,W-1)])\n            \n            if S == 0:\n                out[y,x] = [0, 0, 255]\n            elif S == 1:\n                out[y,x] = [0, 255, 0]\n            elif S == 2:\n                out[y,x] = [255, 0, 0]\n            elif S == 3:\n                out[y,x] = [255, 255, 0]\n            elif S == 4:\n                out[y,x] = [255, 0, 255]\n                    \n    out = out.astype(np.uint8)\n\n    return out\n\n\n# Read image\nimg = cv2.imread(\"renketsu.png\").astype(np.float32)\n\n# connect 8\nout = connect_8(img)\n\n\n# Save result\ncv2.imwrite(\"out.png\", out)\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)\ncv2.destroyAllWindows()"}], "task": "code_generation", "index": 261, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_262", "instruction": "pythonを用いて、gazo.pngを細線化せよ。\n\n細線化とは画素の幅を1にする処理であり、ここでは次のアルゴリズムに沿って処理を行え。\n\n1. 左上からラスタスキャンする。\n2. x0(x,y)=0ならば、処理なし。x0(x,y)=1ならば次の3条件を満たす時にx0=0に変える。\n(1) 注目画素の4近傍に0が一つ以上存在する\n(2) x0の4-連結数が1である\n(3) x0の8近傍に1が3つ以上存在する\n3. 一回のラスタスキャンで2の変更数が0になるまで、ラスタスキャンを繰り返す。\n\n細線化にはヒルディッチのアルゴリズム(Q.64)や、Zhang-Suenのアルゴリズム(Q.65)、田村のアルゴリズムなどが存在する。", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n# thining algorythm\ndef thining(img):\n    # get shape\n    H, W, C = img.shape\n\n    # prepare out image\n    out = np.zeros((H, W), dtype=np.int)\n    out[img[..., 0] > 0] = 1\n\n    count = 1\n    while count > 0:\n        count = 0\n        tmp = out.copy()\n        # each pixel ( rasta scan )\n        for y in range(H):\n            for x in range(W):\n                # skip black pixel\n                if out[y, x] < 1:\n                    continue\n                \n                # count satisfied conditions\n                judge = 0\n                \n                ## condition 1\n                if (tmp[y, min(x+1, W-1)] + tmp[max(y-1, 0), x] + tmp[y, max(x-1, 0)] + tmp[min(y+1, H-1), x]) < 4:\n                    judge += 1\n                    \n                ## condition 2\n                c = 0\n                c += (tmp[y,min(x+1, W-1)] - tmp[y, min(x+1, W-1)] * tmp[max(y-1, 0),min(x+1, W-1)] * tmp[max(y-1, 0), x])\n                c += (tmp[max(y-1,0), x] - tmp[max(y-1,0), x] * tmp[max(y-1, 0), max(x-1, 0)] * tmp[y, max(x-1, 0)])\n                c += (tmp[y, max(x-1, 0)] - tmp[y,max(x-1, 0)] * tmp[min(y+1, H-1), max(x-1, 0)] * tmp[min(y+1, H-1), x])\n                c += (tmp[min(y+1, H-1), x] - tmp[min(y+1, H-1), x] * tmp[min(y+1, H-1), min(x+1, W-1)] * tmp[y, min(x+1, W-1)])\n                if c == 1:\n                    judge += 1\n                    \n                ##x condition 3\n                if np.sum(tmp[max(y-1, 0) : min(y+2, H), max(x-1, 0) : min(x+2, W)]) >= 4:\n                    judge += 1\n                \n                # if all conditions are satisfied\n                if judge == 3:\n                    out[y, x] = 0\n                    count += 1\n\n    out = out.astype(np.uint8) * 255\n\n    return out\n\n\n# Read image\nimg = cv2.imread(\"gazo.png\").astype(np.float32)\n\n# thining\nout = thining(img)\n\n# Save result\ncv2.imwrite(\"out.png\", out)\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)\ncv2.destroyAllWindows()"}], "task": "code_generation", "index": 262, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_263", "instruction": "pythonを用いて、gazo.pngにヒルディッチの細線化を行え。\n\nアルゴリズムは、次の通り。\n\n1. 左上からラスタスキャンする。\n2. x0(x,y)=0ならば、処理なし。x0(x,y)=1ならば次の5条件を満たす時にx0=-1に変える。\n    1. 注目画素の4近傍に0が一つ以上存在する\n    2. x0の8-連結数が1である\n    3. x1〜x8の絶対値の合計が2以上\n    4. x0の8近傍に1が1つ以上存在する\n    5. xn(n=1〜8)全てに対して以下のどちらかが成り立つ\n        - xnが-1以外\n        - xnを0とした時、x0の8-連結数が1である\n3. 各画素の-1を0に変える\n4. 一回のラスタスキャンで3の変更数が0になるまで、ラスタスキャンを繰り返す。", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n# hilditch thining\ndef hilditch(img):\n    # get shape\n    H, W, C = img.shape\n\n    # prepare out image\n    out = np.zeros((H, W), dtype=np.int)\n    out[img[..., 0] > 0] = 1\n\n    # inverse pixel value\n    tmp = out.copy()\n    _tmp = 1 - tmp\n\n    count = 1\n    while count > 0:\n        count = 0\n        tmp = out.copy()\n        _tmp = 1 - tmp\n\n        tmp2 = out.copy()\n        _tmp2 = 1 - tmp2\n        \n        # each pixel\n        for y in range(H):\n            for x in range(W):\n                # skip black pixel\n                if out[y, x] < 1:\n                    continue\n                \n                judge = 0\n                \n                ## condition 1\n                if (tmp[y, min(x+1, W-1)] * tmp[max(y-1,0 ), x] * tmp[y, max(x-1, 0)] * tmp[min(y+1, H-1), x]) == 0:\n                    judge += 1\n                    \n                ## condition 2\n                c = 0\n                c += (_tmp[y, min(x+1, W-1)] - _tmp[y, min(x+1, W-1)] * _tmp[max(y-1, 0), min(x+1, W-1)] * _tmp[max(y-1, 0), x])\n                c += (_tmp[max(y-1, 0), x] - _tmp[max(y-1, 0), x] * _tmp[max(y-1, 0), max(x-1, 0)] * _tmp[y, max(x-1, 0)])\n                c += (_tmp[y, max(x-1, 0)] - _tmp[y, max(x-1, 0)] * _tmp[min(y+1, H-1), max(x-1, 0)] * _tmp[min(y+1, H-1), x])\n                c += (_tmp[min(y+1, H-1), x] - _tmp[min(y+1, H-1), x] * _tmp[min(y+1, H-1), min(x+1, W-1)] * _tmp[y, min(x+1, W-1)])\n                if c == 1:\n                    judge += 1\n                    \n                ## condition 3\n                if np.sum(tmp[max(y-1, 0) : min(y+2, H), max(x-1, 0) : min(x+2, W)]) >= 3:\n                    judge += 1\n\n                ## condition 4\n                if np.sum(out[max(y-1, 0) : min(y+2, H), max(x-1, 0) : min(x+2, W)]) >= 2:\n                    judge += 1\n\n                ## condition 5\n                _tmp2 = 1 - out\n\n                c = 0\n                c += (_tmp2[y, min(x+1, W-1)] - _tmp2[y, min(x+1, W-1)] * _tmp2[max(y-1, 0), min(x+1, W-1)] * _tmp2[max(y-1, 0), x])\n                c += (_tmp2[max(y-1, 0), x] - _tmp2[max(y-1, 0), x] * (1 - tmp[max(y-1, 0), max(x-1, 0)]) * _tmp2[y, max(x-1, 0)])\n                c += (_tmp2[y, max(x-1, 0)] - _tmp2[y, max(x-1, 0)] * _tmp2[min(y+1, H-1), max(x-1, 0)] * _tmp2[min(y+1, H-1), x])\n                c += (_tmp2[min(y+1, H-1), x] - _tmp2[min(y+1, H-1), x] * _tmp2[min(y+1, H-1), min(x+1, W-1)] * _tmp2[y, min(x+1, W-1)])\n                if c == 1 or (out[max(y-1, 0), max(x-1,0 )] != tmp[max(y-1, 0), max(x-1, 0)]):\n                    judge += 1\n\n                c = 0\n                c += (_tmp2[y, min(x+1, W-1)] - _tmp2[y, min(x+1, W-1)] * _tmp2[max(y-1, 0), min(x+1, W-1)] * (1 - tmp[max(y-1, 0), x]))\n                c += ((1-tmp[max(y-1, 0), x]) - (1 - tmp[max(y-1, 0), x]) * _tmp2[max(y-1, 0), max(x-1, 0)] * _tmp2[y, max(x-1, 0)])\n                c += (_tmp2[y, max(x-1,0 )] - _tmp2[y, max(x-1,0 )] * _tmp2[min(y+1, H-1), max(x-1, 0)] * _tmp2[min(y+1, H-1), x])\n                c += (_tmp2[min(y+1, H-1), x] - _tmp2[min(y+1, H-1), x] * _tmp2[min(y+1, H-1), min(x+1, W-1)] * _tmp2[y, min(x+1, W-1)])\n                if c == 1 or (out[max(y-1, 0), x] != tmp[max(y-1, 0), x]):\n                    judge += 1\n\n                c = 0\n                c += (_tmp2[y, min(x+1, W-1)] - _tmp2[y, min(x+1, W-1)] * (1 - tmp[max(y-1, 0), min(x+1, W-1)]) * _tmp2[max(y-1, 0), x])\n                c += (_tmp2[max(y-1, 0), x] - _tmp2[max(y-1, 0), x] * _tmp2[max(y-1, 0), max(x-1, 0)] * _tmp2[y, max(x-1, 0)])\n                c += (_tmp2[y, max(x-1, 0)] - _tmp2[y, max(x-1, 0)] * _tmp2[min(y+1, H-1), max(x-1, 0)] * _tmp2[min(y+1, H-1), x])\n                c += (_tmp2[min(y+1, H-1), x] - _tmp2[min(y+1, H-1), x] * _tmp2[min(y+1, H-1), min(x+1, W-1)] * _tmp2[y, min(x+1, W-1)])\n                if c == 1 or (out[max(y-1, 0), min(x+1, W-1)] != tmp[max(y-1, 0), min(x+1, W-1)]):\n                    judge += 1\n\n                c = 0\n                c += (_tmp2[y, min(x+1, W-1)] - _tmp2[y, min(x+1, W-1)] * _tmp2[max(y-1, 0), min(x+1, W-1)] * _tmp2[max(y-1, 0), x])\n                c += (_tmp2[max(y-1, 0), x] - _tmp2[max(y-1, 0), x] * _tmp2[max(y-1, 0), max(x-1, 0)] * (1 - tmp[y, max(x-1, 0)]))\n                c += ((1 - tmp[y, max(x-1, 0)]) - (1 - tmp[y, max(x-1, 0)]) * _tmp2[min(y+1, H-1), max(x-1, 0)] * _tmp2[min(y+1, H-1), x])\n                c += (_tmp2[min(y+1, H-1), x] - _tmp2[min(y+1, H-1), x] * _tmp2[min(y+1, H-1), min(x+1, W-1)] * _tmp2[y, min(x+1, W-1)])\n                if c == 1 or (out[y, max(x-1, 0)] != tmp[y, max(x-1, 0)]):\n                    judge += 1\n                \n                if judge >= 8:\n                    out[y, x] = 0\n                    count += 1\n                    \n    out = out.astype(np.uint8) * 255\n\n    return out\n\n\n# Read image\nimg = cv2.imread(\"gazo.png\").astype(np.float32)\n\n# hilditch thining\nout = hilditch(img)\n\n# Save result\ncv2.imwrite(\"out.png\", out)\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)\ncv2.destroyAllWindows()"}], "task": "code_generation", "index": 263, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_264", "instruction": "pythonを用いて、gazo.pngにZhang-Suenの細線化を行え。\n\nただし、以下の操作は全て0が線、1が背景とするので、*gazo.png*の値を反転させる必要があることに注意。\n\n注目画素x1(x,y)に対して8近傍を次のように定義する。\n\n\nx9 x2 x3\nx8 x1 x4\nx7 x6 x5\n\nこれらに対して二つのステップを考える。\n\nStep.1\nラスタスキャンを行い、以下の5条件を満たすピクセルを全て記録する。\n1. 黒画素である\n2. x2, x3, ..., x9, x2と時計まわりに見て、0から1に変わる回数がちょうど1\n3. x2, x3, ..., x9の中で1の個数が2以上6以下\n4. x2, x4, x6のどれかが1\n5. x4, x6, x8のどれかが1\n記録したピクセルを全て1に変更する。\n\nStep.2\nラスタスキャンを行い、以下の5条件を満たすピクセルを全て記録する。\n1. 黒画素である\n2. x2, x3, ..., x9, x2と時計まわりに見て、0から1に変わる回数がちょうど1\n3. x2, x3, ..., x9の中で1の個数が2以上6以下\n4. x2, x4, x8のどれかが1\n5. x2, x6, x8のどれかが1\n記録したピクセルを全て1に変更する。\n\nStep1, 2で変更する点がなくなるまで交互に繰り返す。", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n# Zhang Suen thining algorythm\ndef Zhang_Suen_thining(img):\n    # get shape\n    H, W, C = img.shape\n\n    # prepare out image\n    out = np.zeros((H, W), dtype=np.int)\n    out[img[..., 0] > 0] = 1\n\n    # inverse\n    out = 1 - out\n\n    while True:\n        s1 = []\n        s2 = []\n\n        # step 1 ( rasta scan )\n        for y in range(1, H-1):\n            for x in range(1, W-1):\n                \n                # condition 1\n                if out[y, x] > 0:\n                    continue\n\n                # condition 2\n                f1 = 0\n                if (out[y-1, x+1] - out[y-1, x]) == 1:\n                    f1 += 1\n                if (out[y, x+1] - out[y-1, x+1]) == 1:\n                    f1 += 1\n                if (out[y+1, x+1] - out[y, x+1]) == 1:\n                    f1 += 1\n                if (out[y+1, x] - out[y+1,x+1]) == 1:\n                    f1 += 1\n                if (out[y+1, x-1] - out[y+1, x]) == 1:\n                    f1 += 1\n                if (out[y, x-1] - out[y+1, x-1]) == 1:\n                    f1 += 1\n                if (out[y-1, x-1] - out[y, x-1]) == 1:\n                    f1 += 1\n                if (out[y-1, x] - out[y-1, x-1]) == 1:\n                    f1 += 1\n\n                if f1 != 1:\n                    continue\n                    \n                # condition 3\n                f2 = np.sum(out[y-1:y+2, x-1:x+2])\n                if f2 < 2 or f2 > 6:\n                    continue\n                \n                # condition 4\n                if out[y-1, x] + out[y, x+1] + out[y+1, x] < 1:\n                    continue\n\n                # condition 5\n                if out[y, x+1] + out[y+1, x] + out[y, x-1] < 1:\n                    continue\n                    \n                s1.append([y, x])\n\n        for v in s1:\n            out[v[0], v[1]] = 1\n\n        # step 2 ( rasta scan )\n        for y in range(1, H-1):\n            for x in range(1, W-1):\n                \n                # condition 1\n                if out[y, x] > 0:\n                    continue\n\n                # condition 2\n                f1 = 0\n                if (out[y-1, x+1] - out[y-1, x]) == 1:\n                    f1 += 1\n                if (out[y, x+1] - out[y-1, x+1]) == 1:\n                    f1 += 1\n                if (out[y+1, x+1] - out[y, x+1]) == 1:\n                    f1 += 1\n                if (out[y+1, x] - out[y+1,x+1]) == 1:\n                    f1 += 1\n                if (out[y+1, x-1] - out[y+1, x]) == 1:\n                    f1 += 1\n                if (out[y, x-1] - out[y+1, x-1]) == 1:\n                    f1 += 1\n                if (out[y-1, x-1] - out[y, x-1]) == 1:\n                    f1 += 1\n                if (out[y-1, x] - out[y-1, x-1]) == 1:\n                    f1 += 1\n\n                if f1 != 1:\n                    continue\n                    \n                # condition 3\n                f2 = np.sum(out[y-1:y+2, x-1:x+2])\n                if f2 < 2 or f2 > 6:\n                    continue\n                \n                # condition 4\n                if out[y-1, x] + out[y, x+1] + out[y, x-1] < 1:\n                    continue\n\n                # condition 5\n                if out[y-1, x] + out[y+1, x] + out[y, x-1] < 1:\n                    continue\n                    \n                s2.append([y, x])\n\n        for v in s2:\n            out[v[0], v[1]] = 1\n\n        # if not any pixel is changed\n        if len(s1) < 1 and len(s2) < 1:\n            break\n\n    out = 1 - out\n    out = out.astype(np.uint8) * 255\n\n    return out\n\n\n# Read image\nimg = cv2.imread(\"gazo.png\").astype(np.float32)\n\n# Zhang Suen thining\nout = Zhang_Suen_thining(img)\n\n\n# Save result\ncv2.imwrite(\"out.png\", out)\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)\ncv2.destroyAllWindows()"}], "task": "code_generation", "index": 264, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_265", "instruction": "pythonを用いて、imoir.jpgのHOG特徴量の勾配強度・勾配角度を求めよ。\n\nHOG(Histogram of Oriented Gradients)とは画像の特徴量表現の一種である。\n\n特徴量とは画像の状態などを表すベクトル集合のことである。\n\n画像認識(画像が何を写した画像か)や検出（画像の中で物体がどこにあるか）では、(1)画像から特徴量を得て(特徴抽出)、(2)特徴量を基に認識や検出を行う(認識・検出)。\n\nディープラーニングでは特徴抽出から認識までを機械学習により自動で行うため、HOGなどは見られなくなっているが、ディープラーニングが流行る前まではHOGは特徴量表現としてよく使われたらしい。\n\nHOGは以下のアルゴリズムで得られる。\n1. 画像をグレースケール化し、x、ｙ方向の輝度勾配を求める\n \nx方向: gx = I(x+1, y) - I(x-1, y)\ny方向: gy = I(x, y+1) - I(x, y-1)\n\n2. gx, gyから勾配強度と勾配角度を求める。\n\n勾配強度: mag = sqrt(gt  2 + gy  2)\n勾配角度: ang = arctan(gy / gx)\n\n3. 勾配角度を [0, 180]で9分割した値に量子化する。つまり、[0,20]には0、[20, 40]には1というインデックスを求める。\n4. 画像をN x Nの領域に分割し(この領域をセルという)、セル内で3で求めたインデックスのヒストグラムを作成する。ただし、当表示は1でなく勾配角度を求める。\n5. C x Cのセルを１つとして(これをブロックという)、ブロック内のセルのヒストグラムを次式で正規化する。これを1セルずつずらしながら行うので、一つのセルが何回も正規化される。", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n# get HOG step1\ndef HOG_step1(img):\n     # Grayscale\n     def BGR2GRAY(img):\n          gray = 0.2126 * img[..., 2] + 0.7152 * img[..., 1] + 0.0722 * img[..., 0]\n          return gray\n\n     # Magnitude and gradient\n     def get_gradXY(gray):\n          H, W = gray.shape\n\n          # padding before grad\n          gray = np.pad(gray, (1, 1), 'edge')\n\n          # get grad x\n          gx = gray[1:H+1, 2:] - gray[1:H+1, :W]\n          # get grad y\n          gy = gray[2:, 1:W+1] - gray[:H, 1:W+1]\n          # replace 0 with \n          gx[gx == 0] = 1e-6\n\n          return gx, gy\n\n     # get magnitude and gradient\n     def get_MagGrad(gx, gy):\n          # get gradient maginitude\n          magnitude = np.sqrt(gx ** 2 + gy ** 2)\n\n          # get gradient angle\n          gradient = np.arctan(gy / gx)\n\n          gradient[gradient < 0] = np.pi / 2 + gradient[gradient < 0] + np.pi / 2\n\n          return magnitude, gradient\n\n     # Gradient histogram\n     def quantization(gradient):\n          # prepare quantization table\n          gradient_quantized = np.zeros_like(gradient, dtype=np.int)\n\n          # quantization base\n          d = np.pi / 9\n\n          # quantization\n          for i in range(9):\n               gradient_quantized[np.where((gradient >= d * i) & (gradient <= d * (i + 1)))] = i\n\n          return gradient_quantized\n\n     # 1. BGR -> Gray\n     gray = BGR2GRAY(img)\n\n     # 1. Gray -> Gradient x and y\n     gx, gy = get_gradXY(gray)\n\n     # 2. get gradient magnitude and angle\n     magnitude, gradient = get_MagGrad(gx, gy)\n\n     # 3. Quantization\n     gradient_quantized = quantization(gradient)\n\n     return magnitude, gradient_quantized\n\n\n# Read image\nimg = cv2.imread(\"imori.jpg\").astype(np.float32)\n\n# get HOG step1\nmagnitude, gradient_quantized = HOG_step1(img)\n\n# Write gradient magnitude to file\n_magnitude = (magnitude / magnitude.max() * 255).astype(np.uint8)\n\ncv2.imwrite(\"out_mag.jpg\", _magnitude)\n\n# Write gradient angle to file\nH, W, C = img.shape\nout = np.zeros((H, W, 3), dtype=np.uint8)\n\n# define color\nC = [[255, 0, 0], [0, 255, 0], [0, 0, 255], [255, 255, 0], [255, 0, 255], [0, 255, 255],\n     [127, 127, 0], [127, 0, 127], [0, 127, 127]]\n\n# draw color\nfor i in range(9):\n     out[gradient_quantized == i] = C[i]\n\n\ncv2.imwrite(\"out_gra.jpg\", out)\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)\ncv2.destroyAllWindows()"}], "task": "code_generation", "index": 265, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_266", "instruction": "HOGは以下のアルゴリズムで得られる。\n\n1. 画像をグレースケール化し、x、ｙ方向の輝度勾配を求める\n \nx方向: gx = I(x+1, y) - I(x-1, y)\ny方向: gy = I(x, y+1) - I(x, y-1)\n\n2. gx, gyから勾配強度と勾配角度を求める。\n\n勾配強度: mag = sqrt(gt  2 + gy  2)\n勾配角度: ang = arctan(gy / gx)\n\n3. 勾配角度を [0, 180]で9分割した値に量子化する。つまり、[0,20]には0、[20, 40]には1というインデックスを求める。\n4. 画像をN x Nの領域に分割し(この領域をセルという)、セル内で3で求めたインデックスのヒストグラムを作成する。ただし、当表示は1でなく勾配角度を求める。\n5. C x Cのセルを１つとして(これをブロックという)、ブロック内のセルのヒストグラムを次式で正規化する。これを1セルずつずらしながら行うので、一つのセルが何回も正規化される。\n\nここではpythonを用いてHOGの4の処理を実装する。\n\nN=8として、8x8の領域を1セルとして、勾配角度のインデックスに勾配強度を投票する形式でヒストグラムを作成せよ。\n\n解答は \n\n1 2 3\n4 5 6\n7 8 9\n\nの順に量子化したインデックスに対応するヒストグラムを示す。", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# get HOG step2\ndef HOG_step2(img):\n    # Grayscale\n    def BGR2GRAY(img):\n        gray = 0.2126 * img[..., 2] + 0.7152 * img[..., 1] + 0.0722 * img[..., 0]\n        return gray\n\n    # Magnitude and gradient\n    def get_gradXY(gray):\n        H, W = gray.shape\n\n        # padding before grad\n        gray = np.pad(gray, (1, 1), 'edge')\n\n        # get grad x\n        gx = gray[1:H+1, 2:] - gray[1:H+1, :W]\n        # get grad y\n        gy = gray[2:, 1:W+1] - gray[:H, 1:W+1]\n        # replace 0 with \n        gx[gx == 0] = 1e-6\n\n        return gx, gy\n\n    # get magnitude and gradient\n    def get_MagGrad(gx, gy):\n        # get gradient maginitude\n        magnitude = np.sqrt(gx ** 2 + gy ** 2)\n\n        # get gradient angle\n        gradient = np.arctan(gy / gx)\n\n        gradient[gradient < 0] = np.pi / 2 + gradient[gradient < 0] + np.pi / 2\n\n        return magnitude, gradient\n\n    # Gradient histogram\n    def quantization(gradient):\n        # prepare quantization table\n        gradient_quantized = np.zeros_like(gradient, dtype=np.int)\n\n        # quantization base\n        d = np.pi / 9\n\n        # quantization\n        for i in range(9):\n            gradient_quantized[np.where((gradient >= d * i) & (gradient <= d * (i + 1)))] = i\n\n        return gradient_quantized\n\n        \n    # get gradient histogram\n    def gradient_histogram(gradient_quantized, magnitude, N=8):\n        # get shape\n        H, W = magnitude.shape\n\n        # get cell num\n        cell_N_H = H // N\n        cell_N_W = W // N\n        histogram = np.zeros((cell_N_H, cell_N_W, 9), dtype=np.float32)\n\n        # each pixel\n        for y in range(cell_N_H):\n            for x in range(cell_N_W):\n                for j in range(N):\n                    for i in range(N):\n                        histogram[y, x, gradient_quantized[y * 4 + j, x * 4 + i]] += magnitude[y * 4 + j, x * 4 + i]\n\n        return histogram\n\n    # 1. BGR -> Gray\n    gray = BGR2GRAY(img)\n\n    # 1. Gray -> Gradient x and y\n    gx, gy = get_gradXY(gray)\n\n    # 2. get gradient magnitude and angle\n    magnitude, gradient = get_MagGrad(gx, gy)\n\n    # 3. Quantization\n    gradient_quantized = quantization(gradient)\n\n    # 4. Gradient histogram\n    histogram = gradient_histogram(gradient_quantized, magnitude)\n\n    return histogram\n\n\n# Read image\nimg = cv2.imread(\"imori.jpg\").astype(np.float32)\n\n# get HOG step2\nhistogram = HOG_step2(img)\n                \n# write histogram to file\nfor i in range(9):\n    plt.subplot(3,3,i+1)\n    plt.imshow(histogram[..., i])\n    plt.axis('off')\n    plt.xticks(color=\"None\")\n    plt.yticks(color=\"None\")\nplt.savefig(\"out.png\")\nplt.show()"}], "task": "code_generation", "index": 266, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_267", "instruction": "HOGは以下のアルゴリズムで得られる。\n1. 画像をグレースケール化し、x、ｙ方向の輝度勾配を求める\n \nx方向: gx = I(x+1, y) - I(x-1, y)\ny方向: gy = I(x, y+1) - I(x, y-1)\n\n2. gx, gyから勾配強度と勾配角度を求める。\n\n勾配強度: mag = sqrt(gt  2 + gy  2)\n勾配角度: ang = arctan(gy / gx)\n\n3. 勾配角度を [0, 180]で9分割した値に量子化する。つまり、[0,20]には0、[20, 40]には1というインデックスを求める。\n4. 画像をN x Nの領域に分割し(この領域をセルという)、セル内で3で求めたインデックスのヒストグラムを作成する。ただし、当表示は1でなく勾配角度を求める。\n5. C x Cのセルを１つとして(これをブロックという)、ブロック内のセルのヒストグラムを次式で正規化する。これを1セルずつずらしながら行うので、一つのセルが何回も正規化される。\n\nここではpythonを用いてHOGの5を実装をしなさい。\n\nC = 3 として、3 x 3のセルを1ブロックとして扱い、ヒストグラムの正規化を行え。\n\nh(t) = h(t) / sqrt(Sum h(t) + epsilon)\n通常は　epsilon=1\n\nこれでHOG特徴量が得られた。", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# get HOG\ndef HOG(img):\n    # Grayscale\n    def BGR2GRAY(img):\n        gray = 0.2126 * img[..., 2] + 0.7152 * img[..., 1] + 0.0722 * img[..., 0]\n        return gray\n\n    # Magnitude and gradient\n    def get_gradXY(gray):\n        H, W = gray.shape\n\n        # padding before grad\n        gray = np.pad(gray, (1, 1), 'edge')\n\n        # get grad x\n        gx = gray[1:H+1, 2:] - gray[1:H+1, :W]\n        # get grad y\n        gy = gray[2:, 1:W+1] - gray[:H, 1:W+1]\n        # replace 0 with \n        gx[gx == 0] = 1e-6\n\n        return gx, gy\n\n    # get magnitude and gradient\n    def get_MagGrad(gx, gy):\n        # get gradient maginitude\n        magnitude = np.sqrt(gx ** 2 + gy ** 2)\n\n        # get gradient angle\n        gradient = np.arctan(gy / gx)\n\n        gradient[gradient < 0] = np.pi / 2 + gradient[gradient < 0] + np.pi / 2\n\n        return magnitude, gradient\n\n    # Gradient histogram\n    def quantization(gradient):\n        # prepare quantization table\n        gradient_quantized = np.zeros_like(gradient, dtype=np.int)\n\n        # quantization base\n        d = np.pi / 9\n\n        # quantization\n        for i in range(9):\n            gradient_quantized[np.where((gradient >= d * i) & (gradient <= d * (i + 1)))] = i\n\n        return gradient_quantized\n\n\n    # get gradient histogram\n    def gradient_histogram(gradient_quantized, magnitude, N=8):\n        # get shape\n        H, W = magnitude.shape\n\n        # get cell num\n        cell_N_H = H // N\n        cell_N_W = W // N\n        histogram = np.zeros((cell_N_H, cell_N_W, 9), dtype=np.float32)\n\n        # each pixel\n        for y in range(cell_N_H):\n            for x in range(cell_N_W):\n                for j in range(N):\n                    for i in range(N):\n                        histogram[y, x, gradient_quantized[y * 4 + j, x * 4 + i]] += magnitude[y * 4 + j, x * 4 + i]\n\n        return histogram\n\n\t\t# histogram normalization\n    def normalization(histogram, C=3, epsilon=1):\n        cell_N_H, cell_N_W, _ = histogram.shape\n        ## each histogram\n        for y in range(cell_N_H):\n    \t    for x in range(cell_N_W):\n       \t    #for i in range(9):\n                histogram[y, x] /= np.sqrt(np.sum(histogram[max(y - 1, 0) : min(y + 2, cell_N_H),\n                                                            max(x - 1, 0) : min(x + 2, cell_N_W)] ** 2) + epsilon)\n\n        return histogram\n\n    # 1. BGR -> Gray\n    gray = BGR2GRAY(img)\n\n    # 1. Gray -> Gradient x and y\n    gx, gy = get_gradXY(gray)\n\n    # 2. get gradient magnitude and angle\n    magnitude, gradient = get_MagGrad(gx, gy)\n\n    # 3. Quantization\n    gradient_quantized = quantization(gradient)\n\n    # 4. Gradient histogram\n    histogram = gradient_histogram(gradient_quantized, magnitude)\n    \n    # 5. Histogram normalization\n    histogram = normalization(histogram)\n\n    return histogram\n\n\n# Read image\nimg = cv2.imread(\"imori.jpg\").astype(np.float32)\n\n# get HOG\nhistogram = HOG(img)\n                \n# Write result to file\nfor i in range(9):\n    plt.subplot(3,3,i+1)\n    plt.imshow(histogram[..., i])\n    plt.axis('off')\n    plt.xticks(color=\"None\")\n    plt.yticks(color=\"None\")\nplt.savefig(\"out.png\")\nplt.show()\n"}], "task": "code_generation", "index": 267, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_268", "instruction": "pythonを用いて、ここではHOG特徴量を描画せよ。\n\n描画はimori.jpgをグレースケール化したものに重ねれば見やすい。\n\n方法としては、セル内のインデックスごとに角度がついて直線を描けばよく、ヒストグラムの値が大きいほど白、値が小さいほど黒で描くと見やすい。", "instances": [{"input": "", "output": "mport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# get HOG\ndef HOG(img):\n    # Grayscale\n    def BGR2GRAY(img):\n        gray = 0.2126 * img[..., 2] + 0.7152 * img[..., 1] + 0.0722 * img[..., 0]\n        return gray\n\n    # Magnitude and gradient\n    def get_gradXY(gray):\n        H, W = gray.shape\n\n        # padding before grad\n        gray = np.pad(gray, (1, 1), 'edge')\n\n        # get grad x\n        gx = gray[1:H+1, 2:] - gray[1:H+1, :W]\n        # get grad y\n        gy = gray[2:, 1:W+1] - gray[:H, 1:W+1]\n        # replace 0 with \n        gx[gx == 0] = 1e-6\n\n        return gx, gy\n\n    # get magnitude and gradient\n    def get_MagGrad(gx, gy):\n        # get gradient maginitude\n        magnitude = np.sqrt(gx ** 2 + gy ** 2)\n\n        # get gradient angle\n        gradient = np.arctan(gy / gx)\n\n        gradient[gradient < 0] = np.pi / 2 + gradient[gradient < 0] + np.pi / 2\n\n        return magnitude, gradient\n\n    # Gradient histogram\n    def quantization(gradient):\n        # prepare quantization table\n        gradient_quantized = np.zeros_like(gradient, dtype=np.int)\n\n        # quantization base\n        d = np.pi / 9\n\n        # quantization\n        for i in range(9):\n            gradient_quantized[np.where((gradient >= d * i) & (gradient <= d * (i + 1)))] = i\n\n        return gradient_quantized\n\n\n    # get gradient histogram\n    def gradient_histogram(gradient_quantized, magnitude, N=8):\n        # get shape\n        H, W = magnitude.shape\n\n        # get cell num\n        cell_N_H = H // N\n        cell_N_W = W // N\n        histogram = np.zeros((cell_N_H, cell_N_W, 9), dtype=np.float32)\n\n        # each pixel\n        for y in range(cell_N_H):\n            for x in range(cell_N_W):\n                for j in range(N):\n                    for i in range(N):\n                        histogram[y, x, gradient_quantized[y * 4 + j, x * 4 + i]] += magnitude[y * 4 + j, x * 4 + i]\n\n        return histogram\n\n\t\t# histogram normalization\n    def normalization(histogram, C=3, epsilon=1):\n        cell_N_H, cell_N_W, _ = histogram.shape\n        ## each histogram\n        for y in range(cell_N_H):\n    \t    for x in range(cell_N_W):\n       \t    #for i in range(9):\n                histogram[y, x] /= np.sqrt(np.sum(histogram[max(y - 1, 0) : min(y + 2, cell_N_H),\n                                                            max(x - 1, 0) : min(x + 2, cell_N_W)] ** 2) + epsilon)\n\n        return histogram\n\n    # 1. BGR -> Gray\n    gray = BGR2GRAY(img)\n\n    # 1. Gray -> Gradient x and y\n    gx, gy = get_gradXY(gray)\n\n    # 2. get gradient magnitude and angle\n    magnitude, gradient = get_MagGrad(gx, gy)\n\n    # 3. Quantization\n    gradient_quantized = quantization(gradient)\n\n    # 4. Gradient histogram\n    histogram = gradient_histogram(gradient_quantized, magnitude)\n    \n    # 5. Histogram normalization\n    histogram = normalization(histogram)\n\n    return histogram\n\n\n# draw HOG\ndef draw_HOG(img, histogram):\n    # Grayscale\n    def BGR2GRAY(img):\n        gray = 0.2126 * img[..., 2] + 0.7152 * img[..., 1] + 0.0722 * img[..., 0]\n        return gray\n\n    def draw(gray, histogram, N=8):\n        # get shape\n        H, W = gray.shape\n        cell_N_H, cell_N_W, _ = histogram.shape\n        \n        ## Draw\n        out = gray[1 : H + 1, 1 : W + 1].copy().astype(np.uint8)\n\n        for y in range(cell_N_H):\n            for x in range(cell_N_W):\n                cx = x * N + N // 2\n                cy = y * N + N // 2\n                x1 = cx + N // 2 - 1\n                y1 = cy\n                x2 = cx - N // 2 + 1\n                y2 = cy\n                \n                h = histogram[y, x] / np.sum(histogram[y, x])\n                h /= h.max()\n        \n                for c in range(9):\n                    #angle = (20 * c + 10 - 90) / 180. * np.pi\n                    # get angle\n                    angle = (20 * c + 10) / 180. * np.pi\n                    rx = int(np.sin(angle) * (x1 - cx) + np.cos(angle) * (y1 - cy) + cx)\n                    ry = int(np.cos(angle) * (x1 - cx) - np.cos(angle) * (y1 - cy) + cy)\n                    lx = int(np.sin(angle) * (x2 - cx) + np.cos(angle) * (y2 - cy) + cx)\n                    ly = int(np.cos(angle) * (x2 - cx) - np.cos(angle) * (y2 - cy) + cy)\n\n                    # color is HOG value\n                    c = int(255. * h[c])\n\n                    # draw line\n                    cv2.line(out, (lx, ly), (rx, ry), (c, c, c), thickness=1)\n\n        return out\n    \n\n    # get gray\n    gray = BGR2GRAY(img)\n\n    # draw HOG\n    out = draw(gray, histogram)\n\n    return out\n\n# Read image\nimg = cv2.imread(\"imori.jpg\").astype(np.float32)\n\n# get HOG\nhistogram = HOG(img)\n\n# draw HOG\nout = draw_HOG(img, histogram)\n\n\n# Save result\ncv2.imwrite(\"out.jpg\", out)\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)\ncv2.destroyAllWindows()"}], "task": "code_generation", "index": 268, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_269", "instruction": "pythonを用いて、imori.jpgに対してHSVを用いて青色の箇所のみが255となる画像を作成せよ。\n\nカラートラッキングとは特定の色の箇所を抽出する手法である。\n\nただし、RGBの状態で色成分を指定するのは256^3のパターンがあり、とても大変である（というか手動ではかなり難しい）ので、HSV変換を用いる。\n\nHSV変換とは Q.5で用いた処理であるが、RGBをH(色相)、S(彩度)、V(明度)に変換する手法である。\n\n- Saturation(彩度) 彩度が小さいほど白、彩度が大きいほど色が濃くなる。 0<=S<=1\n- Value (明度) 明度が小さいほど黒くなり、明度が大きいほど色がきれいになる。 0<=V<=1\n- Hue(色相) 色を0<=H<=360の角度で表し、具体的には次のように表される。\n\n赤 黄色  緑  水色  青  紫   赤\n0  60  120  180 240 300 360\n\nつまり、青色のカラートラッキングを行うにはHSV変換を行い、180<=H<=260となる位置が255となるような二値画像を出力すればよい。", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# BGR -> HSV\ndef BGR2HSV(_img):\n\timg = _img.copy() / 255.\n\n\thsv = np.zeros_like(img, dtype=np.float32)\n\n\t# get max and min\n\tmax_v = np.max(img, axis=2).copy()\n\tmin_v = np.min(img, axis=2).copy()\n\tmin_arg = np.argmin(img, axis=2)\n\n\t# H\n\thsv[..., 0][np.where(max_v == min_v)]= 0\n\t## if min == B\n\tind = np.where(min_arg == 0)\n\thsv[..., 0][ind] = 60 * (img[..., 1][ind] - img[..., 2][ind]) / (max_v[ind] - min_v[ind]) + 60\n\t## if min == R\n\tind = np.where(min_arg == 2)\n\thsv[..., 0][ind] = 60 * (img[..., 0][ind] - img[..., 1][ind]) / (max_v[ind] - min_v[ind]) + 180\n\t## if min == G\n\tind = np.where(min_arg == 1)\n\thsv[..., 0][ind] = 60 * (img[..., 2][ind] - img[..., 0][ind]) / (max_v[ind] - min_v[ind]) + 300\n\t\t\n\t# S\n\thsv[..., 1] = max_v.copy() - min_v.copy()\n\n\t# V\n\thsv[..., 2] = max_v.copy()\n\t\n\treturn hsv\n\n# make mask\ndef get_mask(hsv):\n\tmask = np.zeros_like(hsv[..., 0])\n\t#mask[np.where((hsv > 180) & (hsv[0] < 260))] = 255\n\tmask[np.logical_and((hsv[..., 0] > 180), (hsv[..., 0] < 260))] = 255\n\treturn mask\n\n\n# Read image\nimg = cv2.imread(\"imori.jpg\").astype(np.float32)\n\n# RGB > HSV\nhsv = BGR2HSV(img)\n\n\n# color tracking\nmask = get_mask(hsv)\n\nout = mask.astype(np.uint8)\n\n# Save result\ncv2.imwrite(\"out.png\", out)\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)\ncv2.destroyAllWindows()"}], "task": "code_generation", "index": 269, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_270", "instruction": "pythonを用いて、imori.jpgに対してHSVを用いて青色の箇所のみが黒くなるようにマスキングせよ。\n\nこのように白黒のバイナリ画像を用いて黒部分に対応する元画像の画素を黒に変更する操作をマスキングという。\n\n青色箇所の抽出はHSVで180<=H<=260となる位置が1となるような二値画像を作成し、それの0と1を反転したものと元画像との積をとればよい。\n\nこれによりある程度のイモリの部分の抽出ができる。", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# BGR -> HSV\ndef BGR2HSV(_img):\n\timg = _img.copy() / 255.\n\n\thsv = np.zeros_like(img, dtype=np.float32)\n\n\t# get max and min\n\tmax_v = np.max(img, axis=2).copy()\n\tmin_v = np.min(img, axis=2).copy()\n\tmin_arg = np.argmin(img, axis=2)\n\n\t# H\n\thsv[..., 0][np.where(max_v == min_v)]= 0\n\t## if min == B\n\tind = np.where(min_arg == 0)\n\thsv[..., 0][ind] = 60 * (img[..., 1][ind] - img[..., 2][ind]) / (max_v[ind] - min_v[ind]) + 60\n\t## if min == R\n\tind = np.where(min_arg == 2)\n\thsv[..., 0][ind] = 60 * (img[..., 0][ind] - img[..., 1][ind]) / (max_v[ind] - min_v[ind]) + 180\n\t## if min == G\n\tind = np.where(min_arg == 1)\n\thsv[..., 0][ind] = 60 * (img[..., 2][ind] - img[..., 0][ind]) / (max_v[ind] - min_v[ind]) + 300\n\t\t\n\t# S\n\thsv[..., 1] = max_v.copy() - min_v.copy()\n\n\t# V\n\thsv[..., 2] = max_v.copy()\n\t\n\treturn hsv\n\n# make mask\ndef get_mask(hsv):\n\tmask = np.zeros_like(hsv[..., 0])\n\t#mask[np.where((hsv > 180) & (hsv[0] < 260))] = 255\n\tmask[np.logical_and((hsv[..., 0] > 180), (hsv[..., 0] < 260))] = 1\n\treturn mask\n\n# masking\ndef masking(img, mask):\n\tmask = 1 - mask\n\tout = img.copy()\n\t# mask [h, w] -> [h, w, channel]\n\tmask = np.tile(mask, [3, 1, 1]).transpose([1, 2, 0])\n\tout *= mask\n\n\treturn out\n\n\n# Read image\nimg = cv2.imread(\"imori.jpg\").astype(np.float32)\n\n# RGB > HSV\nhsv = BGR2HSV(img / 255.)\n\n# color tracking\nmask = get_mask(hsv)\n\n# masking\nout = masking(img, mask)\n\nout = out.astype(np.uint8)\n\n# Save result\ncv2.imwrite(\"out.jpg\", out)\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)\ncv2.destroyAllWindows()"}], "task": "code_generation", "index": 270, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_271", "instruction": "マスク処理が雑になってしまっていたので、画像の細かい部分が削除されていたり、背景がところどころ残ってしまった。\n\npythonを用いて、マスク画像にN=5のクロージング処理とオープニング処理を施してマスク画像を正確にして、マスキングを行え。", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# BGR -> HSV\ndef BGR2HSV(_img):\n\timg = _img.copy() / 255.\n\n\thsv = np.zeros_like(img, dtype=np.float32)\n\n\t# get max and min\n\tmax_v = np.max(img, axis=2).copy()\n\tmin_v = np.min(img, axis=2).copy()\n\tmin_arg = np.argmin(img, axis=2)\n\n\t# H\n\thsv[..., 0][np.where(max_v == min_v)]= 0\n\t## if min == B\n\tind = np.where(min_arg == 0)\n\thsv[..., 0][ind] = 60 * (img[..., 1][ind] - img[..., 2][ind]) / (max_v[ind] - min_v[ind]) + 60\n\t## if min == R\n\tind = np.where(min_arg == 2)\n\thsv[..., 0][ind] = 60 * (img[..., 0][ind] - img[..., 1][ind]) / (max_v[ind] - min_v[ind]) + 180\n\t## if min == G\n\tind = np.where(min_arg == 1)\n\thsv[..., 0][ind] = 60 * (img[..., 2][ind] - img[..., 0][ind]) / (max_v[ind] - min_v[ind]) + 300\n\t\t\n\t# S\n\thsv[..., 1] = max_v.copy() - min_v.copy()\n\n\t# V\n\thsv[..., 2] = max_v.copy()\n\t\n\treturn hsv\n\n# make mask\ndef get_mask(hsv):\n\tmask = np.zeros_like(hsv[..., 0])\n\t#mask[np.where((hsv > 180) & (hsv[0] < 260))] = 255\n\tmask[np.logical_and((hsv[..., 0] > 180), (hsv[..., 0] < 260))] = 1\n\treturn mask\n\n# masking\ndef masking(img, mask):\n\tmask = 1 - mask\n\tout = img.copy()\n\t# mask [h, w] -> [h, w, channel]\n\tmask = np.tile(mask, [3, 1, 1]).transpose([1, 2, 0])\n\tout *= mask\n\n\treturn out\n\n\n# Erosion\ndef Erode(img, Erode_time=1):\n\tH, W = img.shape\n\tout = img.copy()\n\n\t# kernel\n\tMF = np.array(((0, 1, 0),\n\t\t\t\t(1, 0, 1),\n\t\t\t\t(0, 1, 0)), dtype=np.int)\n\n\t# each erode\n\tfor i in range(Erode_time):\n\t\ttmp = np.pad(out, (1, 1), 'edge')\n\t\t# erode\n\t\tfor y in range(1, H + 1):\n\t\t\tfor x in range(1, W + 1):\n\t\t\t\tif np.sum(MF * tmp[y - 1 : y + 2 , x - 1 : x + 2]) < 1 * 4:\n\t\t\t\t\tout[y - 1, x - 1] = 0\n\n\treturn out\n\n\n# Dilation\ndef Dilate(img, Dil_time=1):\n\tH, W = img.shape\n\n\t# kernel\n\tMF = np.array(((0, 1, 0),\n\t\t\t\t(1, 0, 1),\n\t\t\t\t(0, 1, 0)), dtype=np.int)\n\n\t# each dilate time\n\tout = img.copy()\n\tfor i in range(Dil_time):\n\t\ttmp = np.pad(out, (1, 1), 'edge')\n\t\tfor y in range(1, H + 1):\n\t\t\tfor x in range(1, W + 1):\n\t\t\t\tif np.sum(MF * tmp[y - 1 : y + 2, x - 1 : x + 2]) >= 1:\n\t\t\t\t\tout[y - 1, x - 1] = 1\n\n\treturn out\n\n\n# Opening morphology\ndef Morphology_Opening(img, time=1):\n    out = Erode(img, Erode_time=time)\n    out = Dilate(out, Dil_time=time)\n    return out\n\n# Closing morphology\ndef Morphology_Closing(img, time=1):\n\tout = Dilate(img, Dil_time=time)\n\tout = Erode(out, Erode_time=time)\n\treturn out\n\n\n# Read image\nimg = cv2.imread(\"imori.jpg\").astype(np.float32)\n\n# RGB > HSV\nhsv = BGR2HSV(img / 255.)\n\n# color tracking\nmask = get_mask(hsv)\n\n# closing\nmask = Morphology_Closing(mask, time=5)\n\n# opening\nmask = Morphology_Opening(mask, time=5)\n\n# masking\nout = masking(img, mask)\n\nout = out.astype(np.uint8)\n\n# Save result\ncv2.imwrite(\"out.jpg\", out)\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)\ncv2.destroyAllWindows()"}], "task": "code_generation", "index": 271, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_272", "instruction": "pythonを用いて、imori.jpgをグレースケールにしたものを0.5倍に縮小した後に2倍に拡大した画像を求めよ。この処理を行うと、ぼやけた画像ができる。\n\n拡大縮小にはbi-linear補間を用いよ。bi-linear補間をメソッド（関数）化すると、プログラムが簡潔にできる。", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Grayscale\ndef BGR2GRAY(img):\n\t# Grayscale\n\tgray = 0.2126 * img[..., 2] + 0.7152 * img[..., 1] + 0.0722 * img[..., 0]\n\treturn gray\n\n# Bi-Linear interpolation\ndef bl_interpolate(img, ax=1., ay=1.):\n\tif len(img.shape) > 2:\n\t\tH, W, C = img.shape\n\telse:\n\t\tH, W = img.shape\n\t\tC = 1\n\n\taH = int(ay * H)\n\taW = int(ax * W)\n\n\t# get position of resized image\n\ty = np.arange(aH).repeat(aW).reshape(aW, -1)\n\tx = np.tile(np.arange(aW), (aH, 1))\n\n\t# get position of original position\n\ty = (y / ay)\n\tx = (x / ax)\n\n\tix = np.floor(x).astype(np.int)\n\tiy = np.floor(y).astype(np.int)\n\n\tix = np.minimum(ix, W-2)\n\tiy = np.minimum(iy, H-2)\n\n\t# get distance \n\tdx = x - ix\n\tdy = y - iy\n\n\tif C > 1:\n\t\tdx = np.repeat(np.expand_dims(dx, axis=-1), C, axis=-1)\n\t\tdy = np.repeat(np.expand_dims(dy, axis=-1), C, axis=-1)\n\n\t# interpolation\n\tout = (1-dx) * (1-dy) * img[iy, ix] + dx * (1 - dy) * img[iy, ix+1] + (1 - dx) * dy * img[iy+1, ix] + dx * dy * img[iy+1, ix+1]\n\n\tout = np.clip(out, 0, 255)\n\tout = out.astype(np.uint8)\n\n\treturn out\n\n\n# Read image\nimg = cv2.imread(\"imori.jpg\").astype(np.float)\n\ngray = BGR2GRAY(img)\n\n# Bilinear interpolation\nout = bl_interpolate(gray.astype(np.float32), ax=0.5, ay=0.5)\n\n# Bilinear interpolation\nout = bl_interpolate(out, ax=2., ay=2.)\n\nout = out.astype(np.uint8)\n\n# Save result\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)\ncv2.imwrite(\"out.jpg\", out)"}], "task": "code_generation", "index": 272, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_273", "instruction": "imori.jpgをグレースケールにしたものを0.5倍に縮小した後に2倍に拡大した画像を求めよ。\nこの処理を行うと、ぼやけた画像ができる。拡大縮小にはbi-linear補間を用いよ。\n\npythonを用いて、上記の処理で得た画像と元画像の差分を求め、[0,255]に正規化せよ。\n\nここで求めた画像はエッジとなっている。つまり、画像中の高周波成分をとったことになる。", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Grayscale\ndef BGR2GRAY(img):\n\t# Grayscale\n\tgray = 0.2126 * img[..., 2] + 0.7152 * img[..., 1] + 0.0722 * img[..., 0]\n\treturn gray\n\n# Bi-Linear interpolation\ndef bl_interpolate(img, ax=1., ay=1.):\n\tif len(img.shape) > 2:\n\t\tH, W, C = img.shape\n\telse:\n\t\tH, W = img.shape\n\t\tC = 1\n\n\taH = int(ay * H)\n\taW = int(ax * W)\n\n\t# get position of resized image\n\ty = np.arange(aH).repeat(aW).reshape(aW, -1)\n\tx = np.tile(np.arange(aW), (aH, 1))\n\n\t# get position of original position\n\ty = (y / ay)\n\tx = (x / ax)\n\n\tix = np.floor(x).astype(np.int)\n\tiy = np.floor(y).astype(np.int)\n\n\tix = np.minimum(ix, W-2)\n\tiy = np.minimum(iy, H-2)\n\n\t# get distance \n\tdx = x - ix\n\tdy = y - iy\n\n\tif C > 1:\n\t\tdx = np.repeat(np.expand_dims(dx, axis=-1), C, axis=-1)\n\t\tdy = np.repeat(np.expand_dims(dy, axis=-1), C, axis=-1)\n\n\t# interpolation\n\tout = (1-dx) * (1-dy) * img[iy, ix] + dx * (1 - dy) * img[iy, ix+1] + (1 - dx) * dy * img[iy+1, ix] + dx * dy * img[iy+1, ix+1]\n\n\tout = np.clip(out, 0, 255)\n\tout = out.astype(np.uint8)\n\n\treturn out\n\n\n# Read image\nimg = cv2.imread(\"imori.jpg\").astype(np.float)\n\ngray = BGR2GRAY(img)\n\n# Bilinear interpolation\nout = bl_interpolate(gray.astype(np.float32), ax=0.5, ay=0.5)\n\n# Bilinear interpolation\nout = bl_interpolate(out, ax=2., ay=2.)\n\nout = np.abs(out - gray)\n\nout = out / out.max() * 255\n\nout = out.astype(np.uint8)\n\n# Save result\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)\ncv2.imwrite(\"out.jpg\", out)"}], "task": "code_generation", "index": 273, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_274", "instruction": "pythonを用いて、imori.pngを1/2, 1/4, 1/8, 1/16, 1/32にリサイズした画像を求めよ。\n\nこのように元画像を小さくリサイズして重ねたものをガウシアンピラミッドと呼ぶ。\n\nこのガウシアンピラミッドの概念は現在でも有効であり、画像をきれいにする超解像を行うディープラーニングの手法でもガウシアンピラミッドの概念が用いられる。", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Grayscale\ndef BGR2GRAY(img):\n\t# Grayscale\n\tgray = 0.2126 * img[..., 2] + 0.7152 * img[..., 1] + 0.0722 * img[..., 0]\n\treturn gray\n\n# Bi-Linear interpolation\ndef bl_interpolate(img, ax=1., ay=1.):\n\tif len(img.shape) > 2:\n\t\tH, W, C = img.shape\n\telse:\n\t\tH, W = img.shape\n\t\tC = 1\n\n\taH = int(ay * H)\n\taW = int(ax * W)\n\n\t# get position of resized image\n\ty = np.arange(aH).repeat(aW).reshape(aW, -1)\n\tx = np.tile(np.arange(aW), (aH, 1))\n\n\t# get position of original position\n\ty = (y / ay)\n\tx = (x / ax)\n\n\tix = np.floor(x).astype(np.int)\n\tiy = np.floor(y).astype(np.int)\n\n\tix = np.minimum(ix, W-2)\n\tiy = np.minimum(iy, H-2)\n\n\t# get distance \n\tdx = x - ix\n\tdy = y - iy\n\n\tif C > 1:\n\t\tdx = np.repeat(np.expand_dims(dx, axis=-1), C, axis=-1)\n\t\tdy = np.repeat(np.expand_dims(dy, axis=-1), C, axis=-1)\n\n\t# interpolation\n\tout = (1-dx) * (1-dy) * img[iy, ix] + dx * (1 - dy) * img[iy, ix+1] + (1 - dx) * dy * img[iy+1, ix] + dx * dy * img[iy+1, ix+1]\n\n\tout = np.clip(out, 0, 255)\n\tout = out.astype(np.uint8)\n\n\treturn out\n\n# make image pyramid\ndef make_pyramid(gray):\n\t# first element\n\tpyramid = [gray]\n\t# each scale\n\tfor i in range(1, 6):\n\t\t# define scale\n\t\ta = 2. ** i\n\n\t\t# down scale\n\t\tp = bl_interpolate(gray, ax=1./a, ay=1. / a)\n\n\t\t# add pyramid list\n\t\tpyramid.append(p)\n\t\t\n\treturn pyramid\n\n# Read image\nimg = cv2.imread(\"imori.jpg\").astype(np.float)\n\ngray = BGR2GRAY(img)\n\n# pyramid\npyramid = make_pyramid(gray)\n\nfor i in range(6):\n    cv2.imwrite(\"out_{}.jpg\".format(2**i), pyramid[i].astype(np.uint8))\n    plt.subplot(1, 6, i+1)\n    plt.imshow(pyramid[i], cmap='gray')\n    plt.axis('off')\n    plt.xticks(color=\"None\")\n    plt.yticks(color=\"None\")\n\nplt.show()"}], "task": "code_generation", "index": 274, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_275", "instruction": "pythonを用いて、ガウシアンピラミッドを用いた簡単な顕著性マップを作成しなさい。\n\n顕著性マップとは画像の中で人間の目を引きやすい領域を表した画像である。\n\n現在ではディープラーニングによる顕著性マップがよく用いられるが、本来は画像のRGB成分やHSV成分などのガウシアンピラミッドを作成し、それらの差分から求める手法がよく用いられた(例えばIttiらの手法などがある)。\n\nガウシアンピラミッドから簡単な顕著性マップを作成する。\nアルゴリズムは、\n1. ガウシアンピラミッドをそれぞれ、128, 64, 32, ...というサイズになっているが、はじめにそれらを128にリサイズせよ。リサイズはbi-linear補間を用いよ。\n2. 作成したピラミッド（それぞれ0, 1, 2, 3, 4, 5と番号をふる）の２つを選び差分を求める。\n3. 2で求めた差分を全て足し合わせ、[0, 255]に正規化せよ。\n\n以上で顕著性マップが求められる。\n2で選ぶ２つの画像は特に指定はしないが、いいものを選べば解答例のように顕著性マップが作成できる。\n\n画像の細かい部分や色が周辺と極端に違う部分など、人の目に止まりやすい領域が白くなっているのが分かる。\n\n解答例( (0,1), (0,3), (0,5), (1,4), (2,3), (3,5) を使用)", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Grayscale\ndef BGR2GRAY(img):\n\t# Grayscale\n\tgray = 0.2126 * img[..., 2] + 0.7152 * img[..., 1] + 0.0722 * img[..., 0]\n\treturn gray\n\n# Bi-Linear interpolation\ndef bl_interpolate(img, ax=1., ay=1.):\n\tif len(img.shape) > 2:\n\t\tH, W, C = img.shape\n\telse:\n\t\tH, W = img.shape\n\t\tC = 1\n\n\taH = int(ay * H)\n\taW = int(ax * W)\n\n\t# get position of resized image\n\ty = np.arange(aH).repeat(aW).reshape(aW, -1)\n\tx = np.tile(np.arange(aW), (aH, 1))\n\n\t# get position of original position\n\ty = (y / ay)\n\tx = (x / ax)\n\n\tix = np.floor(x).astype(np.int)\n\tiy = np.floor(y).astype(np.int)\n\n\tix = np.minimum(ix, W-2)\n\tiy = np.minimum(iy, H-2)\n\n\t# get distance \n\tdx = x - ix\n\tdy = y - iy\n\n\tif C > 1:\n\t\tdx = np.repeat(np.expand_dims(dx, axis=-1), C, axis=-1)\n\t\tdy = np.repeat(np.expand_dims(dy, axis=-1), C, axis=-1)\n\n\t# interpolation\n\tout = (1-dx) * (1-dy) * img[iy, ix] + dx * (1 - dy) * img[iy, ix+1] + (1 - dx) * dy * img[iy+1, ix] + dx * dy * img[iy+1, ix+1]\n\n\tout = np.clip(out, 0, 255)\n\tout = out.astype(np.uint8)\n\n\treturn out\n\n# make image pyramid\ndef make_pyramid(gray):\n\t# first element\n\tpyramid = [gray]\n\t# each scale\n\tfor i in range(1, 6):\n\t\t# define scale\n\t\ta = 2. ** i\n\n\t\t# down scale\n\t\tp = bl_interpolate(gray, ax=1./a, ay=1. / a)\n\n\t\t# up scale\n\t\tp = bl_interpolate(p, ax=a, ay=a)\n\n\t\t# add pyramid list\n\t\tpyramid.append(p.astype(np.float32))\n\n\treturn pyramid\n\n# make saliency map\ndef saliency_map(pyramid):\n\t# get shape\n\tH, W = pyramid[0].shape\n\n\t# prepare out image\n\tout = np.zeros((H, W), dtype=np.float32)\n\n\t# add each difference\n\tout += np.abs(pyramid[0] - pyramid[1])\n\tout += np.abs(pyramid[0] - pyramid[3])\n\tout += np.abs(pyramid[0] - pyramid[5])\n\tout += np.abs(pyramid[1] - pyramid[4])\n\tout += np.abs(pyramid[2] - pyramid[3])\n\tout += np.abs(pyramid[3] - pyramid[5])\n\n\t# normalization\n\tout = out / out.max() * 255\n\n\treturn out\n\n\n# Read image\nimg = cv2.imread(\"imori.jpg\").astype(np.float)\n\n# grayscale\ngray = BGR2GRAY(img)\n\n# pyramid\npyramid = make_pyramid(gray)\n    \n# pyramid -> saliency\nout = saliency_map(pyramid)\n\nout = out.astype(np.uint8)\n\n# Save result\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)\ncv2.imwrite(\"out.jpg\", out)"}], "task": "code_generation", "index": 275, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_276", "instruction": "pythonを用いて、ガボールフィルタを実装せよ。\n\nガボールフィルタとはガウス分布と周波数変換を合わせたフィルタであり、画像の特定方向のみのエッジを抽出する時に使われる。\n\nフィルタは次式で定義される。\n\nG(y, x) = exp(-(x'^2 + g^2 y'^2) / 2 s^2) * cos(2 pi x' / l + p)\nx' = cosA * x + sinA * y\ny' = -sinA * x + cosA * y\n\ny, x はフィルタの位置　フィルタサイズがKとすると、 y, x は [-K//2, k//2]　の値を取る。\ng ... gamma ガボールフィルタの楕円率\ns ... sigma ガウス分布の標準偏差\nl ... lambda 周波数の波長\np ... 位相オフセット\nA ... フィルタの回転　抽出したい角度を指定する。\n\nここでは、K=111, s=10, g = 1.2, l =10, p=0, A=0としてガボールフィルタを可視化せよ。\n\nガボールフィルタを実際に使う時は、フィルタ値の絶対値の和が1になるように正規化すると使いやすくなる。\n\n答えでは可視化のためにフィルタの値を[0,255]に正規化している。", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Gabor\ndef Gabor_filter(K_size=111, Sigma=10, Gamma=1.2, Lambda=10, Psi=0, angle=0):\n\t# get half size\n\td = K_size // 2\n\n\t# prepare kernel\n\tgabor = np.zeros((K_size, K_size), dtype=np.float32)\n\n\t# each value\n\tfor y in range(K_size):\n\t\tfor x in range(K_size):\n\t\t\t# distance from center\n\t\t\tpx = x - d\n\t\t\tpy = y - d\n\n\t\t\t# degree -> radian\n\t\t\ttheta = angle / 180. * np.pi\n\n\t\t\t# get kernel x\n\t\t\t_x = np.cos(theta) * px + np.sin(theta) * py\n\n\t\t\t# get kernel y\n\t\t\t_y = -np.sin(theta) * px + np.cos(theta) * py\n\n\t\t\t# fill kernel\n\t\t\tgabor[y, x] = np.exp(-(_x**2 + Gamma**2 * _y**2) / (2 * Sigma**2)) * np.cos(2*np.pi*_x/Lambda + Psi)\n\n\t# kernel normalization\n\tgabor /= np.sum(np.abs(gabor))\n\n\treturn gabor\n\n\n# get gabor kernel\ngabor = Gabor_filter(K_size=111, Sigma=10, Gamma=1.2, Lambda=10, Psi=0, angle=0)\n\n# Visualize\n# normalize to [0, 255]\nout = gabor - np.min(gabor)\nout /= np.max(out)\nout *= 255\n\nout = out.astype(np.uint8)\ncv2.imwrite(\"out.jpg\", out)\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)"}], "task": "code_generation", "index": 276, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_277", "instruction": "pythonを用いて、A=0, 45, 90, 135として回転方向のガボールフィルタを求めよ。\nその他のパラメータは、K=111, s=10, g = 1.2, l =10, p=0とせよ。\n\nガボールフィルタとはガウス分布と周波数変換を合わせたフィルタであり、画像の特定方向のみのエッジを抽出する時に使われる。\n\nフィルタは次式で定義される。\n\n\nG(y, x) = exp(-(x'^2 + g^2 y'^2) / 2 s^2) * cos(2 pi x' / l + p)\nx' = cosA * x + sinA * y\ny' = -sinA * x + cosA * y\n\ny, x はフィルタの位置　フィルタサイズがKとすると、 y, x は [-K//2, k//2]　の値を取る。\ng ... gamma ガボールフィルタの楕円率\ns ... sigma ガウス分布の標準偏差\nl ... lambda 周波数の波長\np ... 位相オフセット\nA ... フィルタの回転　抽出したい角度を指定する。\n\n\nここではガボールフィルタをメソッド化すれば簡単に実装できる。", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n# Gabor\ndef Gabor_filter(K_size=111, Sigma=10, Gamma=1.2, Lambda=10, Psi=0, angle=0):\n\t# get half size\n\td = K_size // 2\n\n\t# prepare kernel\n\tgabor = np.zeros((K_size, K_size), dtype=np.float32)\n\n\t# each value\n\tfor y in range(K_size):\n\t\tfor x in range(K_size):\n\t\t\t# distance from center\n\t\t\tpx = x - d\n\t\t\tpy = y - d\n\n\t\t\t# degree -> radian\n\t\t\ttheta = angle / 180. * np.pi\n\n\t\t\t# get kernel x\n\t\t\t_x = np.cos(theta) * px + np.sin(theta) * py\n\n\t\t\t# get kernel y\n\t\t\t_y = -np.sin(theta) * px + np.cos(theta) * py\n\n\t\t\t# fill kernel\n\t\t\tgabor[y, x] = np.exp(-(_x**2 + Gamma**2 * _y**2) / (2 * Sigma**2)) * np.cos(2*np.pi*_x/Lambda + Psi)\n\n\t# kernel normalization\n\tgabor /= np.sum(np.abs(gabor))\n\n\treturn gabor\n\n\n# define each angle\nAs = [0, 45, 90, 135]\n\n# prepare pyplot\nplt.subplots_adjust(left=0, right=1, top=1, bottom=0, hspace=0, wspace=0.2)\n\n# each angle\nfor i, A in enumerate(As):\n    # get gabor kernel\n    gabor = Gabor_filter(K_size=111, Sigma=10, Gamma=1.2, Lambda=10, Psi=0, angle=A)\n\n    # normalize to [0, 255]\n    out = gabor - np.min(gabor)\n    out /= np.max(out)\n    out *= 255\n    \n    out = out.astype(np.uint8)\n    plt.subplot(1, 4, i+1)\n    plt.imshow(out, cmap='gray')\n    plt.axis('off')\n    plt.title(\"Angle \"+str(A))\n\nplt.savefig(\"out.png\")\nplt.show()"}], "task": "code_generation", "index": 277, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_278", "instruction": "pythonを用いて、imori.jpgをグレースケール化し、A=0, 45, 90, 135 のガボールフィルタでフィルタリングせよ。\n\nパラメータはK=11, s=1.5, g=1.2, l=3, p=0とする。\n\nガボールフィルタでは指定した方向のエッジを抽出することができ、ガボールフィルタはエッジの特徴抽出に優れている。\n\nガボールフィルタは生物の視神経における脳内の一次視覚野(V1)での働きに近いとされていて、つまり生物が見ている時の眼の前の画像の特徴抽出を再現しているともいわれる。\n\nディープラーニングのConvolutional層はガボールフィルタの働きに近いとも考えられている。しかし、ディープラーニングではフィルタの係数が機械学習によって自動的に決定される。機械学習の結果、ガボールフィルタに近い働きが生じると言われる。", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Grayscale\ndef BGR2GRAY(img):\n\t# Grayscale\n\tgray = 0.2126 * img[..., 2] + 0.7152 * img[..., 1] + 0.0722 * img[..., 0]\n\treturn gray\n\n# Gabor\ndef Gabor_filter(K_size=111, Sigma=10, Gamma=1.2, Lambda=10, Psi=0, angle=0):\n\t# get half size\n\td = K_size // 2\n\n\t# prepare kernel\n\tgabor = np.zeros((K_size, K_size), dtype=np.float32)\n\n\t# each value\n\tfor y in range(K_size):\n\t\tfor x in range(K_size):\n\t\t\t# distance from center\n\t\t\tpx = x - d\n\t\t\tpy = y - d\n\n\t\t\t# degree -> radian\n\t\t\ttheta = angle / 180. * np.pi\n\n\t\t\t# get kernel x\n\t\t\t_x = np.cos(theta) * px + np.sin(theta) * py\n\n\t\t\t# get kernel y\n\t\t\t_y = -np.sin(theta) * px + np.cos(theta) * py\n\n\t\t\t# fill kernel\n\t\t\tgabor[y, x] = np.exp(-(_x**2 + Gamma**2 * _y**2) / (2 * Sigma**2)) * np.cos(2*np.pi*_x/Lambda + Psi)\n\n\t# kernel normalization\n\tgabor /= np.sum(np.abs(gabor))\n\n\treturn gabor\n\n\ndef Gabor_filtering(gray, K_size=111, Sigma=10, Gamma=1.2, Lambda=10, Psi=0, angle=0):\n    # get shape\n    H, W = gray.shape\n\n    # padding\n    gray = np.pad(gray, (K_size//2, K_size//2), 'edge')\n\n    # prepare out image\n    out = np.zeros((H, W), dtype=np.float32)\n\n    # get gabor filter\n    gabor = Gabor_filter(K_size=K_size, Sigma=Sigma, Gamma=Gamma, Lambda=Lambda, Psi=0, angle=angle)\n        \n    # filtering\n    for y in range(H):\n        for x in range(W):\n            out[y, x] = np.sum(gray[y : y + K_size, x : x + K_size] * gabor)\n\n    out = np.clip(out, 0, 255)\n    out = out.astype(np.uint8)\n\n    return out\n\n\ndef Gabor_process(img):\n    # gray scale\n    gray = BGR2GRAY(img).astype(np.float32)\n\n    # define angle\n    As = [0, 45, 90, 135]\n\n    # prepare pyplot\n    plt.subplots_adjust(left=0, right=1, top=1, bottom=0, hspace=0, wspace=0.2)\n\n    # each angle\n    for i, A in enumerate(As):\n        # gabor filtering\n        out = Gabor_filtering(gray, K_size=11, Sigma=1.5, Gamma=1.2, Lambda=3, angle=A)\n\n        plt.subplot(1, 4, i+1)\n        plt.imshow(out, cmap='gray')\n        plt.axis('off')\n        plt.title(\"Angle \"+str(A))\n\n    plt.savefig(\"out.png\")\n    plt.show()\n\n# Read image\nimg = cv2.imread(\"imori.jpg\").astype(np.float32)\n\n# gabor process\nGabor_process(img)"}], "task": "code_generation", "index": 278, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_279", "instruction": "pythonを用いて、imori.jpgをグレースケール化し、A=0, 45, 90, 135 のガボールフィルタでフィルタリングした４枚の画像を足し合わせることで、画像の特徴を抽出せよ。\n\n結果を見ると、画像の輪郭部分が白くなっていることからエッジ検出のような出力を得たように見える。\n\nディープラーニングのCNN(Convolutional Neural Network)では、最初に画像の特徴を抽出する働きが備わっているが、その特徴抽出の計算はこの問で行ったような操作を延々と繰り返している。ディープラーニングではこのようにして画像の特徴を自動的に抽出している。", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Grayscale\ndef BGR2GRAY(img):\n\t# Grayscale\n\tgray = 0.2126 * img[..., 2] + 0.7152 * img[..., 1] + 0.0722 * img[..., 0]\n\treturn gray\n\n# Gabor\ndef Gabor_filter(K_size=111, Sigma=10, Gamma=1.2, Lambda=10, Psi=0, angle=0):\n\t# get half size\n\td = K_size // 2\n\n\t# prepare kernel\n\tgabor = np.zeros((K_size, K_size), dtype=np.float32)\n\n\t# each value\n\tfor y in range(K_size):\n\t\tfor x in range(K_size):\n\t\t\t# distance from center\n\t\t\tpx = x - d\n\t\t\tpy = y - d\n\n\t\t\t# degree -> radian\n\t\t\ttheta = angle / 180. * np.pi\n\n\t\t\t# get kernel x\n\t\t\t_x = np.cos(theta) * px + np.sin(theta) * py\n\n\t\t\t# get kernel y\n\t\t\t_y = -np.sin(theta) * px + np.cos(theta) * py\n\n\t\t\t# fill kernel\n\t\t\tgabor[y, x] = np.exp(-(_x**2 + Gamma**2 * _y**2) / (2 * Sigma**2)) * np.cos(2*np.pi*_x/Lambda + Psi)\n\n\t# kernel normalization\n\tgabor /= np.sum(np.abs(gabor))\n\n\treturn gabor\n\n\ndef Gabor_filtering(gray, K_size=111, Sigma=10, Gamma=1.2, Lambda=10, Psi=0, angle=0):\n    # get shape\n    H, W = gray.shape\n\n    # padding\n    gray = np.pad(gray, (K_size//2, K_size//2), 'edge')\n\n    # prepare out image\n    out = np.zeros((H, W), dtype=np.float32)\n\n    # get gabor filter\n    gabor = Gabor_filter(K_size=K_size, Sigma=Sigma, Gamma=Gamma, Lambda=Lambda, Psi=0, angle=angle)\n        \n    # filtering\n    for y in range(H):\n        for x in range(W):\n            out[y, x] = np.sum(gray[y : y + K_size, x : x + K_size] * gabor)\n\n    out = np.clip(out, 0, 255)\n    out = out.astype(np.uint8)\n\n    return out\n\n\ndef Gabor_process(img):\n    # get shape\n    H, W, _ = img.shape\n\n    # gray scale\n    gray = BGR2GRAY(img).astype(np.float32)\n\n    # define angle\n    As = [0, 45, 90, 135]\n\n    # prepare pyplot\n    plt.subplots_adjust(left=0, right=1, top=1, bottom=0, hspace=0, wspace=0.2)\n\n    out = np.zeros([H, W], dtype=np.float32)\n\n    # each angle\n    for i, A in enumerate(As):\n        # gabor filtering\n        _out = Gabor_filtering(gray, K_size=11, Sigma=1.5, Gamma=1.2, Lambda=3, angle=A)\n\n        # add gabor filtered image\n        out += _out\n\n    # scale normalization\n    out = out / out.max() * 255\n    out = out.astype(np.uint8)\n\n    return out\n\n\n# Read image\nimg = cv2.imread(\"imori.jpg\").astype(np.float32)\n\n# gabor process\nout = Gabor_process(img)\n\n\ncv2.imwrite(\"out.jpg\", out)\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)"}], "task": "code_generation", "index": 279, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_280", "instruction": "pythonを用いて、thorino.jpgにHessian(ヘシアン)のコーナー検出を行え。\n\nコーナー検出とはエッジにおける角の点を検出することである。\n\nコーナーは曲率が大きくなる点であり、次式のガウス曲率において、\n\nガウス曲率 K = det(H) / (1 + Ix^2 + Iy^2)^2\n\ndet(H) = Ixx Iyy - IxIy^2\nH ... ヘシアン行列。画像の二次微分(グレースケール画像などに対して、Sobelフィルタを掛けて求められる)。画像上の一点に対して、次式で定義される。\nIx ... x方向のsobelフィルタを掛けたもの。 \nIy ... y方向のsobelフィルタを掛けたもの。\nH = [ Ix^2  IxIy]\n      IxIy  Iy^2\n\nヘシアンのコーナー検出では、det(H)が極大点をコーナーとみなす。\n極大点は注目画素と8近傍を比較して、注目画素の値が最大であれば極大点として扱う。\n\n解答ではdet(H)が極大点かつ、max(det(H))*0.1を超過する点をコーナーとしている。", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Hessian corner detection\ndef Hessian_corner(img):\n\n\t## Grayscale\n\tdef BGR2GRAY(img):\n\t\tgray = 0.2126 * img[..., 2] + 0.7152 * img[..., 1] + 0.0722 * img[..., 0]\n\t\tgray = gray.astype(np.uint8)\n\t\treturn gray\n\n\t## Sobel\n\tdef Sobel_filtering(gray):\n\t\t# get shape\n\t\tH, W = gray.shape\n\n\t\t# sobel kernel\n\t\tsobely = np.array(((1, 2, 1),\n\t\t\t\t\t\t(0, 0, 0),\n\t\t\t\t\t\t(-1, -2, -1)), dtype=np.float32)\n\n\t\tsobelx = np.array(((1, 0, -1),\n\t\t\t\t\t\t(2, 0, -2),\n\t\t\t\t\t\t(1, 0, -1)), dtype=np.float32)\n\n\t\t# padding\n\t\ttmp = np.pad(gray, (1, 1), 'edge')\n\n\t\t# prepare\n\t\tIx = np.zeros_like(gray, dtype=np.float32)\n\t\tIy = np.zeros_like(gray, dtype=np.float32)\n\n\t\t# get differential\n\t\tfor y in range(H):\n\t\t\tfor x in range(W):\n\t\t\t\tIx[y, x] = np.mean(tmp[y : y  + 3, x : x + 3] * sobelx)\n\t\t\t\tIy[y, x] = np.mean(tmp[y : y + 3, x : x + 3] * sobely)\n\t\t\t\n\t\tIx2 = Ix ** 2\n\t\tIy2 = Iy ** 2\n\t\tIxy = Ix * Iy\n\n\t\treturn Ix2, Iy2, Ixy\n\n\t\t\n\n\t## Hessian\n\tdef corner_detect(gray, Ix2, Iy2, Ixy):\n\t\t# get shape\n\t\tH, W = gray.shape\n\n\t\t# prepare for show detection\n\t\tout = np.array((gray, gray, gray))\n\t\tout = np.transpose(out, (1,2,0))\n\n\t\t# get Hessian value\n\t\tHes = np.zeros((H, W))\n\n\t\tfor y in range(H):\n\t\t\tfor x in range(W):\n\t\t\t\tHes[y,x] = Ix2[y,x] * Iy2[y,x] - Ixy[y,x] ** 2\n\n\t\t## Detect Corner and show\n\t\tfor y in range(H):\n\t\t\tfor x in range(W):\n\t\t\t\tif Hes[y,x] == np.max(Hes[max(y-1, 0) : min(y+2, H), max(x-1, 0) : min(x+2, W)]) and Hes[y, x] > np.max(Hes) * 0.1:\n\t\t\t\t\tout[y, x] = [0, 0, 255]\n\n\t\tout = out.astype(np.uint8)\n\n\t\treturn out\n\n\t\n\t# 1. grayscale\n\tgray = BGR2GRAY(img)\n\n\t# 2. get difference image\n\tIx2, Iy2, Ixy = Sobel_filtering(gray)\n\n\t# 3. corner detection\n\tout = corner_detect(gray, Ix2, Iy2, Ixy)\n\n\treturn out\n\n\n# Read image\nimg = cv2.imread(\"thorino.jpg\").astype(np.float32)\n\n# Hessian corner detection\nout = Hessian_corner(img)\n\ncv2.imwrite(\"out.jpg\", out)\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)"}], "task": "code_generation", "index": 280, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_281", "instruction": "Harrisのコーナー検出のアルゴリズムは、\n1. 画像をグレースケール化。\n2. Sobelフィルタにより、ヘシアン行列を求める。\n\nH = [ Ix^2  IxIy]\n      IxIy  Iy^2\n\n3. Ix^2, Iy^2, IxIyにそれぞれガウシアンフィルターをかける。\n4. 各ピクセル毎に、R = det(H) - k (trace(H))^2 を計算する。 (kは実験的に0.04 - 0.16らへんが良いとされる)\n5. R >= max(R) * th を満たすピクセルがコーナーとなる。  (thは0.1となることが多い)\n\n各パラメータは以下の通り。\n- ガウシアンフィルター(k=3, sigma=3)\n- k = 0.04, th = 0.1\n\npythonを用いて、処理1-3までを実装せよ。", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n# Harris corner detection\ndef Harris_corner_step1(img):\n\n\t## Grayscale\n\tdef BGR2GRAY(img):\n\t\tgray = 0.2126 * img[..., 2] + 0.7152 * img[..., 1] + 0.0722 * img[..., 0]\n\t\tgray = gray.astype(np.uint8)\n\t\treturn gray\n\n\t## Sobel\n\tdef Sobel_filtering(gray):\n\t\t# get shape\n\t\tH, W = gray.shape\n\n\t\t# sobel kernel\n\t\tsobely = np.array(((1, 2, 1),\n\t\t\t\t\t\t(0, 0, 0),\n\t\t\t\t\t\t(-1, -2, -1)), dtype=np.float32)\n\n\t\tsobelx = np.array(((1, 0, -1),\n\t\t\t\t\t\t(2, 0, -2),\n\t\t\t\t\t\t(1, 0, -1)), dtype=np.float32)\n\n\t\t# padding\n\t\ttmp = np.pad(gray, (1, 1), 'edge')\n\n\t\t# prepare\n\t\tIx = np.zeros_like(gray, dtype=np.float32)\n\t\tIy = np.zeros_like(gray, dtype=np.float32)\n\n\t\t# get differential\n\t\tfor y in range(H):\n\t\t\tfor x in range(W):\n\t\t\t\tIx[y, x] = np.mean(tmp[y : y  + 3, x : x + 3] * sobelx)\n\t\t\t\tIy[y, x] = np.mean(tmp[y : y + 3, x : x + 3] * sobely)\n\t\t\t\n\t\tIx2 = Ix ** 2\n\t\tIy2 = Iy ** 2\n\t\tIxy = Ix * Iy\n\n\t\treturn Ix2, Iy2, Ixy\n\n\n\t# gaussian filtering\n\tdef gaussian_filtering(I, K_size=3, sigma=3):\n\t\t# get shape\n\t\tH, W = I.shape\n\n\t\t## gaussian\n\t\tI_t = np.pad(I, (K_size // 2, K_size // 2), 'edge')\n\n\t\t# gaussian kernel\n\t\tK = np.zeros((K_size, K_size), dtype=np.float)\n\t\tfor x in range(K_size):\n\t\t\tfor y in range(K_size):\n\t\t\t\t_x = x - K_size // 2\n\t\t\t\t_y = y - K_size // 2\n\t\t\t\tK[y, x] = np.exp( -(_x ** 2 + _y ** 2) / (2 * (sigma ** 2)))\n\t\tK /= (sigma * np.sqrt(2 * np.pi))\n\t\tK /= K.sum()\n\n\t\t# filtering\n\t\tfor y in range(H):\n\t\t\tfor x in range(W):\n\t\t\t\tI[y,x] = np.sum(I_t[y : y + K_size, x : x + K_size] * K)\n\t\t\t\t\n\t\treturn I\n\n\t\n\t# 1. grayscale\n\tgray = BGR2GRAY(img)\n\n\t# 2. get difference image\n\tIx2, Iy2, Ixy = Sobel_filtering(gray)\n\n\t# 3. gaussian filtering\n\tIx2 = gaussian_filtering(Ix2, K_size=3, sigma=3)\n\tIy2 = gaussian_filtering(Iy2, K_size=3, sigma=3)\n\tIxy = gaussian_filtering(Ixy, K_size=3, sigma=3)\n\n\t# show result\n\tplt.subplots_adjust(left=0, right=1, top=1, bottom=0, hspace=0, wspace=0.2)\n\n\tplt.subplot(1,3,1)\n\tplt.imshow(Ix2, cmap='gray')\n\tplt.title(\"Ix^2\")\n\tplt.axis(\"off\")\n\n\tplt.subplot(1,3,2)\n\tplt.imshow(Iy2, cmap='gray')\n\tplt.title(\"Iy^2\")\n\tplt.axis(\"off\")\n\n\tplt.subplot(1,3,3)\n\tplt.imshow(Ixy, cmap='gray')\n\tplt.title(\"Ixy\")\n\tplt.axis(\"off\")\n\n\tplt.savefig(\"out.png\")\n\tplt.show()\n\n\n# Read image\nimg = cv2.imread(\"thorino.jpg\").astype(np.float32)\n\n# Harris corner detection step1\nout = Harris_corner_step1(img)"}], "task": "code_generation", "index": 281, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_282", "instruction": "Harrisのコーナー検出のアルゴリズムは、\n1. 画像をグレースケール化。\n2. Sobelフィルタにより、ヘシアン行列を求める。\n\nH = [ Ix^2  IxIy]\n      IxIy  Iy^2\n\n3. Ix^2, Iy^2, IxIyにそれぞれガウシアンフィルターをかける。\n4. 各ピクセル毎に、R = det(H) - k (trace(H))^2 を計算する。 (kは実験的に0.04 - 0.16らへんが良いとされる)\n5. R >= max(R) * th を満たすピクセルがコーナーとなる。  (thは0.1となることが多い)\n\n各パラメータは以下の通り。\n- ガウシアンフィルター(k=3, sigma=3)\n- k = 0.04, th = 0.1\n\npythonを用いて、処理4-5を実装せよ。", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n# Harris corner detection\ndef Harris_corner(img):\n\n\t## Grayscale\n\tdef BGR2GRAY(img):\n\t\tgray = 0.2126 * img[..., 2] + 0.7152 * img[..., 1] + 0.0722 * img[..., 0]\n\t\tgray = gray.astype(np.uint8)\n\t\treturn gray\n\n\t## Sobel\n\tdef Sobel_filtering(gray):\n\t\t# get shape\n\t\tH, W = gray.shape\n\n\t\t# sobel kernel\n\t\tsobely = np.array(((1, 2, 1),\n\t\t\t\t\t\t(0, 0, 0),\n\t\t\t\t\t\t(-1, -2, -1)), dtype=np.float32)\n\n\t\tsobelx = np.array(((1, 0, -1),\n\t\t\t\t\t\t(2, 0, -2),\n\t\t\t\t\t\t(1, 0, -1)), dtype=np.float32)\n\n\t\t# padding\n\t\ttmp = np.pad(gray, (1, 1), 'edge')\n\n\t\t# prepare\n\t\tIx = np.zeros_like(gray, dtype=np.float32)\n\t\tIy = np.zeros_like(gray, dtype=np.float32)\n\n\t\t# get differential\n\t\tfor y in range(H):\n\t\t\tfor x in range(W):\n\t\t\t\tIx[y, x] = np.mean(tmp[y : y  + 3, x : x + 3] * sobelx)\n\t\t\t\tIy[y, x] = np.mean(tmp[y : y + 3, x : x + 3] * sobely)\n\t\t\t\n\t\tIx2 = Ix ** 2\n\t\tIy2 = Iy ** 2\n\t\tIxy = Ix * Iy\n\n\t\treturn Ix2, Iy2, Ixy\n\n\n\t# gaussian filtering\n\tdef gaussian_filtering(I, K_size=3, sigma=3):\n\t\t# get shape\n\t\tH, W = I.shape\n\n\t\t## gaussian\n\t\tI_t = np.pad(I, (K_size // 2, K_size // 2), 'edge')\n\n\t\t# gaussian kernel\n\t\tK = np.zeros((K_size, K_size), dtype=np.float)\n\t\tfor x in range(K_size):\n\t\t\tfor y in range(K_size):\n\t\t\t\t_x = x - K_size // 2\n\t\t\t\t_y = y - K_size // 2\n\t\t\t\tK[y, x] = np.exp( -(_x ** 2 + _y ** 2) / (2 * (sigma ** 2)))\n\t\tK /= (sigma * np.sqrt(2 * np.pi))\n\t\tK /= K.sum()\n\n\t\t# filtering\n\t\tfor y in range(H):\n\t\t\tfor x in range(W):\n\t\t\t\tI[y,x] = np.sum(I_t[y : y + K_size, x : x + K_size] * K)\n\t\t\t\t\n\t\treturn I\n\n\t# corner detect\n\tdef corner_detect(gray, Ix2, Iy2, Ixy, k=0.04, th=0.1):\n\t\t# prepare output image\n\t\tout = np.array((gray, gray, gray))\n\t\tout = np.transpose(out, (1,2,0))\n\n\t\t# get R\n\t\tR = (Ix2 * Iy2 - Ixy ** 2) - k * ((Ix2 + Iy2) ** 2)\n\n\t\t# detect corner\n\t\tout[R >= np.max(R) * th] = [0, 0, 255]\n\n\t\tout = out.astype(np.uint8)\n\n\t\treturn out\n\n\t\n\t# 1. grayscale\n\tgray = BGR2GRAY(img)\n\n\t# 2. get difference image\n\tIx2, Iy2, Ixy = Sobel_filtering(gray)\n\n\t# 3. gaussian filtering\n\tIx2 = gaussian_filtering(Ix2, K_size=3, sigma=3)\n\tIy2 = gaussian_filtering(Iy2, K_size=3, sigma=3)\n\tIxy = gaussian_filtering(Ixy, K_size=3, sigma=3)\n\n\t# 4. corner detect\n\tout = corner_detect(gray, Ix2, Iy2, Ixy)\n\n\treturn out\n\n\n# Read image\nimg = cv2.imread(\"thorino.jpg\").astype(np.float32)\n\n# Harris corner detection\nout = Harris_corner(img)\n\ncv2.imwrite(\"out.jpg\", out)\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)"}], "task": "code_generation", "index": 282, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_283", "instruction": "pythonを用いて、簡単な画像認識を作成しなさい。\n\n画像認識とは画像に写っているモノが何か（どのクラスに属するか）を特定するタスクである。画像認識はよく、画像分類、Classification(クラス分類)、Categorization(カテゴライゼーション)、Clustering(クラスタリング)、などと呼ばれる。\n\nよくある手法は画像から何らかの特徴(HOGやSIFT, SURFなど)を抽出し、その特徴によりクラスを判別する。CNNが流行る以前はこのアプローチがよく取られたが、CNNは特徴抽出から判別までを一括して行える。\n\nここでは、画像の色ヒストグラムを用いた簡単な画像認識を行う。\n\nアルゴリズムとしては、\n1. 画像(train_*.jpg)を減色処理(Q.6. RGBをそれぞれ4階調)する。\n2. 減色処理した画像のヒストグラムを作成する。ここでのヒストグラムはRGBがそれぞれ4値をとるが、それらを区別するため、B=[1,4], G=[5,8], R=[9,12]のbin=12となる。それぞれの画像に対応するヒストグラムも保存する必要があるので注意。\n 　つまり、database = np.zeros((10(学習データ数), 13(RGB + class), dtype=np.int)　に学習データのヒストグラムを格納する必要がある。\n3. ２のヒストグラムをデータベースとする。\n4. 認識したい画像(test_@@@.jpg)とヒストグラムの差を計算して、特徴量とする。\n5. ヒストグラムの差の合計で、最小となった画像が予測となるクラスである。つまり、色が近い画像と同じクラスになると考えられる。\n\nここではpythonを用いて処理1-3を実装し、ヒストグラムを可視化せよ。\n学習データはdatasetフォルダにあり train_akahara_@@@.jpg (クラス1)と train_madara_@@@.jpg(クラス2) を用いる。(計10枚)\nakaharaとはアカハライモリ、madaraはマダライモリである。\n\n\nこのような予め特徴量を保存しておくデータベース型は人工知能第一世代の手法である。ようは、全部のパターンを暗記しておけばOKという考え方である。ただし、そうするとメモリを大量に消費するので使用が限られる手法である。", "instances": [{"input": "", "output": "mport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom glob import glob\n\n## Dicrease color\ndef dic_color(img):\n    img //= 63\n    img = img * 64 + 32\n    return img\n\n## Database\ndef get_DB():\n    # get image paths\n    train = glob(\"dataset/train_*\")\n    train.sort()\n\n    # prepare database\n    db = np.zeros((len(train), 13), dtype=np.int32)\n\n    # each image\n    for i, path in enumerate(train):\n        img = dic_color(cv2.imread(path))\n        # get histogram\n        for j in range(4):\n            db[i, j] = len(np.where(img[..., 0] == (64 * j + 32))[0])\n            db[i, j+4] = len(np.where(img[..., 1] == (64 * j + 32))[0])\n            db[i, j+8] = len(np.where(img[..., 2] == (64 * j + 32))[0])\n\n        # get class\n        if 'akahara' in path:\n            cls = 0\n        elif 'madara' in path:\n            cls = 1\n\n        # store class label\n        db[i, -1] = cls\n\n        img_h = img.copy() // 64\n        img_h[..., 1] += 4\n        img_h[..., 2] += 8\n        plt.subplot(2, 5, i+1)\n        plt.hist(img_h.ravel(), bins=12, rwidth=0.8)\n        plt.title(path)\n\n    print(db)\n    plt.show()\n\n# get database\nget_DB()"}], "task": "code_generation", "index": 283, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_284", "instruction": "pythonを用いて、簡単な画像認識を作成しなさい。\n\n画像認識とは画像に写っているモノが何か（どのクラスに属するか）を特定するタスクである。画像認識はよく、画像分類、Classification(クラス分類)、Categorization(カテゴライゼーション)、Clustering(クラスタリング)、などと呼ばれる。\n\nよくある手法は画像から何らかの特徴(HOGやSIFT, SURFなど)を抽出し、その特徴によりクラスを判別する。CNNが流行る以前はこのアプローチがよく取られたが、CNNは特徴抽出から判別までを一括して行える。\n\nここでは、画像の色ヒストグラムを用いた簡単な画像認識を行う。\n\nアルゴリズムとしては、\n1. 画像(train_*.jpg)を減色処理(Q.6. RGBをそれぞれ4階調)する。\n2. 減色処理した画像のヒストグラムを作成する。ここでのヒストグラムはRGBがそれぞれ4値をとるが、それらを区別するため、B=[1,4], G=[5,8], R=[9,12]のbin=12となる。それぞれの画像に対応するヒストグラムも保存する必要があるので注意。\n 　つまり、database = np.zeros((10(学習データ数), 13(RGB + class), dtype=np.int)　に学習データのヒストグラムを格納する必要がある。\n3. ２のヒストグラムをデータベースとする。\n4. 認識したい画像(test_@@@.jpg)とヒストグラムの差を計算して、特徴量とする。\n5. ヒストグラムの差の合計で、最小となった画像が予測となるクラスである。つまり、色が近い画像と同じクラスになると考えられる。\n\npythonを用いて処理4-5を実装せよ。\n\nテストデータには test_akahara_@@@.jpgとtest_madara_@@@.jpgを用いよ。（計4枚）\nただし、各画像と最もヒストグラム差分が小さい画像の名前と予測クラスの2つを出力せよ。\n\nこれはNearesetNeighbourと呼ばれる評価方法である。", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom glob import glob\n\n# Dicrease color\ndef dic_color(img):\n    img //= 63\n    img = img * 64 + 32\n    return img\n\n# Database\ndef get_DB():\n    # get training image path\n    train = glob(\"dataset/train_*\")\n    train.sort()\n\n    # prepare database\n    db = np.zeros((len(train), 13), dtype=np.int32)\n\n    # prepare path database\n    pdb = []\n\n    # each image\n    for i, path in enumerate(train):\n        # read image\n        img = dic_color(cv2.imread(path))\n\n        #get histogram\n        for j in range(4):\n            db[i, j] = len(np.where(img[..., 0] == (64 * j + 32))[0])\n            db[i, j+4] = len(np.where(img[..., 1] == (64 * j + 32))[0])\n            db[i, j+8] = len(np.where(img[..., 2] == (64 * j + 32))[0])\n\n        # get class\n        if 'akahara' in path:\n            cls = 0\n        elif 'madara' in path:\n            cls = 1\n\n        # store class label\n        db[i, -1] = cls\n\n        # store image path\n        pdb.append(path)\n\n    return db, pdb\n\n# test\ndef test_DB(db, pdb):\n    # get test image path\n    test = glob(\"dataset/test_*\")\n    test.sort()\n\n    success_num = 0.\n\n    # each image\n    for path in test:\n        # read image\n        img = dic_color(cv2.imread(path))\n\n        # get histogram\n        hist = np.zeros(12, dtype=np.int32)\n        for j in range(4):\n            hist[j] = len(np.where(img[..., 0] == (64 * j + 32))[0])\n            hist[j+4] = len(np.where(img[..., 1] == (64 * j + 32))[0])\n            hist[j+8] = len(np.where(img[..., 2] == (64 * j + 32))[0])\n\n        # get histogram difference\n        difs = np.abs(db[:, :12] - hist)\n        difs = np.sum(difs, axis=1)\n\n        # get argmin of difference\n        pred_i = np.argmin(difs)\n\n        # get prediction label\n        pred = db[pred_i, -1]\n\n        if pred == 0:\n            pl = \"akahara\"\n        elif pred == 1:\n            pl = \"madara\"\n        \n        print(path, \"is similar >>\", pdb[pred_i], \" Pred >>\", pl)\n\ndb, pdb = get_DB()\ntest_DB(db, pdb)"}], "task": "code_generation", "index": 284, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_285", "instruction": "pythonを用いて、画像認識の結果を評価しなさい。\n\n画像認識の場合はどれくらい正解クラスを予想できたかを示すAccuracy(Precisionといったりもする)が一般的な評価指標である。Accuracyは次式で計算される。要はテストにおける得点率である。小数表示するときや、100掛けてパーセンテージで表すこともある。\n\n\nAccuracy = (正解した画像数) / (テストした画像の総数)\n\n以上を踏まえて、画像認識のAccuracyを求めよ。\n\nなお、画像認識のアルゴリズムは以下の通りである。\n\n画像認識アルゴリズムとしては、\n1. 画像(train_*.jpg)を減色処理(Q.6. RGBをそれぞれ4階調)する。\n2. 減色処理した画像のヒストグラムを作成する。ここでのヒストグラムはRGBがそれぞれ4値をとるが、それらを区別するため、B=[1,4], G=[5,8], R=[9,12]のbin=12となる。それぞれの画像に対応するヒストグラムも保存する必要があるので注意。\n 　つまり、database = np.zeros((10(学習データ数), 13(RGB + class), dtype=np.int)　に学習データのヒストグラムを格納する必要がある。\n3. ２のヒストグラムをデータベースとする。\n4. 認識したい画像(test_@@@.jpg)とヒストグラムの差を計算して、特徴量とする。\n5. ヒストグラムの差の合計で、最小となった画像が予測となるクラスである。つまり、色が近い画像と同じクラスになると考えられる。", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom glob import glob\n\n# Dicrease color\ndef dic_color(img):\n    img //= 63\n    img = img * 64 + 32\n    return img\n\n# Database\ndef get_DB():\n    # get training image path\n    train = glob(\"dataset/train_*\")\n    train.sort()\n\n    # prepare database\n    db = np.zeros((len(train), 13), dtype=np.int32)\n\n    # prepare path database\n    pdb = []\n\n    # each image\n    for i, path in enumerate(train):\n        # read image\n        img = dic_color(cv2.imread(path))\n\n        #get histogram\n        for j in range(4):\n            db[i, j] = len(np.where(img[..., 0] == (64 * j + 32))[0])\n            db[i, j+4] = len(np.where(img[..., 1] == (64 * j + 32))[0])\n            db[i, j+8] = len(np.where(img[..., 2] == (64 * j + 32))[0])\n\n        # get class\n        if 'akahara' in path:\n            cls = 0\n        elif 'madara' in path:\n            cls = 1\n\n        # store class label\n        db[i, -1] = cls\n\n        # store image path\n        pdb.append(path)\n\n    return db, pdb\n\n# test\ndef test_DB(db, pdb):\n    # get test image path\n    test = glob(\"dataset/test_*\")\n    test.sort()\n\n    accurate_N = 0.\n\n    # each image\n    for path in test:\n        # read image\n        img = dic_color(cv2.imread(path))\n\n        # get histogram\n        hist = np.zeros(12, dtype=np.int32)\n        for j in range(4):\n            hist[j] = len(np.where(img[..., 0] == (64 * j + 32))[0])\n            hist[j+4] = len(np.where(img[..., 1] == (64 * j + 32))[0])\n            hist[j+8] = len(np.where(img[..., 2] == (64 * j + 32))[0])\n\n        # get histogram difference\n        difs = np.abs(db[:, :12] - hist)\n        difs = np.sum(difs, axis=1)\n\n        # get argmin of difference\n        pred_i = np.argmin(difs)\n\n        # get prediction label\n        pred = db[pred_i, -1]\n\n        if pred == 0:\n            pred_label = \"akahara\"\n        elif pred == 1:\n            pred_label = \"madara\"\n\n        gt = \"akahara\" if \"akahara\" in path else \"madara\"\n\n        if gt == pred_label:\n            accurate_N += 1\n        \n        print(path, \"is similar >>\", pdb[pred_i], \" Pred >>\", pred_label)\n\n    accuracy = accurate_N / len(test)\n    print(\"Accuracy >>\", accuracy, \"({}/{})\".format(int(accurate_N), len(test)))\n\ndb, pdb = get_DB()\ntest_DB(db, pdb)"}], "task": "code_generation", "index": 285, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_286", "instruction": "ある画像認識タスクでは、test_madara_2.jpgがtrain_akahara_2.jpgと最も色が近い画像と判断された。\n\n|test_marada_2.jpg|train_akahara_2.jpg|\n|:---:|:---:|\n|![](dataset/test_madara_2.jpg)|![](dataset/train_akahara_2.jpg)|\n\n2つの画像を見比べるとたしかに両方とも緑の割合と黒の割合が近く見えるので、画像全体に対しての画像の色味が同じ様に見えてしまう。これは認識時のサンプルを一つにしたことによって、例外的な画像が選ばれてしまったためである。このように学習データの特徴は常にきれいに分離されているわけではなく、時に特徴の分布から逸しているサンプルも含まれる。\n\nこれを回避するために、ここではpythonを用いて色合いが近い画像認識したい画像(test_@@@.jpg)を3つ選び、それらの多数決によって予測クラスを決定し、Accuracyを計算せよ。\n\nこのように特徴が近いものを学習データからk個選んで判断する手法をk近傍(k-NN: k-Nearest Neighbor)という。NN法はk=1の場合とみれる。", "instances": [{"input": "", "output": "mport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom glob import glob\n\n# Dicrease color\ndef dic_color(img):\n    img //= 63\n    img = img * 64 + 32\n    return img\n\n# Database\ndef get_DB():\n    # get training image path\n    train = glob(\"dataset/train_*\")\n    train.sort()\n\n    # prepare database\n    db = np.zeros((len(train), 13), dtype=np.int32)\n    pdb = []\n\n    # each train\n    for i, path in enumerate(train):\n        # read image\n        img = dic_color(cv2.imread(path))\n        # histogram\n        for j in range(4):\n            db[i, j] = len(np.where(img[..., 0] == (64 * j + 32))[0])\n            db[i, j+4] = len(np.where(img[..., 1] == (64 * j + 32))[0])\n            db[i, j+8] = len(np.where(img[..., 2] == (64 * j + 32))[0])\n\n        # get class\n        if 'akahara' in path:\n            cls = 0\n        elif 'madara' in path:\n            cls = 1\n\n        # store class label\n        db[i, -1] = cls\n\n        # add image path\n        pdb.append(path)\n\n    return db, pdb\n\n# test\ndef test_DB(db, pdb, N=3):\n    # get test image path\n    test = glob(\"dataset/test_*\")\n    test.sort()\n\n    accuracy_N = 0.\n\n    # each image\n    for path in test:\n        # read image\n        img = dic_color(cv2.imread(path))\n\n        # get histogram\n        hist = np.zeros(12, dtype=np.int32)\n        for j in range(4):\n            hist[j] = len(np.where(img[..., 0] == (64 * j + 32))[0])\n            hist[j+4] = len(np.where(img[..., 1] == (64 * j + 32))[0])\n            hist[j+8] = len(np.where(img[..., 2] == (64 * j + 32))[0])\n\n        # get histogram difference\n        difs = np.abs(db[:, :12] - hist)\n        difs = np.sum(difs, axis=1)\n\n        # get top N\n        pred_i = np.argsort(difs)[:N]\n\n        # predict class index\n        pred = db[pred_i, -1]\n\n        # get class label\n        if len(pred[pred == 0]) > len(pred[pred == 1]):\n            pl = \"akahara\"\n        else:\n            pl = 'madara'\n\n        print(path, \"is similar >> \", end='')\n        for i in pred_i:\n            print(pdb[i], end=', ')\n        print(\"|Pred >>\", pl)\n\n        # count accuracy\n        gt = \"akahara\" if \"akahara\" in path else \"madara\"\n        if gt == pl:\n            accuracy_N += 1.\n\n    accuracy = accuracy_N / len(test)\n    print(\"Accuracy >>\", accuracy, \"({}/{})\".format(int(accuracy_N), len(test)))\n\n\ndb, pdb = get_DB()\ntest_DB(db, pdb)"}], "task": "code_generation", "index": 286, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_287", "instruction": "画像認識は教師データを必要とするいわゆる教師あり学習(supervised-training)のものすごく簡単なものだったが、ここでは教師を必要としない教師なし学習(unsupervised-training)で画像を分類する。\n\n最も簡単な方法がK-meansクラスタリング法である。\n\nこれは予めクラス数が分かっている場合に使うことができ、特徴量を重心に分けながらクラスタリングする手法である。\n\nK-Meansアルゴリズムとしては、\n1. データにそれぞれランダムにクラスを割り当てる。\n2. クラスごとに重心を計算する。\n3. 各データと重心の距離を計算し、最も距離が近い重心のクラスを割り当てる。\n4. 2-3をクラス変更がなくなるまで繰り返す。\n\nここでは、減色化とヒストグラムを特徴量として次のようにアルゴリズムを作成する。\n1. 画像を減色化し、ヒストグラムを作成し、これを特徴量とする。\n2. 各画像にランダムに0か1のクラスを割り当てる。 (ここでは、クラス数=2, np.random.seed(1)　として、np.random.random() < thなら0、>= thなら1を割り当てる。th=0.5)\n3. クラスが0、１の特徴量の重心(mean)をそれぞれ取る。(重心は gs = np.zeros((Class, 12), dtype=np.float32)に格納する。)\n4. 各画像に対して、特徴量と重心の距離(ユークリッド距離(L1ノルム): 差を二乗し、その合計のsqrtをとったもの)を計算し、距離が近い重心のクラスを割り当てる。\n5. 3-4をクラスの変更がなくなるまで繰り返す。\n\nここでは、pythonを用いて処理1-3までを実装せよ(4,5のことを考えてループを作らなくてもよい)。分類する画像は*test_@@@.jpg*とする。", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom glob import glob\n\n# Dicrease color\ndef dic_color(img):\n    img //= 63\n    img = img * 64 + 32\n    return img\n\n\n# Database\ndef get_DB():\n    # get training image path\n    train = glob(\"dataset/test_*\")\n    train.sort()\n\n    # prepare database\n    db = np.zeros((len(train), 13), dtype=np.int32)\n    pdb = []\n\n    # each train\n    for i, path in enumerate(train):\n        # read image\n        img = dic_color(cv2.imread(path))\n        # histogram\n        for j in range(4):\n            db[i, j] = len(np.where(img[..., 0] == (64 * j + 32))[0])\n            db[i, j+4] = len(np.where(img[..., 1] == (64 * j + 32))[0])\n            db[i, j+8] = len(np.where(img[..., 2] == (64 * j + 32))[0])\n\n        # get class\n        if 'akahara' in path:\n            cls = 0\n        elif 'madara' in path:\n            cls = 1\n\n        # store class label\n        db[i, -1] = cls\n\n        # add image path\n        pdb.append(path)\n\n    return db, pdb\n\n# k-Means step1\ndef k_means_step1(db, pdb, Class=2):\n    # copy database\n    feats = db.copy()\n\n    # initiate random seed\n    np.random.seed(1)\n\n    # assign random class \n    for i in range(len(feats)):\n        if np.random.random() < 0.5:\n            feats[i, -1] = 0\n        else:\n            feats[i, -1] = 1\n\n    # prepare gravity\n    gs = np.zeros((Class, 12), dtype=np.float32)\n        \n    # get gravity\n    for i in range(Class):\n        gs[i] = np.mean(feats[np.where(feats[..., -1] == i)[0], :12], axis=0)\n    print(\"assigned label\")\n    print(feats)\n    print(\"Grabity\")\n    print(gs)\n\n\ndb, pdb = get_DB()\nk_means_step1(db, pdb)"}], "task": "code_generation", "index": 287, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_288", "instruction": "K-Meansアルゴリズムでは、\n1. データにそれぞれランダムにクラスを割り当てる。\n2. クラスごとに重心を計算する。\n3. 各データと重心の距離を計算し、最も距離が近い重心のクラスを割り当てる。\n4. 2-3をクラス変更がなくなるまで繰り返す。\n\nここでは、減色化とヒストグラムを特徴量として次のようにアルゴリズムを作成する。\n1. 画像を減色化し、ヒストグラムを作成し、これを特徴量とする。\n2. 各画像にランダムに0か1のクラスを割り当てる。 (ここでは、クラス数=2, np.random.seed(1)　として、np.random.random() < thなら0、>= thなら1を割り当てる。th=0.5)\n3. クラスが0、１の特徴量の重心(mean)をそれぞれ取る。(重心は gs = np.zeros((Class, 12), dtype=np.float32)に格納する。)\n4. 各画像に対して、特徴量と重心の距離(ユークリッド距離(L1ノルム): 差を二乗し、その合計のsqrtをとったもの)を計算し、距離が近い重心のクラスを割り当てる。\n5. 3-4をクラスの変更がなくなるまで繰り返す。\n\nここではpythonを用いて処理4-5も実装して、クラスタリングを行え。\n\nここで予測クラスが0,1となっているが、Q.85-87と違いラベルの順番はバラバラである。\nなので、K-meansはあくまでカテゴリ別に分類する手法であり、それが具体的に何のクラスかまでは分からない。\nまた、クラス数は予めこちらが知って置かなければいけない。\n\nK-meansクラスタリングでは最初に割り当てるラベルの状態によって、最後の出力が大きく左右されるので注意が必要である。\nまた、データ数が少ないと失敗しやすい。これはデータ数が少ないことで、真のデータの分布をサンプリングしにくいことが原因である。つまり、データ数が多いほどデータの分布が精度良くえられることによる。", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom glob import glob\n\n# Dicrease color\ndef dic_color(img):\n    img //= 63\n    img = img * 64 + 32\n    return img\n\n\n# Database\ndef get_DB():\n    # get training image path\n    train = glob(\"dataset/test_*\")\n    train.sort()\n\n    # prepare database\n    db = np.zeros((len(train), 13), dtype=np.int32)\n    pdb = []\n\n    # each train\n    for i, path in enumerate(train):\n        # read image\n        img = dic_color(cv2.imread(path))\n        # histogram\n        for j in range(4):\n            db[i, j] = len(np.where(img[..., 0] == (64 * j + 32))[0])\n            db[i, j+4] = len(np.where(img[..., 1] == (64 * j + 32))[0])\n            db[i, j+8] = len(np.where(img[..., 2] == (64 * j + 32))[0])\n\n        # get class\n        if 'akahara' in path:\n            cls = 0\n        elif 'madara' in path:\n            cls = 1\n\n        # store class label\n        db[i, -1] = cls\n\n        # add image path\n        pdb.append(path)\n\n    return db, pdb\n\n# k-Means step2\ndef k_means_step2(db, pdb, Class=2):\n    # copy database\n    feats = db.copy()\n\n    # initiate random seed\n    np.random.seed(1)\n\n    # assign random class \n    for i in range(len(feats)):\n        if np.random.random() < 0.5:\n            feats[i, -1] = 0\n        else:\n            feats[i, -1] = 1\n\n    while True:\n        # prepare greavity\n        gs = np.zeros((Class, 12), dtype=np.float32)\n        change_count = 0\n\n        # compute gravity\n        for i in range(Class):\n            gs[i] = np.mean(feats[np.where(feats[..., -1] == i)[0], :12], axis=0)\n\n        # re-labeling\n        for i in range(len(feats)):\n            # get distance each nearest graviry\n            dis = np.sqrt(np.sum(np.square(np.abs(gs - feats[i, :12])), axis=1))\n\n            # get new label\n            pred = np.argmin(dis, axis=0)\n\n            # if label is difference from old label\n            if int(feats[i, -1]) != pred:\n                change_count += 1\n                feats[i, -1] = pred\n\n        if change_count < 1:\n            break\n\n    for i in range(db.shape[0]):\n        print(pdb[i], \" Pred:\", feats[i, -1])\n\n\ndb, pdb = get_DB()\nk_means_step2(db, pdb)"}], "task": "code_generation", "index": 288, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_289", "instruction": "pythonで、K-meansを用いて*train_@@@.jpg*の10枚を完璧にクラスタリングせよ。\n\nここでは、np.random.seed()の値やラベルを割り当てる閾値 np.random.random() < th　のthを変更して、K-meansでクラスを完璧に予測せよ。\ntrain_@@@.jpgはQ.89より画像数が２倍以上になっているので、クラスタリングしやすくなっている。\n\nこれは試行錯誤するしかない。", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom glob import glob\n\n# Dicrease color\ndef dic_color(img):\n    img //= 63\n    img = img * 64 + 32\n    return img\n\n\n# Database\ndef get_DB():\n    # get training image path\n    train = glob(\"dataset/train_*\")\n    train.sort()\n\n    # prepare database\n    db = np.zeros((len(train), 13), dtype=np.int32)\n    pdb = []\n\n    # each train\n    for i, path in enumerate(train):\n        # read image\n        img = dic_color(cv2.imread(path))\n        # histogram\n        for j in range(4):\n            db[i, j] = len(np.where(img[..., 0] == (64 * j + 32))[0])\n            db[i, j+4] = len(np.where(img[..., 1] == (64 * j + 32))[0])\n            db[i, j+8] = len(np.where(img[..., 2] == (64 * j + 32))[0])\n\n        # get class\n        if 'akahara' in path:\n            cls = 0\n        elif 'madara' in path:\n            cls = 1\n\n        # store class label\n        db[i, -1] = cls\n\n        # add image path\n        pdb.append(path)\n\n    return db, pdb\n\n# k-Means\ndef k_means(db, pdb, Class=2, th=0.5):\n    # copy database\n    feats = db.copy()\n\n    # initiate random seed\n    np.random.seed(4)\n\n    # assign random class \n    for i in range(len(feats)):\n        if np.random.random() < th:\n            feats[i, -1] = 0\n        else:\n            feats[i, -1] = 1\n\n    while True:\n        # prepare greavity\n        gs = np.zeros((Class, 12), dtype=np.float32)\n        change_count = 0\n\n        # compute gravity\n        for i in range(Class):\n            gs[i] = np.mean(feats[np.where(feats[..., -1] == i)[0], :12], axis=0)\n\n        # re-labeling\n        for i in range(len(feats)):\n            # get distance each nearest graviry\n            dis = np.sqrt(np.sum(np.square(np.abs(gs - feats[i, :12])), axis=1))\n\n            # get new label\n            pred = np.argmin(dis, axis=0)\n\n            # if label is difference from old label\n            if int(feats[i, -1]) != pred:\n                change_count += 1\n                feats[i, -1] = pred\n\n        if change_count < 1:\n            break\n\n    for i in range(db.shape[0]):\n        print(pdb[i], \" Pred:\", feats[i, -1])\n\n\ndb, pdb = get_DB()\nk_means(db, pdb, th=0.3)"}], "task": "code_generation", "index": 289, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_290", "instruction": "pythonを用いて、imori.jpgをK-meansを用いた減色処理せよ。\n\n減色処理はQ.6でも扱ったが、Q.6では予め決めた色に減色した。ここで扱うのはK-meansを用いて動的に減色する色を決定する。\n\nアルゴリズムは,\n1. 画像からランダムにK個のRGB成分をサンプリングする。（これをクラスと呼ぶことにする。）\n2. 画像のそれぞれの画素に対して色の距離が最小となるクラスのインデックスを割り振る。\n\n色の距離 dis = sqrt( (R-R')^2 + (G-G')^2 + (B-B')^2)\n\n3. 各インデックスに対応する色成分の平均をRGBそれぞれに対して取り、新たなクラスとする。\n4. 元のクラスと新しいクラスが全く同じならK-meansを終了する。そうでなければ、新しいクラスを元クラスとして2-3を繰り返す。\n5. 元画像の各画素で色の距離が最小となるクラスのRGBを割り当てる。\n\nここでは1-2を実装せよ。\n- クラス数はk=5とする\n- ここでは画像をreshape((HxW, 3))にreshapeすると扱いやすくなる。\n- 1においてはnp.random.seed(0)として、np.random.choice(np.arrange(画像のWxH), 5, replace=False)\n- まずは3-5のループを考えずに実装せよ\n\n\n\n# 最初に選べれた色\n[[140. 121. 148.]\n [135. 109. 122.]\n [211. 189. 213.]\n [135.  86.  84.]\n [118.  99.  96.]]\n\n\n最初に選ばれた色との色の距離でクラスのインデックスをつけたもの(アルゴリズム2)。\n解答では0-4にインデックスの値をx50にして見やすいようにしている。", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom glob import glob\n\n\n# K-means step1\ndef k_means_step1(img, Class=5):\n\t#  get shape\n\tH, W, C = img.shape\n\n\t# initiate random seed\n\tnp.random.seed(0)\n\n\t# reshape\n\timg = np.reshape(img, (H * W, -1))\n\n\t# select one index randomly\n\ti = np.random.choice(np.arange(H * W), Class, replace=False)\n\tCs = img[i].copy()\n\n\tprint(Cs)\n\n\tclss = np.zeros((H * W), dtype=int)\n\n\t# each pixel\n\tfor i in range(H * W):\n\t\t# get distance from base pixel\n\t\tdis = np.sqrt(np.sum((Cs - img[i]) ** 2, axis=1))\n\t\t# get argmin distance\n\t\tclss[i] = np.argmin(dis)\n\n\t# show\n\tout = np.reshape(clss, (H, W)) * 50\n\tout = out.astype(np.uint8)\n\n\treturn out\n\n\n# read image\nimg = cv2.imread(\"imori.jpg\").astype(np.float32)\n\n# K-means step2\nout = k_means_step1(img)\n\ncv2.imwrite(\"out.jpg\", out)\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)"}], "task": "code_generation", "index": 290, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_291", "instruction": "減色処理のアルゴリズムは,\n1. 画像からランダムにK個のRGB成分をサンプリングする。（これをクラスと呼ぶことにする。）\n2. 画像のそれぞれの画素に対して色の距離が最小となるクラスのインデックスを割り振る。\n\n色の距離 dis = sqrt( (R-R')^2 + (G-G')^2 + (B-B')^2)\n\n3. 各インデックスに対応する色成分の平均をRGBそれぞれに対して取り、新たなクラスとする。\n4. 元のクラスと新しいクラスが全く同じならK-meansを終了する。そうでなければ、新しいクラスを元クラスとして2-3を繰り返す。\n5. 元画像の各画素で色の距離が最小となるクラスのRGBを割り当てる。\n\npythonを用いて処理3-5を実装せよ。\n\n# 選ばれた色\n[[182.90548706 156.39289856 181.05880737]\n [157.28413391 124.02828979 136.6774292 ]\n [228.36817932 201.76049805 211.80619812]\n [ 91.52492523  57.49259949  56.78660583]\n [121.73962402  88.02610779  96.16177368]]\n\n減色処理したもの。塗り絵イラスト風な画像にできる。k=10にすればある程度の色を保持しながらもイラスト風に減色できる。\n\nまた、k=5にしてmadara.jpgにも試してみよ。", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom glob import glob\n\ndef k_means(img, Class=5):\n    # get shape\n    H, W, C = img.shape\n\n    # initiate random seed\n    np.random.seed(0)\n\n    # reshape image\n    img = np.reshape(img, (H * W, -1))\n\n    # get index randomly\n    i = np.random.choice(np.arange(H * W), Class, replace=False)\n    Cs = img[i].copy()\n\n    while True:\n        # prepare pixel class label\n        clss = np.zeros((H * W), dtype=int)\n        \n        # each pixel\n        for i in range(H * W):\n            # get distance from index pixel\n            dis = np.sqrt(np.sum((Cs - img[i])**2, axis=1))\n            # get argmin distance\n            clss[i] = np.argmin(dis)\n\n        # selected pixel values\n        Cs_tmp = np.zeros((Class, 3))\n        \n        # each class label\n        for i in range(Class):\n            Cs_tmp[i] = np.mean(img[clss == i], axis=0)\n\n        # if not any change\n        if (Cs == Cs_tmp).all():\n            break\n        else:\n            Cs = Cs_tmp.copy()\n\n    # prepare out image\n    out = np.zeros((H * W, 3), dtype=np.float32)\n\n    # assign selected pixel values  \n    for i in range(Class):\n        out[clss == i] = Cs[i]\n\n    print(Cs)\n        \n    out = np.clip(out, 0, 255)\n\n    # reshape out image\n    out = np.reshape(out, (H, W, 3))\n    out = out.astype(np.uint8)\n\n    return out\n\n# read image\nimg = cv2.imread(\"imori.jpg\").astype(np.float32)\n\n# K-means\nout = k_means(img)\n\ncv2.imwrite(\"out.jpg\", out)\ncv2.imshow(\"result\", out)\ncv2.waitKey(0)"}], "task": "code_generation", "index": 291, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_292", "instruction": "機械学習で用いる学習データの準備を行う。\n\n最終的にはイモリの顔か否かを判別する識別器を作りたい。そのためにはイモリの顔の画像とイモリの顔以外の画像が必要になる。それらを用意するためのプログラムを作成する。\n\nそのためにはイモリの顔周辺を一枚の画像から切り抜く必要がある。\nそこで一つの矩形を設定して(GT: Ground-truth, 正解と呼ぶ)、ランダムに切り抜いた矩形がGTとある程度重なっていれば、イモリの顔となる。\n\nその重なり具合を計算するのが、IoU: Intersection over unionであり、次式で計算される。\n\nR1...Ground-truthの領域 , R2...切り抜いた矩形 , Rol...R1とR2の重なっている領域\nIoU = |Rol| / |R1 + R2 - Rol|\n\npythonを用いて、以下の２つの矩形のIoUを計算せよ。\n\n# [x1, y1, x2, y2] x1,y1...矩形の左上のx,y  x2,y2...矩形の右下のx,y\na = np.array((50, 50, 150, 150), dtype=np.float32)\n\nb = np.array((60, 60, 170, 160), dtype=np.float32)\n\n", "instances": [{"input": "", "output": "import numpy as np\n\n# get IoU overlap ratio\ndef iou(a, b):\n\t# get area of a\n    area_a = (a[2] - a[0]) * (a[3] - a[1])\n\t# get area of b\n    area_b = (b[2] - b[0]) * (b[3] - b[1])\n\n\t# get left top x of IoU\n    iou_x1 = np.maximum(a[0], b[0])\n\t# get left top y of IoU\n    iou_y1 = np.maximum(a[1], b[1])\n\t# get right bottom of IoU\n    iou_x2 = np.minimum(a[2], b[2])\n\t# get right bottom of IoU\n    iou_y2 = np.minimum(a[3], b[3])\n\n\t# get width of IoU\n    iou_w = iou_x2 - iou_x1\n\t# get height of IoU\n    iou_h = iou_y2 - iou_y1\n\n\t# get area of IoU\n    area_iou = iou_w * iou_h\n\t# get overlap ratio between IoU and all area\n    iou = area_iou / (area_a + area_b - area_iou)\n\n    return iou\n\n# [x1, y1, x2, y2]\na = np.array((50, 50, 150, 150), dtype=np.float32)\n\nb = np.array((60, 60, 170, 160), dtype=np.float32)\n\nprint(iou(a, b))"}], "task": "code_generation", "index": 292, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_293", "instruction": "pythonを用いて、imori_1.jpgからランダムに画像を切り抜いて(cropping, クラッピングと呼ぶ)学習データを作成する。\n\nここでは画像から60x60のサイズの矩形をランダムに200個切り抜け。\n\nただし、以下の条件を満たせ。\n1. np.random.seed(0)として、切り抜く矩形の左上のx1 = np.random.randint(W-60), y1=np.random.randint(H-60)で求めよ。\n2. GT (gt = np.array((47, 41, 129, 103), dtype=np.float32))とのIoUが0.5以上の時はその矩形に教師ラベル1, 0.5未満の場合はラベル0を与えよ。", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\n\nnp.random.seed(0)\n\n# get IoU overlap ratio\ndef iou(a, b):\n\t# get area of a\n    area_a = (a[2] - a[0]) * (a[3] - a[1])\n\t# get area of b\n    area_b = (b[2] - b[0]) * (b[3] - b[1])\n\n\t# get left top x of IoU\n    iou_x1 = np.maximum(a[0], b[0])\n\t# get left top y of IoU\n    iou_y1 = np.maximum(a[1], b[1])\n\t# get right bottom of IoU\n    iou_x2 = np.minimum(a[2], b[2])\n\t# get right bottom of IoU\n    iou_y2 = np.minimum(a[3], b[3])\n\n\t# get width of IoU\n    iou_w = iou_x2 - iou_x1\n\t# get height of IoU\n    iou_h = iou_y2 - iou_y1\n\n\t# get area of IoU\n    area_iou = iou_w * iou_h\n\t# get overlap ratio between IoU and all area\n    iou = area_iou / (area_a + area_b - area_iou)\n\n    return iou\n\n\n# crop and create database\ndef crop_bbox(img, gt, Crop_N=200, L=60, th=0.5):\n    # get shape\n    H, W, C = img.shape\n\n    # each crop\n    for i in range(Crop_N):\n        # get left top x of crop bounding box\n        x1 = np.random.randint(W - L)\n        # get left top y of crop bounding box\n        y1 = np.random.randint(H - L)\n        # get right bottom x of crop bounding box\n        x2 = x1 + L\n        # get right bottom y of crop bounding box\n        y2 = y1 + L\n\n        # crop bounding box\n        crop = np.array((x1, y1, x2, y2))\n\n        # get IoU between crop box and gt\n        _iou = iou(gt, crop)\n\n        # assign label\n        if _iou >= th:\n            cv2.rectangle(img, (x1, y1), (x2, y2), (0,0,255), 1)\n            label = 1\n        else:\n            cv2.rectangle(img, (x1, y1), (x2, y2), (255,0,0), 1)\n            label = 0\n\n    return img\n\n# read image\nimg = cv2.imread(\"imori_1.jpg\")\n\n# gt bounding box\ngt = np.array((47, 41, 129, 103), dtype=np.float32)\n\n# get crop bounding box\nimg = crop_bbox(img, gt)\n\n# draw gt\ncv2.rectangle(img, (gt[0], gt[1]), (gt[2], gt[3]), (0,255,0), 1)\n\ncv2.imwrite(\"out.jpg\", img)\ncv2.imshow(\"result\", img)\ncv2.waitKey(0)"}], "task": "code_generation", "index": 293, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_294", "instruction": "ニューラルネットワークを用いて識別を行う。\nこれは現在流行っているディープラーニングである。\n\n入力層、中間層(ユニット数:64)、出力層(1)のネットワークは次のようにプログラムできる。これは、排他的論理和を実現するネットワークである。\n\n\nimport numpy as np\n\nnp.random.seed(0)\n\nclass NN:\n    def __init__(self, ind=2, w=64, outd=1, lr=0.1):\n        self.w1 = np.random.normal(0, 1, [ind, w])\n        self.b1 = np.random.normal(0, 1, [w])\n        self.wout = np.random.normal(0, 1, [w, outd])\n        self.bout = np.random.normal(0, 1, [outd])\n        self.lr = lr\n\n    def forward(self, x):\n        self.z1 = x\n        self.z2 = sigmoid(np.dot(self.z1, self.w1) + self.b1)\n        self.out = sigmoid(np.dot(self.z2, self.wout) + self.bout)\n        return self.out\n\n    def train(self, x, t):\n        # backpropagation output layer\n        #En = t * np.log(self.out) + (1-t) * np.log(1-self.out)\n        En = (self.out - t) * self.out * (1 - self.out)\n        grad_En = En #np.array([En for _ in range(t.shape[0])])\n        grad_wout = np.dot(self.z2.T, En)\n        grad_bout = np.dot(np.ones([En.shape[0]]), En)\n        self.wout -= self.lr * grad_wout#np.expand_dims(grad_wout, axis=-1)\n        self.bout -= self.lr * grad_bout\n\n        # backpropagation inter layer\n        grad_u1 = np.dot(En, self.wout.T) * self.z2 * (1 - self.z2)\n        grad_w1 = np.dot(self.z1.T, grad_u1)\n        grad_b1 = np.dot(np.ones([grad_u1.shape[0]]), grad_u1)\n        self.w1 -= self.lr * grad_w1\n        self.b1 -= self.lr * grad_b1\n\ndef sigmoid(x):\n    return 1. / (1. + np.exp(-x))\n\ntrain_x = np.array([[0,0], [0,1], [1,0], [1,1]], dtype=np.float32)\ntrain_t = np.array([[0], [1], [1], [0]], dtype=np.float32)\n\nnn = NN(ind=train_x.shape[1])\n\n# train\nfor i in range(1000):\n    nn.forward(train_x)\n    nn.train(train_x, train_t)\n\n# test\nfor j in range(4):\n    x = train_x[j]\n    t = train_t[j]\n    print(\"in:\", x, \"pred:\", nn.forward(x))\n\n\nここでは、pythonを用いて、中間層(ユニット数:64)をもう一層増やし、学習・テストを行いなさい。", "instances": [{"input": "", "output": "import numpy as np\n\nnp.random.seed(0)\n\n# neural network\nclass NN:\n    def __init__(self, ind=2, w=64, w2=64, outd=1, lr=0.1):\n        # layer 1 weight\n        self.w1 = np.random.normal(0, 1, [ind, w])\n        # layer 1 bias\n        self.b1 = np.random.normal(0, 1, [w])\n        # layer 2 weight\n        self.w2 = np.random.normal(0, 1, [w, w2])\n        # layer 2 bias\n        self.b2 = np.random.normal(0, 1, [w2])\n        # output layer weight\n        self.wout = np.random.normal(0, 1, [w2, outd])\n        # output layer bias\n        self.bout = np.random.normal(0, 1, [outd])\n        # learning rate\n        self.lr = lr\n\n    def forward(self, x):\n        # input tensor\n        self.z1 = x\n        # layer 1 output tensor\n        self.z2 = sigmoid(np.dot(self.z1, self.w1) + self.b1)\n        # layer 2 output tensor\n        self.z3 = sigmoid(np.dot(self.z2, self.w2) + self.b2)\n        # output layer tensor\n        self.out = sigmoid(np.dot(self.z3, self.wout) + self.bout)\n        return self.out\n\n    def train(self, x, t):\n        # backpropagation output layer\n        #En = t * np.log(self.out) + (1-t) * np.log(1-self.out)\n        En = (self.out - t) * self.out * (1 - self.out)\n        # get gradients for weight and bias\n        grad_wout = np.dot(self.z3.T, En)\n        grad_bout = np.dot(np.ones([En.shape[0]]), En)\n        # update weight and bias\n        self.wout -= self.lr * grad_wout\n        self.bout -= self.lr * grad_bout\n\n        # backpropagation inter layer\n        # get gradients for weight and bias\n        grad_u2 = np.dot(En, self.wout.T) * self.z3 * (1 - self.z3)\n        grad_w2 = np.dot(self.z2.T, grad_u2)\n        grad_b2 = np.dot(np.ones([grad_u2.shape[0]]), grad_u2)\n        # update weight and bias\n        self.w2 -= self.lr * grad_w2\n        self.b2 -= self.lr * grad_b2\n        \n        # get gradients for weight and bias\n        grad_u1 = np.dot(grad_u2, self.w2.T) * self.z2 * (1 - self.z2)\n        grad_w1 = np.dot(self.z1.T, grad_u1)\n        grad_b1 = np.dot(np.ones([grad_u1.shape[0]]), grad_u1)\n        # update weight and bias\n        self.w1 -= self.lr * grad_w1\n        self.b1 -= self.lr * grad_b1\n\n# sigmoid\ndef sigmoid(x):\n    return 1. / (1. + np.exp(-x))\n\n# train\ndef train_nn(nn, train_x, train_t, iteration_N=5000):\n    for i in range(5000):\n        # feed-forward data\n        nn.forward(train_x)\n        #print(\"ite>>\", i, 'y >>', nn.forward(train_x))\n        # update parameters\n        nn.train(train_x, train_t)\n\n    return nn\n\n\n# test\ndef test_nn(nn, test_x, test_t):\n    for j in range(len(test_x)):\n        x = train_x[j]\n        t = train_t[j]\n        print(\"in:\", x, \"pred:\", nn.forward(x))\n\n\n\n# train data\ntrain_x = np.array([[0,0], [0,1], [1,0], [1,1]], dtype=np.float32)\n\n# train label data\ntrain_t = np.array([[0], [1], [1], [0]], dtype=np.float32)\n\n# prepare neural network\nnn = NN()\n\n# train\nnn = train_nn(nn, train_x, train_t, iteration_N=5000)\n\n# test\ntest_nn(nn, train_x, train_t)"}], "task": "code_generation", "index": 294, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_295", "instruction": "pythonを用いて、imori_1.jpgからランダムに画像を切り抜いて(cropping, クラッピングと呼ぶ)作成した200個の学習データのHOG特徴量を入力として以下に示すニューラルネットワークを学習せよ。\n\n\nimport numpy as np\n\nnp.random.seed(0)\n\nclass NN:\n    def __init__(self, ind=2, w=64, outd=1, lr=0.1):\n        self.w1 = np.random.normal(0, 1, [ind, w])\n        self.b1 = np.random.normal(0, 1, [w])\n        self.wout = np.random.normal(0, 1, [w, outd])\n        self.bout = np.random.normal(0, 1, [outd])\n        self.lr = lr\n\n    def forward(self, x):\n        self.z1 = x\n        self.z2 = sigmoid(np.dot(self.z1, self.w1) + self.b1)\n        self.out = sigmoid(np.dot(self.z2, self.wout) + self.bout)\n        return self.out\n\n    def train(self, x, t):\n        # backpropagation output layer\n        #En = t * np.log(self.out) + (1-t) * np.log(1-self.out)\n        En = (self.out - t) * self.out * (1 - self.out)\n        grad_En = En #np.array([En for _ in range(t.shape[0])])\n        grad_wout = np.dot(self.z2.T, En)\n        grad_bout = np.dot(np.ones([En.shape[0]]), En)\n        self.wout -= self.lr * grad_wout#np.expand_dims(grad_wout, axis=-1)\n        self.bout -= self.lr * grad_bout\n\n        # backpropagation inter layer\n        grad_u1 = np.dot(En, self.wout.T) * self.z2 * (1 - self.z2)\n        grad_w1 = np.dot(self.z1.T, grad_u1)\n        grad_b1 = np.dot(np.ones([grad_u1.shape[0]]), grad_u1)\n        self.w1 -= self.lr * grad_w1\n        self.b1 -= self.lr * grad_b1\n\ndef sigmoid(x):\n    return 1. / (1. + np.exp(-x))\n\ntrain_x = np.array([[0,0], [0,1], [1,0], [1,1]], dtype=np.float32)\ntrain_t = np.array([[0], [1], [1], [0]], dtype=np.float32)\n\nnn = NN(ind=train_x.shape[1])\n\n# train\nfor i in range(1000):\n    nn.forward(train_x)\n    nn.train(train_x, train_t)\n\n# test\nfor j in range(4):\n    x = train_x[j]\n    t = train_t[j]\n    print(\"in:\", x, \"pred:\", nn.forward(x))\n\n\nここでは、学習データに対してAccuracyを計算せよ。ただし、出力(予測確率)が0.5以上で予測ラベルが1、0.5未満で予測ラベルは0としてAccuracyを計算せよ。\n学習のハイパーパラメータは、下記の通り。\n- 学習率 lr= 0.01\n- 学習回数 epch=10000\n- 切り抜いた画像を32x32にリサイズして、HOG特徴量を取得せよ。(HOGは8x8を1セルとする。)", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\n\nnp.random.seed(0)\n\n\n# get HOG\ndef HOG(img):\n    # Grayscale\n    def BGR2GRAY(img):\n        gray = 0.2126 * img[..., 2] + 0.7152 * img[..., 1] + 0.0722 * img[..., 0]\n        return gray\n\n    # Magnitude and gradient\n    def get_gradXY(gray):\n        H, W = gray.shape\n\n        # padding before grad\n        gray = np.pad(gray, (1, 1), 'edge')\n\n        # get grad x\n        gx = gray[1:H+1, 2:] - gray[1:H+1, :W]\n        # get grad y\n        gy = gray[2:, 1:W+1] - gray[:H, 1:W+1]\n        # replace 0 with \n        gx[gx == 0] = 1e-6\n\n        return gx, gy\n\n    # get magnitude and gradient\n    def get_MagGrad(gx, gy):\n        # get gradient maginitude\n        magnitude = np.sqrt(gx ** 2 + gy ** 2)\n\n        # get gradient angle\n        gradient = np.arctan(gy / gx)\n\n        gradient[gradient < 0] = np.pi / 2 + gradient[gradient < 0] + np.pi / 2\n\n        return magnitude, gradient\n\n    # Gradient histogram\n    def quantization(gradient):\n        # prepare quantization table\n        gradient_quantized = np.zeros_like(gradient, dtype=np.int)\n\n        # quantization base\n        d = np.pi / 9\n\n        # quantization\n        for i in range(9):\n            gradient_quantized[np.where((gradient >= d * i) & (gradient <= d * (i + 1)))] = i\n\n        return gradient_quantized\n\n\n    # get gradient histogram\n    def gradient_histogram(gradient_quantized, magnitude, N=8):\n        # get shape\n        H, W = magnitude.shape\n\n        # get cell num\n        cell_N_H = H // N\n        cell_N_W = W // N\n        histogram = np.zeros((cell_N_H, cell_N_W, 9), dtype=np.float32)\n\n        # each pixel\n        for y in range(cell_N_H):\n            for x in range(cell_N_W):\n                for j in range(N):\n                    for i in range(N):\n                        histogram[y, x, gradient_quantized[y * 4 + j, x * 4 + i]] += magnitude[y * 4 + j, x * 4 + i]\n\n        return histogram\n\n\t\t# histogram normalization\n    def normalization(histogram, C=3, epsilon=1):\n        cell_N_H, cell_N_W, _ = histogram.shape\n        ## each histogram\n        for y in range(cell_N_H):\n    \t    for x in range(cell_N_W):\n       \t    #for i in range(9):\n                histogram[y, x] /= np.sqrt(np.sum(histogram[max(y - 1, 0) : min(y + 2, cell_N_H),\n                                                            max(x - 1, 0) : min(x + 2, cell_N_W)] ** 2) + epsilon)\n\n        return histogram\n\n    # 1. BGR -> Gray\n    gray = BGR2GRAY(img)\n\n    # 1. Gray -> Gradient x and y\n    gx, gy = get_gradXY(gray)\n\n    # 2. get gradient magnitude and angle\n    magnitude, gradient = get_MagGrad(gx, gy)\n\n    # 3. Quantization\n    gradient_quantized = quantization(gradient)\n\n    # 4. Gradient histogram\n    histogram = gradient_histogram(gradient_quantized, magnitude)\n    \n    # 5. Histogram normalization\n    histogram = normalization(histogram)\n\n    return histogram\n\n\n# get IoU overlap ratio\ndef iou(a, b):\n\t# get area of a\n    area_a = (a[2] - a[0]) * (a[3] - a[1])\n\t# get area of b\n    area_b = (b[2] - b[0]) * (b[3] - b[1])\n\n\t# get left top x of IoU\n    iou_x1 = np.maximum(a[0], b[0])\n\t# get left top y of IoU\n    iou_y1 = np.maximum(a[1], b[1])\n\t# get right bottom of IoU\n    iou_x2 = np.minimum(a[2], b[2])\n\t# get right bottom of IoU\n    iou_y2 = np.minimum(a[3], b[3])\n\n\t# get width of IoU\n    iou_w = iou_x2 - iou_x1\n\t# get height of IoU\n    iou_h = iou_y2 - iou_y1\n\n\t# get area of IoU\n    area_iou = iou_w * iou_h\n\t# get overlap ratio between IoU and all area\n    iou = area_iou / (area_a + area_b - area_iou)\n\n    return iou\n\n# resize using bi-linear\ndef resize(img, h, w):\n    # get shape\n    _h, _w, _c  = img.shape\n\n    # get resize ratio\n    ah = 1. * h / _h\n    aw = 1. * w / _w\n\n    # get index of each y\n    y = np.arange(h).repeat(w).reshape(w, -1)\n    # get index of each x\n    x = np.tile(np.arange(w), (h, 1))\n\n    # get coordinate toward x and y of resized image\n    y = (y / ah)\n    x = (x / aw)\n\n    # transfer to int\n    ix = np.floor(x).astype(np.int32)\n    iy = np.floor(y).astype(np.int32)\n\n    # clip index\n    ix = np.minimum(ix, _w-2)\n    iy = np.minimum(iy, _h-2)\n\n    # get distance between original image index and resized image index\n    dx = x - ix\n    dy = y - iy\n\n    dx = np.tile(dx, [_c, 1, 1]).transpose(1, 2, 0)\n    dy = np.tile(dy, [_c, 1, 1]).transpose(1, 2, 0)\n    \n    # resize\n    out = (1 - dx) * (1 - dy) * img[iy, ix] + dx * (1 - dy) * img[iy, ix + 1] + (1 - dx) * dy * img[iy + 1, ix] + dx * dy * img[iy + 1, ix + 1]\n    out[out > 255] = 255\n\n    return out\n\n\n# neural network\nclass NN:\n    def __init__(self, ind=2, w=64, w2=64, outd=1, lr=0.1):\n        # layer 1 weight\n        self.w1 = np.random.normal(0, 1, [ind, w])\n        # layer 1 bias\n        self.b1 = np.random.normal(0, 1, [w])\n        # layer 2 weight\n        self.w2 = np.random.normal(0, 1, [w, w2])\n        # layer 2 bias\n        self.b2 = np.random.normal(0, 1, [w2])\n        # output layer weight\n        self.wout = np.random.normal(0, 1, [w2, outd])\n        # output layer bias\n        self.bout = np.random.normal(0, 1, [outd])\n        # learning rate\n        self.lr = lr\n\n    def forward(self, x):\n        # input tensor\n        self.z1 = x\n        # layer 1 output tensor\n        self.z2 = sigmoid(np.dot(self.z1, self.w1) + self.b1)\n        # layer 2 output tensor\n        self.z3 = sigmoid(np.dot(self.z2, self.w2) + self.b2)\n        # output layer tensor\n        self.out = sigmoid(np.dot(self.z3, self.wout) + self.bout)\n        return self.out\n\n    def train(self, x, t):\n        # backpropagation output layer\n        #En = t * np.log(self.out) + (1-t) * np.log(1-self.out)\n        En = (self.out - t) * self.out * (1 - self.out)\n        # get gradients for weight and bias\n        grad_wout = np.dot(self.z3.T, En)\n        grad_bout = np.dot(np.ones([En.shape[0]]), En)\n        # update weight and bias\n        self.wout -= self.lr * grad_wout\n        self.bout -= self.lr * grad_bout\n\n        # backpropagation inter layer\n        # get gradients for weight and bias\n        grad_u2 = np.dot(En, self.wout.T) * self.z3 * (1 - self.z3)\n        grad_w2 = np.dot(self.z2.T, grad_u2)\n        grad_b2 = np.dot(np.ones([grad_u2.shape[0]]), grad_u2)\n        # update weight and bias\n        self.w2 -= self.lr * grad_w2\n        self.b2 -= self.lr * grad_b2\n        \n        # get gradients for weight and bias\n        grad_u1 = np.dot(grad_u2, self.w2.T) * self.z2 * (1 - self.z2)\n        grad_w1 = np.dot(self.z1.T, grad_u1)\n        grad_b1 = np.dot(np.ones([grad_u1.shape[0]]), grad_u1)\n        # update weight and bias\n        self.w1 -= self.lr * grad_w1\n        self.b1 -= self.lr * grad_b1\n\n# sigmoid\ndef sigmoid(x):\n    return 1. / (1. + np.exp(-x))\n\n# train\ndef train_nn(nn, train_x, train_t, iteration_N=10000):\n    # each iteration\n    for i in range(iteration_N):\n        # feed-forward data\n        nn.forward(train_x)\n        # update parameter\n        nn.train(train_x, train_t)\n\n    return nn\n\n# test\ndef test_nn(nn, test_x, test_t, pred_th=0.5):\n    accuracy_N = 0.\n\n    # each data\n    for data, t in zip(test_x, test_t):\n        # get prediction\n        prob = nn.forward(data)\n\n        # count accuracy\n        pred = 1 if prob >= pred_th else 0\n        if t == pred:\n            accuracy_N += 1\n\n    # get accuracy \n    accuracy = accuracy_N / len(db)\n\n    print(\"Accuracy >> {} ({} / {})\".format(accuracy, accuracy_N, len(db)))\n\n\n# crop bounding box and make dataset\ndef make_dataset(img, gt, Crop_N=200, L=60, th=0.5, H_size=32):\n    # get shape\n    H, W, _ = img.shape\n\n    # get HOG feature dimension\n    HOG_feature_N = ((H_size // 8) ** 2) * 9\n\n    # prepare database\n    db = np.zeros([Crop_N, HOG_feature_N + 1])\n\n    # each crop\n    for i in range(Crop_N):\n        # get left top x of crop bounding box\n        x1 = np.random.randint(W - L)\n        # get left top y of crop bounding box\n        y1 = np.random.randint(H - L)\n        # get right bottom x of crop bounding box\n        x2 = x1 + L\n        # get right bottom y of crop bounding box\n        y2 = y1 + L\n\n        # get bounding box\n        crop = np.array((x1, y1, x2, y2))\n\n        _iou = np.zeros((3,))\n        _iou[0] = iou(gt, crop)\n        #_iou[1] = iou(gt2, crop)\n        #_iou[2] = iou(gt3, crop)\n\n        # get label\n        if _iou.max() >= th:\n            cv2.rectangle(img, (x1, y1), (x2, y2), (0,0,255), 1)\n            label = 1\n        else:\n            cv2.rectangle(img, (x1, y1), (x2, y2), (255,0,0), 1)\n            label = 0\n\n        # crop area\n        crop_area = img[y1:y2, x1:x2]\n\n        # resize crop area\n        crop_area = resize(crop_area, H_size, H_size)\n\n        # get HOG feature\n        _hog = HOG(crop_area)\n        \n        # store HOG feature and label\n        db[i, :HOG_feature_N] = _hog.ravel()\n        db[i, -1] = label\n\n    return db\n\n# Read image\nimg = cv2.imread(\"imori.jpg\").astype(np.float32)\n\n# get HOG\nhistogram = HOG(img)\n\n# prepare gt bounding box\ngt = np.array((47, 41, 129, 103), dtype=np.float32)\n\n# get database\ndb = make_dataset(img, gt)\n\n\n# train neural network\n# get input feature dimension\ninput_dim = db.shape[1] - 1\n# prepare train data X\ntrain_x = db[:, :input_dim]\n# prepare train data t\ntrain_t = db[:, -1][..., None]\n\n# prepare neural network\nnn = NN(ind=input_dim, lr=0.01)\n# training\nnn = train_nn(nn, train_x, train_t, iteration_N=10000)\n\n# test\ntest_nn(nn, train_x, train_t)"}], "task": "code_generation", "index": 295, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_296", "instruction": "pythonを用いて、物体検出を行う。\n\n物体検出とは、画像中でどこに何が写っているかを出力するタスクである。\n例えば、画像の[x1, y1, x2, y2]の位置に犬がいるなど。\nこのような物体を囲む矩形のことをBounding-box(バウンディングボックス)と呼ぶ。\n\nここでは簡単な物体検出のアルゴリズムを作成する。\n\n1. 画像の左上からスライディングウィンドウを行う。\n2. 各画像の位置について、注目位置を中心に複数の矩形を用意する。\n3. それぞれの矩形に対応する画像を切り抜いて、特徴抽出(HOG, SIFTなど)を行う。\n4. 識別機(CNN, SVMなど)に掛けて各矩形が物体か否かを判別する。\n\nこれである程度の物体と矩形の座標が得られる。現在は物体検出はディープラーニングによる手法(Faster R-CNN, YOLO, SSDなど)が主流であるが、ディープラーニングが流行る前まではこのようなスライディングウィンドウの手法が主流であった。今回は検出の基礎を学ぶため、スライディングウィンドウを扱う。\n\nここではpythonで処理1-3を実装しなさい。\n\nimori_many.jpgに対してイモリの顔検出を行う。\n条件は以下。\n- 矩形は下記のものを用いる。\n\n# [h, w]\nrecs = np.array(((42, 42), (56, 56), (70, 70)), dtype=np.float32)\n\n- スライドは4ピクセルおきに行う。(1ピクセルでもいいが、計算が多くなって処理が長くなってしまう。)\n- 矩形が画像サイズをはみ出る場合は、はみ出ないように変形する。\n- 矩形部分を切り抜いたら、その部分を32x32にリサイズする。\n- HOG特徴量の取得は8x8を1セルとする。", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\n\nnp.random.seed(0)\n\n# get HOG\ndef HOG(img):\n    # Grayscale\n    def BGR2GRAY(img):\n        gray = 0.2126 * img[..., 2] + 0.7152 * img[..., 1] + 0.0722 * img[..., 0]\n        return gray\n\n    # Magnitude and gradient\n    def get_gradXY(gray):\n        H, W = gray.shape\n\n        # padding before grad\n        gray = np.pad(gray, (1, 1), 'edge')\n\n        # get grad x\n        gx = gray[1:H+1, 2:] - gray[1:H+1, :W]\n        # get grad y\n        gy = gray[2:, 1:W+1] - gray[:H, 1:W+1]\n        # replace 0 with \n        gx[gx == 0] = 1e-6\n\n        return gx, gy\n\n    # get magnitude and gradient\n    def get_MagGrad(gx, gy):\n        # get gradient maginitude\n        magnitude = np.sqrt(gx ** 2 + gy ** 2)\n\n        # get gradient angle\n        gradient = np.arctan(gy / gx)\n\n        gradient[gradient < 0] = np.pi / 2 + gradient[gradient < 0] + np.pi / 2\n\n        return magnitude, gradient\n\n    # Gradient histogram\n    def quantization(gradient):\n        # prepare quantization table\n        gradient_quantized = np.zeros_like(gradient, dtype=np.int)\n\n        # quantization base\n        d = np.pi / 9\n\n        # quantization\n        for i in range(9):\n            gradient_quantized[np.where((gradient >= d * i) & (gradient <= d * (i + 1)))] = i\n\n        return gradient_quantized\n\n\n    # get gradient histogram\n    def gradient_histogram(gradient_quantized, magnitude, N=8):\n        # get shape\n        H, W = magnitude.shape\n\n        # get cell num\n        cell_N_H = H // N\n        cell_N_W = W // N\n        histogram = np.zeros((cell_N_H, cell_N_W, 9), dtype=np.float32)\n\n        # each pixel\n        for y in range(cell_N_H):\n            for x in range(cell_N_W):\n                for j in range(N):\n                    for i in range(N):\n                        histogram[y, x, gradient_quantized[y * 4 + j, x * 4 + i]] += magnitude[y * 4 + j, x * 4 + i]\n\n        return histogram\n\n\t\t# histogram normalization\n    def normalization(histogram, C=3, epsilon=1):\n        cell_N_H, cell_N_W, _ = histogram.shape\n        ## each histogram\n        for y in range(cell_N_H):\n    \t    for x in range(cell_N_W):\n       \t    #for i in range(9):\n                histogram[y, x] /= np.sqrt(np.sum(histogram[max(y - 1, 0) : min(y + 2, cell_N_H),\n                                                            max(x - 1, 0) : min(x + 2, cell_N_W)] ** 2) + epsilon)\n\n        return histogram\n\n    # 1. BGR -> Gray\n    gray = BGR2GRAY(img)\n\n    # 1. Gray -> Gradient x and y\n    gx, gy = get_gradXY(gray)\n\n    # 2. get gradient magnitude and angle\n    magnitude, gradient = get_MagGrad(gx, gy)\n\n    # 3. Quantization\n    gradient_quantized = quantization(gradient)\n\n    # 4. Gradient histogram\n    histogram = gradient_histogram(gradient_quantized, magnitude)\n    \n    # 5. Histogram normalization\n    histogram = normalization(histogram)\n\n    return histogram\n\n\n# get IoU overlap ratio\ndef iou(a, b):\n\t# get area of a\n    area_a = (a[2] - a[0]) * (a[3] - a[1])\n\t# get area of b\n    area_b = (b[2] - b[0]) * (b[3] - b[1])\n\n\t# get left top x of IoU\n    iou_x1 = np.maximum(a[0], b[0])\n\t# get left top y of IoU\n    iou_y1 = np.maximum(a[1], b[1])\n\t# get right bottom of IoU\n    iou_x2 = np.minimum(a[2], b[2])\n\t# get right bottom of IoU\n    iou_y2 = np.minimum(a[3], b[3])\n\n\t# get width of IoU\n    iou_w = iou_x2 - iou_x1\n\t# get height of IoU\n    iou_h = iou_y2 - iou_y1\n\n\t# get area of IoU\n    area_iou = iou_w * iou_h\n\t# get overlap ratio between IoU and all area\n    iou = area_iou / (area_a + area_b - area_iou)\n\n    return iou\n\n# resize using bi-linear\ndef resize(img, h, w):\n    # get shape\n    _h, _w, _c  = img.shape\n\n    # get resize ratio\n    ah = 1. * h / _h\n    aw = 1. * w / _w\n\n    # get index of each y\n    y = np.arange(h).repeat(w).reshape(w, -1)\n    # get index of each x\n    x = np.tile(np.arange(w), (h, 1))\n\n    # get coordinate toward x and y of resized image\n    y = (y / ah)\n    x = (x / aw)\n\n    # transfer to int\n    ix = np.floor(x).astype(np.int32)\n    iy = np.floor(y).astype(np.int32)\n\n    # clip index\n    ix = np.minimum(ix, _w-2)\n    iy = np.minimum(iy, _h-2)\n\n    # get distance between original image index and resized image index\n    dx = x - ix\n    dy = y - iy\n\n    dx = np.tile(dx, [_c, 1, 1]).transpose(1, 2, 0)\n    dy = np.tile(dy, [_c, 1, 1]).transpose(1, 2, 0)\n    \n    # resize\n    out = (1 - dx) * (1 - dy) * img[iy, ix] + dx * (1 - dy) * img[iy, ix + 1] + (1 - dx) * dy * img[iy + 1, ix] + dx * dy * img[iy + 1, ix + 1]\n    out[out > 255] = 255\n\n    return out\n\n# sliding window\ndef sliding_window(img, H_size=32):\n    # get shape\n    H, W, _ = img.shape\n    \n    # base rectangle [h, w]\n    recs = np.array(((42, 42), (56, 56), (70, 70)), dtype=np.float32)\n\n    # sliding window\n    for y in range(0, H, 4):\n        for x in range(0, W, 4):\n            for rec in recs:\n                # get half size of ractangle\n                dh = int(rec[0] // 2)\n                dw = int(rec[1] // 2)\n\n                # get left top x\n                x1 = max(x - dw, 0)\n                # get left top y\n                x2 = min(x + dw, W)\n                # get right bottom x\n                y1 = max(y - dh, 0)\n                # get right bottom y\n                y2 = min(y + dh, H)\n\n                # crop region\n                region = img[max(y - dh, 0) : min(y + dh, H), max(x - dw, 0) : min(x + dw, W)]\n\n                # resize crop region\n                region = resize(region, H_size, H_size)\n\n                # get HOG feature\n                region_hog = HOG(region).ravel()\n\n\n\n# read detect target image\nimg = cv2.imread(\"imori_many.jpg\")\n\nsliding_window(img)"}], "task": "code_generation", "index": 296, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_297", "instruction": "pythonを用いて、imori_many.jpgの物体検出で求めたが各矩形のHOG特徴量を入力として、以下のニューラルネットでイモリの顔か否かを識別せよ。また、スコア(予測確率)が0.7以上の矩形を描画せよ。\n\n\nimport numpy as np\n\nnp.random.seed(0)\n\nclass NN:\n    def __init__(self, ind=2, w=64, outd=1, lr=0.1):\n        self.w1 = np.random.normal(0, 1, [ind, w])\n        self.b1 = np.random.normal(0, 1, [w])\n        self.wout = np.random.normal(0, 1, [w, outd])\n        self.bout = np.random.normal(0, 1, [outd])\n        self.lr = lr\n\n    def forward(self, x):\n        self.z1 = x\n        self.z2 = sigmoid(np.dot(self.z1, self.w1) + self.b1)\n        self.out = sigmoid(np.dot(self.z2, self.wout) + self.bout)\n        return self.out\n\n    def train(self, x, t):\n        # backpropagation output layer\n        #En = t * np.log(self.out) + (1-t) * np.log(1-self.out)\n        En = (self.out - t) * self.out * (1 - self.out)\n        grad_En = En #np.array([En for _ in range(t.shape[0])])\n        grad_wout = np.dot(self.z2.T, En)\n        grad_bout = np.dot(np.ones([En.shape[0]]), En)\n        self.wout -= self.lr * grad_wout#np.expand_dims(grad_wout, axis=-1)\n        self.bout -= self.lr * grad_bout\n\n        # backpropagation inter layer\n        grad_u1 = np.dot(En, self.wout.T) * self.z2 * (1 - self.z2)\n        grad_w1 = np.dot(self.z1.T, grad_u1)\n        grad_b1 = np.dot(np.ones([grad_u1.shape[0]]), grad_u1)\n        self.w1 -= self.lr * grad_w1\n        self.b1 -= self.lr * grad_b1\n\ndef sigmoid(x):\n    return 1. / (1. + np.exp(-x))\n\ntrain_x = np.array([[0,0], [0,1], [1,0], [1,1]], dtype=np.float32)\ntrain_t = np.array([[0], [1], [1], [0]], dtype=np.float32)\n\nnn = NN(ind=train_x.shape[1])\n\n# train\nfor i in range(1000):\n    nn.forward(train_x)\n    nn.train(train_x, train_t)\n\n# test\nfor j in range(4):\n    x = train_x[j]\n    t = train_t[j]\n    print(\"in:\", x, \"pred:\", nn.forward(x))", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\n\nnp.random.seed(0)\n\n# read image\nimg = cv2.imread(\"imori_1.jpg\")\nH, W, C = img.shape\n\n# Grayscale\ngray = 0.2126 * img[..., 2] + 0.7152 * img[..., 1] + 0.0722 * img[..., 0]\n\ngt = np.array((47, 41, 129, 103), dtype=np.float32)\n\ncv2.rectangle(img, (gt[0], gt[1]), (gt[2], gt[3]), (0,255,255), 1)\n\ndef iou(a, b):\n    area_a = (a[2] - a[0]) * (a[3] - a[1])\n    area_b = (b[2] - b[0]) * (b[3] - b[1])\n    iou_x1 = np.maximum(a[0], b[0])\n    iou_y1 = np.maximum(a[1], b[1])\n    iou_x2 = np.minimum(a[2], b[2])\n    iou_y2 = np.minimum(a[3], b[3])\n    iou_w = max(iou_x2 - iou_x1, 0)\n    iou_h = max(iou_y2 - iou_y1, 0)\n    area_iou = iou_w * iou_h\n    iou = area_iou / (area_a + area_b - area_iou)\n    return iou\n\n\ndef hog(gray):\n    h, w = gray.shape\n    # Magnitude and gradient\n    gray = np.pad(gray, (1, 1), 'edge')\n\n    gx = gray[1:h+1, 2:] - gray[1:h+1, :w]\n    gy = gray[2:, 1:w+1] - gray[:h, 1:w+1]\n    gx[gx == 0] = 0.000001\n\n    mag = np.sqrt(gx ** 2 + gy ** 2)\n    gra = np.arctan(gy / gx)\n    gra[gra<0] = np.pi / 2 + gra[gra < 0] + np.pi / 2\n\n    # Gradient histogram\n    gra_n = np.zeros_like(gra, dtype=np.int)\n\n    d = np.pi / 9\n    for i in range(9):\n        gra_n[np.where((gra >= d * i) & (gra <= d * (i+1)))] = i\n\n    N = 8\n    HH = h // N\n    HW = w // N\n    Hist = np.zeros((HH, HW, 9), dtype=np.float32)\n    for y in range(HH):\n        for x in range(HW):\n            for j in range(N):\n                for i in range(N):\n                    Hist[y, x, gra_n[y*4+j, x*4+i]] += mag[y*4+j, x*4+i]\n\n    ## Normalization\n    C = 3\n    eps = 1\n    for y in range(HH):\n        for x in range(HW):\n            #for i in range(9):\n            Hist[y, x] /= np.sqrt(np.sum(Hist[max(y-1,0):min(y+2, HH), max(x-1,0):min(x+2, HW)] ** 2) + eps)\n\n    return Hist\n\ndef resize(img, h, w):\n    _h, _w  = img.shape\n    ah = 1. * h / _h\n    aw = 1. * w / _w\n    y = np.arange(h).repeat(w).reshape(w, -1)\n    x = np.tile(np.arange(w), (h, 1))\n    y = (y / ah)\n    x = (x / aw)\n\n    ix = np.floor(x).astype(np.int32)\n    iy = np.floor(y).astype(np.int32)\n    ix = np.minimum(ix, _w-2)\n    iy = np.minimum(iy, _h-2)\n\n    dx = x - ix\n    dy = y - iy\n\n    out = (1-dx) * (1-dy) * img[iy, ix] + dx * (1 - dy) * img[iy, ix+1] + (1 - dx) * dy * img[iy+1, ix] + dx * dy * img[iy+1, ix+1]\n    out[out>255] = 255\n\n    return out\n\n\nclass NN:\n    def __init__(self, ind=2, w=64, w2=64, outd=1, lr=0.1):\n        self.w1 = np.random.normal(0, 1, [ind, w])\n        self.b1 = np.random.normal(0, 1, [w])\n        self.w2 = np.random.normal(0, 1, [w, w2])\n        self.b2 = np.random.normal(0, 1, [w2])\n        self.wout = np.random.normal(0, 1, [w2, outd])\n        self.bout = np.random.normal(0, 1, [outd])\n        self.lr = lr\n\n    def forward(self, x):\n        self.z1 = x\n        self.z2 = sigmoid(np.dot(self.z1, self.w1) + self.b1)\n        self.z3 = sigmoid(np.dot(self.z2, self.w2) + self.b2)\n        self.out = sigmoid(np.dot(self.z3, self.wout) + self.bout)\n        return self.out\n\n    def train(self, x, t):\n        # backpropagation output layer\n        #En = t * np.log(self.out) + (1-t) * np.log(1-self.out)\n        En = (self.out - t) * self.out * (1 - self.out)\n        grad_wout = np.dot(self.z3.T, En)\n        grad_bout = np.dot(np.ones([En.shape[0]]), En)\n        self.wout -= self.lr * grad_wout\n        self.bout -= self.lr * grad_bout\n\n        # backpropagation inter layer\n        grad_u2 = np.dot(En, self.wout.T) * self.z3 * (1 - self.z3)\n        grad_w2 = np.dot(self.z2.T, grad_u2)\n        grad_b2 = np.dot(np.ones([grad_u2.shape[0]]), grad_u2)\n        self.w2 -= self.lr * grad_w2\n        self.b2 -= self.lr * grad_b2\n\n        grad_u1 = np.dot(grad_u2, self.w2.T) * self.z2 * (1 - self.z2)\n        grad_w1 = np.dot(self.z1.T, grad_u1)\n        grad_b1 = np.dot(np.ones([grad_u1.shape[0]]), grad_u1)\n        self.w1 -= self.lr * grad_w1\n        self.b1 -= self.lr * grad_b1\n\ndef sigmoid(x):\n    return 1. / (1. + np.exp(-x))\n\n# crop and create database\n\nCrop_num = 200\nL = 60\nH_size = 32\nF_n = ((H_size // 8) ** 2) * 9\n\ndb = np.zeros((Crop_num, F_n+1))\n\nfor i in range(Crop_num):\n    x1 = np.random.randint(W-L)\n    y1 = np.random.randint(H-L)\n    x2 = x1 + L\n    y2 = y1 + L\n    crop = np.array((x1, y1, x2, y2))\n\n    _iou = iou(gt, crop)\n\n    if _iou >= 0.5:\n        cv2.rectangle(img, (x1, y1), (x2, y2), (0,0,255), 1)\n        label = 1\n    else:\n        cv2.rectangle(img, (x1, y1), (x2, y2), (255,0,0), 1)\n        label = 0\n\n    crop_area = gray[y1:y2, x1:x2]\n    crop_area = resize(crop_area, H_size, H_size)\n    _hog = hog(crop_area)\n\n    db[i, :F_n] = _hog.ravel()\n    db[i, -1] = label\n\n## train neural network\nnn = NN(ind=F_n, lr=0.01)\nfor i in range(10000):\n    nn.forward(db[:, :F_n])\n    nn.train(db[:, :F_n], db[:, -1][..., None])\n\n\n# read detect target image\nimg2 = cv2.imread(\"imori_many.jpg\")\nH2, W2, C2 = img2.shape\n\n# Grayscale\ngray2 = 0.2126 * img2[..., 2] + 0.7152 * img2[..., 1] + 0.0722 * img2[..., 0]\n\n# [h, w]\nrecs = np.array(((42, 42), (56, 56), (70, 70)), dtype=np.float32)\n\ndetects = np.ndarray((0, 5), dtype=np.float32)\n\n# sliding window\nfor y in range(0, H2, 4):\n    for x in range(0, W2, 4):\n        for rec in recs:\n            dh = int(rec[0] // 2)\n            dw = int(rec[1] // 2)\n            x1 = max(x-dw, 0)\n            x2 = min(x+dw, W2)\n            y1 = max(y-dh, 0)\n            y2 = min(y+dh, H2)\n            region = gray2[max(y-dh,0):min(y+dh,H2), max(x-dw,0):min(x+dw,W2)]\n            region = resize(region, H_size, H_size)\n            region_hog = hog(region).ravel()\n\n            score = nn.forward(region_hog)\n            if score >= 0.7:\n                cv2.rectangle(img2, (x1, y1), (x2, y2), (0,0,255), 1)\n                detects = np.vstack((detects, np.array((x1, y1, x2, y2, score))))\n\nprint(detects)\n\ncv2.imwrite(\"out.jpg\", img2)\ncv2.imshow(\"result\", img2)\ncv2.waitKey(0)"}], "task": "code_generation", "index": 297, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_298", "instruction": "ある物体検出にてあらかたの検出はできたが、このままではBounding-boxの数が多すぎて、ここから何かしらの処理に繋げるには不便である。\nそこで、NMS: Non-maximum suppressionという手法を用いて矩形の数を減らしなさい。\n\nNMSとはスコアの高いBounding-boxのみを残す手法であり、アルゴリズムは以下の通り。\n\n1. Boundinb-boxの集合Bをスコアが高い順にソートする。\n2. スコアが最大のものをb0とする。\n3. b0と他のBounding-boxのIoUを計算する。IoUが閾値t以上のBounding-boxをBから削除する。B0は出力する集合Rに加え、Bから削除する。\n4. 2-3をBがなくなるまで行う。\n5. Rを出力する。\n\n以下のニューラルネットにNMS(閾値t=0.25)を組み込み、出力を描画せよ。", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\n\nnp.random.seed(0)\n\n# read image\nimg = cv2.imread(\"imori_1.jpg\")\nH, W, C = img.shape\n\n# Grayscale\ngray = 0.2126 * img[..., 2] + 0.7152 * img[..., 1] + 0.0722 * img[..., 0]\n\ngt = np.array((47, 41, 129, 103), dtype=np.float32)\n\ncv2.rectangle(img, (gt[0], gt[1]), (gt[2], gt[3]), (0,255,255), 1)\n\ndef iou(a, b):\n    area_a = (a[2] - a[0]) * (a[3] - a[1])\n    area_b = (b[2] - b[0]) * (b[3] - b[1])\n    iou_x1 = np.maximum(a[0], b[0])\n    iou_y1 = np.maximum(a[1], b[1])\n    iou_x2 = np.minimum(a[2], b[2])\n    iou_y2 = np.minimum(a[3], b[3])\n    iou_w = max(iou_x2 - iou_x1, 0)\n    iou_h = max(iou_y2 - iou_y1, 0)\n    area_iou = iou_w * iou_h\n    iou = area_iou / (area_a + area_b - area_iou)\n    return iou\n\n\ndef hog(gray):\n    h, w = gray.shape\n    # Magnitude and gradient\n    gray = np.pad(gray, (1, 1), 'edge')\n\n    gx = gray[1:h+1, 2:] - gray[1:h+1, :w]\n    gy = gray[2:, 1:w+1] - gray[:h, 1:w+1]\n    gx[gx == 0] = 0.000001\n\n    mag = np.sqrt(gx ** 2 + gy ** 2)\n    gra = np.arctan(gy / gx)\n    gra[gra<0] = np.pi / 2 + gra[gra < 0] + np.pi / 2\n\n    # Gradient histogram\n    gra_n = np.zeros_like(gra, dtype=np.int)\n\n    d = np.pi / 9\n    for i in range(9):\n        gra_n[np.where((gra >= d * i) & (gra <= d * (i+1)))] = i\n\n    N = 8\n    HH = h // N\n    HW = w // N\n    Hist = np.zeros((HH, HW, 9), dtype=np.float32)\n    for y in range(HH):\n        for x in range(HW):\n            for j in range(N):\n                for i in range(N):\n                    Hist[y, x, gra_n[y*4+j, x*4+i]] += mag[y*4+j, x*4+i]\n                \n    ## Normalization\n    C = 3\n    eps = 1\n    for y in range(HH):\n        for x in range(HW):\n            #for i in range(9):\n            Hist[y, x] /= np.sqrt(np.sum(Hist[max(y-1,0):min(y+2, HH), max(x-1,0):min(x+2, HW)] ** 2) + eps)\n\n    return Hist\n\ndef resize(img, h, w):\n    _h, _w  = img.shape\n    ah = 1. * h / _h\n    aw = 1. * w / _w\n    y = np.arange(h).repeat(w).reshape(w, -1)\n    x = np.tile(np.arange(w), (h, 1))\n    y = (y / ah)\n    x = (x / aw)\n\n    ix = np.floor(x).astype(np.int32)\n    iy = np.floor(y).astype(np.int32)\n    ix = np.minimum(ix, _w-2)\n    iy = np.minimum(iy, _h-2)\n\n    dx = x - ix\n    dy = y - iy\n    \n    out = (1-dx) * (1-dy) * img[iy, ix] + dx * (1 - dy) * img[iy, ix+1] + (1 - dx) * dy * img[iy+1, ix] + dx * dy * img[iy+1, ix+1]\n    out[out>255] = 255\n\n    return out\n\n\n\n\n# crop and create database\n\nCrop_num = 200\nL = 60\nH_size = 32\nF_n = ((H_size // 8) ** 2) * 9\n\ndb = np.zeros((Crop_num, F_n+1))\n\nfor i in range(Crop_num):\n    x1 = np.random.randint(W-L)\n    y1 = np.random.randint(H-L)\n    x2 = x1 + L\n    y2 = y1 + L\n    crop = np.array((x1, y1, x2, y2))\n\n    _iou = iou(gt, crop)\n\n    if _iou >= 0.5:\n        cv2.rectangle(img, (x1, y1), (x2, y2), (0,0,255), 1)\n        label = 1\n    else:\n        cv2.rectangle(img, (x1, y1), (x2, y2), (255,0,0), 1)\n        label = 0\n\n    crop_area = gray[y1:y2, x1:x2]\n    crop_area = resize(crop_area, H_size, H_size)\n    _hog = hog(crop_area)\n    \n    db[i, :F_n] = _hog.ravel()\n    db[i, -1] = label\n\n\nclass NN:\n    def __init__(self, ind=2, w=64, w2=64, outd=1, lr=0.1):\n        self.w1 = np.random.normal(0, 1, [ind, w])\n        self.b1 = np.random.normal(0, 1, [w])\n        self.w2 = np.random.normal(0, 1, [w, w2])\n        self.b2 = np.random.normal(0, 1, [w2])\n        self.wout = np.random.normal(0, 1, [w2, outd])\n        self.bout = np.random.normal(0, 1, [outd])\n        self.lr = lr\n\n    def forward(self, x):\n        self.z1 = x\n        self.z2 = sigmoid(np.dot(self.z1, self.w1) + self.b1)\n        self.z3 = sigmoid(np.dot(self.z2, self.w2) + self.b2)\n        self.out = sigmoid(np.dot(self.z3, self.wout) + self.bout)\n        return self.out\n\n    def train(self, x, t):\n        # backpropagation output layer\n        #En = t * np.log(self.out) + (1-t) * np.log(1-self.out)\n        En = (self.out - t) * self.out * (1 - self.out)\n        grad_wout = np.dot(self.z3.T, En)\n        grad_bout = np.dot(np.ones([En.shape[0]]), En)\n        self.wout -= self.lr * grad_wout\n        self.bout -= self.lr * grad_bout\n\n        # backpropagation inter layer\n        grad_u2 = np.dot(En, self.wout.T) * self.z3 * (1 - self.z3)\n        grad_w2 = np.dot(self.z2.T, grad_u2)\n        grad_b2 = np.dot(np.ones([grad_u2.shape[0]]), grad_u2)\n        self.w2 -= self.lr * grad_w2\n        self.b2 -= self.lr * grad_b2\n        \n        grad_u1 = np.dot(grad_u2, self.w2.T) * self.z2 * (1 - self.z2)\n        grad_w1 = np.dot(self.z1.T, grad_u1)\n        grad_b1 = np.dot(np.ones([grad_u1.shape[0]]), grad_u1)\n        self.w1 -= self.lr * grad_w1\n        self.b1 -= self.lr * grad_b1\n\ndef sigmoid(x):\n    return 1. / (1. + np.exp(-x))\n    \n\n## training neural network\nnn = NN(ind=F_n, lr=0.01)\nfor i in range(10000):\n    nn.forward(db[:, :F_n])\n    nn.train(db[:, :F_n], db[:, -1][..., None])\n\n\n# read detect target image\nimg2 = cv2.imread(\"imori_many.jpg\")\nH2, W2, C2 = img2.shape\n\n# Grayscale\ngray2 = 0.2126 * img2[..., 2] + 0.7152 * img2[..., 1] + 0.0722 * img2[..., 0]\n\n# [h, w]\nrecs = np.array(((42, 42), (56, 56), (70, 70)), dtype=np.float32)\n\ndetects = np.ndarray((0, 5), dtype=np.float32)\n\n# sliding window\nfor y in range(0, H2, 4):\n    for x in range(0, W2, 4):\n        for rec in recs:\n            dh = int(rec[0] // 2)\n            dw = int(rec[1] // 2)\n            x1 = max(x-dw, 0)\n            x2 = min(x+dw, W2)\n            y1 = max(y-dh, 0)\n            y2 = min(y+dh, H2)\n            region = gray2[max(y-dh,0):min(y+dh,H2), max(x-dw,0):min(x+dw,W2)]\n            region = resize(region, H_size, H_size)\n            region_hog = hog(region).ravel()\n\n            score = nn.forward(region_hog)\n            if score >= 0.7:\n                #cv2.rectangle(img2, (x1, y1), (x2, y2), (0,0,255), 1)\n                detects = np.vstack((detects, np.array((x1, y1, x2, y2, score))))\n\n\n# Non-maximum suppression\ndef nms(_bboxes, iou_th=0.5, select_num=None, prob_th=None):\n    #\n    # Non Maximum Suppression\n    #\n    # Argument\n    #  bboxes(Nx5) ... [bbox-num, 5(leftTopX,leftTopY,w,h, score)]\n    #  iou_th([float]) ... threshold for iou between bboxes.\n    #  select_num([int]) ... max number for choice bboxes. If None, this is unvalid.\n    #  prob_th([float]) ... probability threshold to choice. If None, this is unvalid.\n    # Return\n    #  inds ... choced indices for bboxes\n    #\n\n    bboxes = _bboxes.copy()\n    \n    bboxes[:, 2] = bboxes[:, 2] - bboxes[:, 0]\n    bboxes[:, 3] = bboxes[:, 3] - bboxes[:, 1]\n    \n    # Sort by bbox's score. High -> Low\n    sort_inds = np.argsort(bboxes[:, -1])[::-1]\n\n    processed_bbox_ind = []\n    return_inds = []\n\n    unselected_inds = sort_inds.copy()\n    \n    while len(unselected_inds) > 0:\n        process_bboxes = bboxes[unselected_inds]\n        argmax_score_ind = np.argmax(process_bboxes[::, -1])\n        max_score_ind = unselected_inds[argmax_score_ind]\n        return_inds += [max_score_ind]\n        unselected_inds = np.delete(unselected_inds, argmax_score_ind)\n\n        base_bbox = bboxes[max_score_ind]\n        compare_bboxes = bboxes[unselected_inds]\n        \n        base_x1 = base_bbox[0]\n        base_y1 = base_bbox[1]\n        base_x2 = base_bbox[2] + base_x1\n        base_y2 = base_bbox[3] + base_y1\n        base_w = np.maximum(base_bbox[2], 0)\n        base_h = np.maximum(base_bbox[3], 0)\n        base_area = base_w * base_h\n\n        # compute iou-area between base bbox and other bboxes\n        iou_x1 = np.maximum(base_x1, compare_bboxes[:, 0])\n        iou_y1 = np.maximum(base_y1, compare_bboxes[:, 1])\n        iou_x2 = np.minimum(base_x2, compare_bboxes[:, 2] + compare_bboxes[:, 0])\n        iou_y2 = np.minimum(base_y2, compare_bboxes[:, 3] + compare_bboxes[:, 1])\n        iou_w = np.maximum(iou_x2 - iou_x1, 0)\n        iou_h = np.maximum(iou_y2 - iou_y1, 0)\n        iou_area = iou_w * iou_h\n\n        compare_w = np.maximum(compare_bboxes[:, 2], 0)\n        compare_h = np.maximum(compare_bboxes[:, 3], 0)\n        compare_area = compare_w * compare_h\n\n        # bbox's index which iou ratio over threshold is excluded\n        all_area = compare_area + base_area - iou_area\n        iou_ratio = np.zeros((len(unselected_inds)))\n        iou_ratio[all_area < 0.9] = 0.\n        _ind = all_area >= 0.9\n        iou_ratio[_ind] = iou_area[_ind] / all_area[_ind]\n        \n        unselected_inds = np.delete(unselected_inds, np.where(iou_ratio >= iou_th)[0])\n\n    if prob_th is not None:\n        preds = bboxes[return_inds][:, -1]\n        return_inds = np.array(return_inds)[np.where(preds >= prob_th)[0]].tolist()\n        \n    # pick bbox's index by defined number with higher score\n    if select_num is not None:\n        return_inds = return_inds[:select_num]\n\n    return return_inds\n\n\ndetects = detects[nms(detects, iou_th=0.25)]\n\nfor d in detects:\n    v = list(map(int, d[:4]))\n    cv2.rectangle(img2, (v[0], v[1]), (v[2], v[3]), (0,0,255), 1)\n    cv2.putText(img2, \"{:.2f}\".format(d[-1]), (v[0], v[1]+9),\n                cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,0,255), 1)\n\ncv2.imwrite(\"out.jpg\", img2)\ncv2.imshow(\"result\", img2)\ncv2.waitKey(0)"}], "task": "code_generation", "index": 298, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_299", "instruction": "pythonを用いて、物体検出の評価指標を実装しなさい。\n\n検出はBounding-boxとそのクラスの２つが一致していないと、精度の評価ができない。\n検出の評価指標には、Recall, Precision, F-score, mAPなどが存在する。\n\n### Recall ... 正解の矩形がどれだけ検出できたか。正解をどれだけ網羅できたかを示す。[0,1]の範囲を取り、1が最高。\n\n\nG' ... Ground-truthの中で検出のどれかとIoUが閾値t以上となったGround-truthの数。\nG ... Ground-truthの矩形の数。\nRecall = G' / G\n\n### Precision ... 検出がどれだけ正確に行われたかを示す。[0,1]の範囲を取り、1が最高。\n\n\nD' ... 検出の中で、Ground-truthのどれかとIoUが閾値t以上となった検出の数。\nD ... 検出の数。\nPrecision = D' / D\n\n\n### F-score ... RecallとPrecisonの調和平均。　２つのバランスを示すもので、[0,1]の範囲を取り、1が最高。\n\n\nF-score = 2 * Recall * Precision / (Recall + Precision)\n\n\n文字を検出する文字検出はRecall, Precision, F-scoreで精度を測ることが多い。\n\n### mAP ... Mean Average Precision。物体を検出する物体検出では、mAPで測ることが多い。mAPの計算方法は少し複雑である。\n\n1. 各検出に関してGround-truthとのIoUが閾値t以上かどうかを判断して、表を作成する。\n\n   Detect | judge\n------------------\n  detect1 |   1   (1はGround-truthとのIoU>=tとなったもの)\n  detect2 |   0   (0はGround-truthとのIoU<tとなったもの)\n  detect3 |   1\n\n2. mAP = 0として、上から順に見て、judgeが1の時は、見ているものの上すべてに関して、Precisionを計算し、mAPに加算する。\n3. 上から順に2を行い、全て行ったら、加算回数でmAPを割る。\n\n以上でmAPが求まる。上の例でいうと、\n1. detect1 が1なので、Precisionを求める。Precision = 1/1 = 1なので、mAP = 1\n2. detect2 が0なので、無視。\n3. detect3 が1なので、Precisionを求める。Precision = 2/3 = 0.67 なので、 mAP = 1 + 0.67 = 1.67\n4. mAPに加算したのは計2回なので、mAP = 1.67 / 2 = 0.835\nとなる。\n\n\nここでは、閾値t=0.5として、Recall, Precision, F-score, mAPを算出せよ。\nGround-truthは次とする。\n\n# [x1, y1, x2, y2]\nGT = np.array(((27, 48, 95, 110), (101, 75, 171, 138)), dtype=np.float32)\n\n\nここでは、GTとのIoUが0.5以上の検出を赤、それ以外を青にして描画せよ。", "instances": [{"input": "", "output": "import cv2\nimport numpy as np\n\nnp.random.seed(0)\n\n# read image\nimg = cv2.imread(\"imori_1.jpg\")\nH, W, C = img.shape\n\n# Grayscale\ngray = 0.2126 * img[..., 2] + 0.7152 * img[..., 1] + 0.0722 * img[..., 0]\n\ngt = np.array((47, 41, 129, 103), dtype=np.float32)\n\ncv2.rectangle(img, (gt[0], gt[1]), (gt[2], gt[3]), (0,255,255), 1)\n\ndef iou(a, b):\n    area_a = (a[2] - a[0]) * (a[3] - a[1])\n    area_b = (b[2] - b[0]) * (b[3] - b[1])\n    iou_x1 = np.maximum(a[0], b[0])\n    iou_y1 = np.maximum(a[1], b[1])\n    iou_x2 = np.minimum(a[2], b[2])\n    iou_y2 = np.minimum(a[3], b[3])\n    iou_w = max(iou_x2 - iou_x1, 0)\n    iou_h = max(iou_y2 - iou_y1, 0)\n    area_iou = iou_w * iou_h\n    iou = area_iou / (area_a + area_b - area_iou)\n    return iou\n\n\ndef hog(gray):\n    h, w = gray.shape\n    # Magnitude and gradient\n    gray = np.pad(gray, (1, 1), 'edge')\n\n    gx = gray[1:h+1, 2:] - gray[1:h+1, :w]\n    gy = gray[2:, 1:w+1] - gray[:h, 1:w+1]\n    gx[gx == 0] = 0.000001\n\n    mag = np.sqrt(gx ** 2 + gy ** 2)\n    gra = np.arctan(gy / gx)\n    gra[gra<0] = np.pi / 2 + gra[gra < 0] + np.pi / 2\n\n    # Gradient histogram\n    gra_n = np.zeros_like(gra, dtype=np.int)\n\n    d = np.pi / 9\n    for i in range(9):\n        gra_n[np.where((gra >= d * i) & (gra <= d * (i+1)))] = i\n\n    N = 8\n    HH = h // N\n    HW = w // N\n    Hist = np.zeros((HH, HW, 9), dtype=np.float32)\n    for y in range(HH):\n        for x in range(HW):\n            for j in range(N):\n                for i in range(N):\n                    Hist[y, x, gra_n[y*4+j, x*4+i]] += mag[y*4+j, x*4+i]\n                \n    ## Normalization\n    C = 3\n    eps = 1\n    for y in range(HH):\n        for x in range(HW):\n            #for i in range(9):\n            Hist[y, x] /= np.sqrt(np.sum(Hist[max(y-1,0):min(y+2, HH), max(x-1,0):min(x+2, HW)] ** 2) + eps)\n\n    return Hist\n\ndef resize(img, h, w):\n    _h, _w  = img.shape\n    ah = 1. * h / _h\n    aw = 1. * w / _w\n    y = np.arange(h).repeat(w).reshape(w, -1)\n    x = np.tile(np.arange(w), (h, 1))\n    y = (y / ah)\n    x = (x / aw)\n\n    ix = np.floor(x).astype(np.int32)\n    iy = np.floor(y).astype(np.int32)\n    ix = np.minimum(ix, _w-2)\n    iy = np.minimum(iy, _h-2)\n\n    dx = x - ix\n    dy = y - iy\n    \n    out = (1-dx) * (1-dy) * img[iy, ix] + dx * (1 - dy) * img[iy, ix+1] + (1 - dx) * dy * img[iy+1, ix] + dx * dy * img[iy+1, ix+1]\n    out[out>255] = 255\n\n    return out\n\n\nclass NN:\n    def __init__(self, ind=2, w=64, w2=64, outd=1, lr=0.1):\n        self.w2 = np.random.randn(ind, w)\n        self.b2 = np.random.randn(w)\n        self.w3 = np.random.randn(w, w2)\n        self.b3 = np.random.randn(w2)\n        self.wout = np.random.randn(w2, outd)\n        self.bout = np.random.randn(outd)\n        self.lr = lr\n\n    def forward(self, x):\n        self.z1 = x\n        self.z2 = self.sigmoid(np.dot(self.z1, self.w2) + self.b2)\n        self.z3 = self.sigmoid(np.dot(self.z2, self.w3) + self.b3)\n        self.out = self.sigmoid(np.dot(self.z3, self.wout) + self.bout)\n        return self.out\n\n    def train(self, x, t):\n        # backpropagation output layer\n        out_d = 2*(self.out - t) * self.out * (1 - self.out)\n        out_dW = np.dot(self.z3.T, out_d)\n        out_dB = np.dot(np.ones([1, out_d.shape[0]]), out_d)\n        self.wout -= self.lr * out_dW\n        self.bout -= self.lr * out_dB[0]\n\n        w3_d = np.dot(out_d, self.wout.T) * self.z3 * (1 - self.z3)\n        w3_dW = np.dot(self.z2.T, w3_d)\n        w3_dB = np.dot(np.ones([1, w3_d.shape[0]]), w3_d)\n        self.w3 -= self.lr * w3_dW\n        self.b3 -= self.lr * w3_dB[0]\n        \n        # backpropagation inter layer\n        w2_d = np.dot(w3_d, self.w3.T) * self.z2 * (1 - self.z2)\n        w2_dW = np.dot(self.z1.T, w2_d)\n        w2_dB = np.dot(np.ones([1, w2_d.shape[0]]), w2_d)\n        self.w2 -= self.lr * w2_dW\n        self.b2 -= self.lr * w2_dB[0]\n\n    def sigmoid(self, x):\n        return 1. / (1. + np.exp(-x))\n\n# crop and create database\n\nCrop_num = 200\nL = 60\nH_size = 32\nF_n = ((H_size // 8) ** 2) * 9\n\ndb = np.zeros((Crop_num, F_n+1))\n\nfor i in range(Crop_num):\n    x1 = np.random.randint(W-L)\n    y1 = np.random.randint(H-L)\n    x2 = x1 + L\n    y2 = y1 + L\n    crop = np.array((x1, y1, x2, y2))\n\n    _iou = iou(gt, crop)\n\n    if _iou >= 0.5:\n        cv2.rectangle(img, (x1, y1), (x2, y2), (0,0,255), 1)\n        label = 1\n    else:\n        cv2.rectangle(img, (x1, y1), (x2, y2), (255,0,0), 1)\n        label = 0\n\n    crop_area = gray[y1:y2, x1:x2]\n    crop_area = resize(crop_area, H_size, H_size)\n    _hog = hog(crop_area)\n    \n    db[i, :F_n] = _hog.ravel()\n    db[i, -1] = label\n\n\n## training neural network\nnn = NN(ind=F_n, lr=0.01)\nfor i in range(10000):\n    nn.forward(db[:, :F_n])\n    nn.train(db[:, :F_n], db[:, -1][..., None])\n\n\n# read detect target image\nimg2 = cv2.imread(\"imori_many.jpg\")\nH2, W2, C2 = img2.shape\n\n# Grayscale\ngray2 = 0.2126 * img2[..., 2] + 0.7152 * img2[..., 1] + 0.0722 * img2[..., 0]\n\n# [h, w]\nrecs = np.array(((42, 42), (56, 56), (70, 70)), dtype=np.float32)\n\ndetects = np.ndarray((0, 5), dtype=np.float32)\n\n# sliding window\nfor y in range(0, H2, 4):\n    for x in range(0, W2, 4):\n        for rec in recs:\n            dh = int(rec[0] // 2)\n            dw = int(rec[1] // 2)\n            x1 = max(x-dw, 0)\n            x2 = min(x+dw, W2)\n            y1 = max(y-dh, 0)\n            y2 = min(y+dh, H2)\n            region = gray2[max(y-dh,0):min(y+dh,H2), max(x-dw,0):min(x+dw,W2)]\n            region = resize(region, H_size, H_size)\n            region_hog = hog(region).ravel()\n\n            score = nn.forward(region_hog)\n            if score >= 0.7:\n                #cv2.rectangle(img2, (x1, y1), (x2, y2), (0,0,255), 1)\n                detects = np.vstack((detects, np.array((x1, y1, x2, y2, score))))\n\n\n# Non-maximum suppression\ndef nms(_bboxes, iou_th=0.5, select_num=None, prob_th=None):\n    #\n    # Non Maximum Suppression\n    #\n    # Argument\n    #  bboxes(Nx5) ... [bbox-num, 5(leftTopX,leftTopY,w,h, score)]\n    #  iou_th([float]) ... threshold for iou between bboxes.\n    #  select_num([int]) ... max number for choice bboxes. If None, this is unvalid.\n    #  prob_th([float]) ... probability threshold to choice. If None, this is unvalid.\n    # Return\n    #  inds ... choced indices for bboxes\n    #\n\n    bboxes = _bboxes.copy()\n    \n    bboxes[:, 2] = bboxes[:, 2] - bboxes[:, 0]\n    bboxes[:, 3] = bboxes[:, 3] - bboxes[:, 1]\n    \n    # Sort by bbox's score. High -> Low\n    sort_inds = np.argsort(bboxes[:, -1])[::-1]\n\n    processed_bbox_ind = []\n    return_inds = []\n\n    unselected_inds = sort_inds.copy()\n    \n    while len(unselected_inds) > 0:\n        process_bboxes = bboxes[unselected_inds]\n        argmax_score_ind = np.argmax(process_bboxes[::, -1])\n        max_score_ind = unselected_inds[argmax_score_ind]\n        return_inds += [max_score_ind]\n        unselected_inds = np.delete(unselected_inds, argmax_score_ind)\n\n        base_bbox = bboxes[max_score_ind]\n        compare_bboxes = bboxes[unselected_inds]\n        \n        base_x1 = base_bbox[0]\n        base_y1 = base_bbox[1]\n        base_x2 = base_bbox[2] + base_x1\n        base_y2 = base_bbox[3] + base_y1\n        base_w = np.maximum(base_bbox[2], 0)\n        base_h = np.maximum(base_bbox[3], 0)\n        base_area = base_w * base_h\n\n        # compute iou-area between base bbox and other bboxes\n        iou_x1 = np.maximum(base_x1, compare_bboxes[:, 0])\n        iou_y1 = np.maximum(base_y1, compare_bboxes[:, 1])\n        iou_x2 = np.minimum(base_x2, compare_bboxes[:, 2] + compare_bboxes[:, 0])\n        iou_y2 = np.minimum(base_y2, compare_bboxes[:, 3] + compare_bboxes[:, 1])\n        iou_w = np.maximum(iou_x2 - iou_x1, 0)\n        iou_h = np.maximum(iou_y2 - iou_y1, 0)\n        iou_area = iou_w * iou_h\n\n        compare_w = np.maximum(compare_bboxes[:, 2], 0)\n        compare_h = np.maximum(compare_bboxes[:, 3], 0)\n        compare_area = compare_w * compare_h\n\n        # bbox's index which iou ratio over threshold is excluded\n        all_area = compare_area + base_area - iou_area\n        iou_ratio = np.zeros((len(unselected_inds)))\n        iou_ratio[all_area < 0.9] = 0.\n        _ind = all_area >= 0.9\n        iou_ratio[_ind] = iou_area[_ind] / all_area[_ind]\n        \n        unselected_inds = np.delete(unselected_inds, np.where(iou_ratio >= iou_th)[0])\n\n    if prob_th is not None:\n        preds = bboxes[return_inds][:, -1]\n        return_inds = np.array(return_inds)[np.where(preds >= prob_th)[0]].tolist()\n        \n    # pick bbox's index by defined number with higher score\n    if select_num is not None:\n        return_inds = return_inds[:select_num]\n\n    return return_inds\n\n\ndetects = detects[nms(detects, iou_th=0.25)]\n\n\n# Evaluation\n\n# [x1, y1, x2, y2]\nGT = np.array(((27, 48, 95, 110), (101, 75, 171, 138)), dtype=np.float32)\n\n## Recall, Precision, F-score\niou_th = 0.5\n\nRs = np.zeros((len(GT)))\nPs = np.zeros((len(detects)))\n\nfor i, g in enumerate(GT):\n    iou_x1 = np.maximum(g[0], detects[:, 0])\n    iou_y1 = np.maximum(g[1], detects[:, 1])\n    iou_x2 = np.minimum(g[2], detects[:, 2])\n    iou_y2 = np.minimum(g[3], detects[:, 3])\n    iou_w = np.maximum(0, iou_x2 - iou_x1)\n    iou_h = np.maximum(0, iou_y2 - iou_y1)\n    iou_area = iou_w * iou_h\n    g_area = (g[2] - g[0]) * (g[3] - g[1])\n    d_area = (detects[:, 2] - detects[:, 0]) * (detects[:, 3] - detects[:, 1])\n    ious = iou_area / (g_area + d_area - iou_area)\n    \n    Rs[i] = 1 if len(np.where(ious >= iou_th)[0]) > 0 else 0\n    Ps[ious >= iou_th] = 1\n    \n\nR = np.sum(Rs) / len(Rs)\nP = np.sum(Ps) / len(Ps)\nF = (2 * P * R) / (P + R) \n\nprint(\"Recall >> {:.2f} ({} / {})\".format(R, np.sum(Rs), len(Rs)))\nprint(\"Precision >> {:.2f} ({} / {})\".format(P, np.sum(Ps), len(Ps)))\nprint(\"F-score >> \", F)\n\n## mAP\nmAP = 0.\nfor i in range(len(detects)):\n    mAP += np.sum(Ps[:i]) / (i + 1) * Ps[i]\nmAP /= np.sum(Ps)\n\nprint(\"mAP >>\", mAP)\n\n# Display\nfor i in range(len(detects)):\n    v = list(map(int, detects[i, :4]))\n    if Ps[i] > 0:\n        cv2.rectangle(img2, (v[0], v[1]), (v[2], v[3]), (0,0,255), 1)\n    else:\n        cv2.rectangle(img2, (v[0], v[1]), (v[2], v[3]), (255,0,0), 1)\n    cv2.putText(img2, \"{:.2f}\".format(detects[i, -1]), (v[0], v[1]+9),\n                cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255,0,255), 1)\n\nfor g in GT:\n    cv2.rectangle(img2, (g[0], g[1]), (g[2], g[3]), (0,255,0), 1)\n\ncv2.imwrite(\"out.jpg\", img2)\ncv2.imshow(\"result\", img2)\ncv2.waitKey(0)"}], "task": "code_generation", "index": 299, "liscence": "MIT", "source": "gasyori_100_knocks"}
{"id": "amenokaku_555", "instruction": "pythonを用いて、文字列”stressed”の文字を逆に（末尾から先頭に向かって）並べた文字列を表示しなさい。", "instances": [{"input": "", "output": "text = 'stressed'\nprint(text[::-1])"}], "task": "code_generation", "index": 555, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_556", "instruction": "pythonを用いて、「パタトクカシーー」という文字列の1,3,5,7文字目を取り出して連結した文字列を表示しなさい。", "instances": [{"input": "", "output": "text = 'パタトクカシーー'\nprint(text[1::2])"}], "task": "code_generation", "index": 556, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_557", "instruction": "pythonを用いて、「パトカー」＋「タクシー」の文字を先頭から交互に連結して文字列「パタトクカシーー」を表示しなさい。", "instances": [{"input": "", "output": "text0 = 'パトカー'\ntext1 = 'タクシー'\nans = ''\n\nfor i in range(len(text0)):\n    ans += text0[i]\n    ans += text1[i]\n\nprint(ans)"}], "task": "code_generation", "index": 557, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_558", "instruction": "pythonを用いて、“Now I need a drink, alcoholic of course, after the heavy lectures involving quantum mechanics.”という文を単語に分解し，各単語の（アルファベットの）文字数を先頭から出現順に並べたリストを作成せよ。", "instances": [{"input": "", "output": "raw_text = 'Now I need a drink, alcoholic of course, after the heavy lectures involving quantum mechanics.'\ntext = raw_text.replace('.', '').replace(',', '')\nans = [len(w) for w in text.split()]\nprint(ans)"}], "task": "code_generation", "index": 558, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_559", "instruction": "pythonを用いて、“Hi He Lied Because Boron Could Not Oxidize Fluorine. New Nations Might Also Sign Peace Security Clause. Arthur King Can.”という文を単語に分解し，1, 5, 6, 7, 8, 9, 15, 16, 19番目の単語は先頭の1文字，それ以外の単語は先頭の2文字を取り出し，取り出した文字列から単語の位置（先頭から何番目の単語か）への連想配列（辞書型もしくはマップ型）を作成せよ。", "instances": [{"input": "", "output": "def extract_chars(i, word):\n    if i in [1, 5, 6, 7, 8, 9, 15, 16, 19]:\n        return (word[0], i)\n    else:\n        return (word[:2], i)\n\n\nraw_text = 'Hi He Lied Because Boron Could Not Oxidize Fluorine. New Nations Might Also Sign Peace Security Clause. Arthur King Can.'\ntext = raw_text.replace('.', '').replace(',', '')\nans = [extract_chars(i, w) for i, w in enumerate(text.split(), 1)]\nprint(dict(ans))"}], "task": "code_generation", "index": 559, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_560", "instruction": "pythonを用いて、与えられたシーケンス（文字列やリストなど）からn-gramを作る関数を作成せよ．この関数を用い，”I am an NLPer”という文から単語bi-gram，文字bi-gramを表示しなさい。", "instances": [{"input": "", "output": "def n_gram(target, n):\n    return [target[idx:idx + n] for idx in range(len(target) - n + 1)]\n\n\ntext = 'I am an NLPer'\nfor i in range(1, 4):\n    print(n_gram(text, i))\n    print(n_gram(text.split(' '), i))"}], "task": "code_generation", "index": 560, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_561", "instruction": "pythonを用いて、“paraparaparadise”と”paragraph”に含まれる文字bi-gramの集合を，それぞれ, XとYとして求め，XとYの和集合，積集合，差集合を求めよ．さらに，’se’というbi-gramがXおよびYに含まれるかどうかを調べなさい。", "instances": [{"input": "", "output": "def n_gram(target, n):\n    return [target[idx:idx + n] for idx in range(len(target) - n + 1)]\n\n\nX_text = 'paraparaparadise'\nY_text = 'paragraph'\nX = n_gram(X_text, 2)\nY = n_gram(Y_text, 2)\n\nprint(f'和集合: {set(X) | set(Y)}')\nprint(f'積集合: {set(X) & set(Y)}')\nprint(f'差集合: {set(X) - set(Y)}')\nprint('se' in (set(X) & set(Y)))"}], "task": "code_generation", "index": 561, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_562", "instruction": "pythonを用いて、引数x, y, zを受け取り「x時のyはz」という文字列を返す関数を実装せよ．さらに，x=12, y=”気温”, z=22.4として，実行結果を確認しなさい。", "instances": [{"input": "", "output": "def generate_text(x, y, z):\n    return f'{x}時の{y}は{z}'\n\n\nx = 12\ny = '気温'\nz = 22.4\nprint(generate_text(x, y, z))"}], "task": "code_generation", "index": 562, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_563", "instruction": "pythonを用いて、与えられた文字列の各文字を，以下の仕様で変換する関数cipherを実装しなさい。\n・英小文字ならば(219 - 文字コード)の文字に置換\n・その他の文字はそのまま出力\n\nまた、この関数を用い，英語のメッセージを暗号化・復号化しなさい。", "instances": [{"input": "", "output": "def cipher(text):\n    text = [chr(219 - ord(w)) if 97 <= ord(w) <= 122 else w for w in text]\n    return ''.join(text)\n\n\ntext = 'this is a message.'\nans = cipher(text)\nprint(ans)\nans = cipher(ans)\nprint(ans)"}], "task": "code_generation", "index": 563, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_564", "instruction": "pythonを用いて、スペースで区切られた単語列に対して，各単語の先頭と末尾の文字は残し，それ以外の文字の順序をランダムに並び替えるプログラムを作成しなさい。ただし、長さが４以下の単語は並び替えないこととする。適当な英語の文（例えば”I couldn’t believe that I could actually understand what I was reading : the phenomenal power of the human mind .”）を与え、その実行結果を確認せよ。", "instances": [{"input": "", "output": "import random\n\n\ndef shuffle_word(word):\n    if len(word) <= 4:\n        return word\n    else:\n        start = word[0]\n        end = word[-1]\n        others = random.sample(list(word[1:-1]), len(word[1:-1]))\n        return ''.join([start] + others + [end])\n\n\ntext = 'I couldn’t believe that I could actually understand what I was reading : the phenomenal power of the human mind .'\nans = [shuffle_word(w) for w in text.split()]\nprint(' '.join(ans))"}], "task": "code_generation", "index": 564, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_565", "instruction": "pythonを用いて、行数をカウントせよ．確認にはwcコマンドを用いよ。", "instances": [{"input": "", "output": "import pandas as pd\n\n\ndf = pd.read_csv('ch02/popular-names.txt', sep='\\t', header=None)\nprint(len(df))\n\n\nwc -l ch02/popular-names.txt"}], "task": "code_generation", "index": 565, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_566", "instruction": "pythonを用いて、タブ1文字につきスペース1文字に置換せよ。確認にはsedコマンド、trコマンド、もしくはexpandコマンドを用いよ。", "instances": [{"input": "", "output": "import pandas as pd\n\n\ndf = pd.read_csv('ch02/popular-names.txt', sep='\\t', header=None)\ndf.to_csv('ch02/ans11.txt', sep=' ', index=False, header=None)\n\n\nsed -e 's/[[:cntrl:]]/ /g' ch02/popular-names.txt >> ch02/ans11.txt"}], "task": "code_generation", "index": 566, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_567", "instruction": "pythonを用いて、各行の1列目だけを抜き出したものをcol1.txtに、2列目だけを抜き出したものをcol2.txtとしてファイルに保存せよ。確認にはcutコマンドを用いよ。", "instances": [{"input": "", "output": "import pandas as pd\n\n\ndf = pd.read_csv('ch02/popular-names.txt', sep='\\t', header=None)\ndf[0].to_csv('ch02/col1.txt', index=False, header=None)\ndf[1].to_csv('ch02/col2.txt', index=False, header=None)\n\n\ncut -f1 -d$'\\t' ch02/popular-names.txt >> ch02/col1.txt\ncut -f2 -d$'\\t' ch02/popular-names.txt >> ch02/col2.txt"}], "task": "code_generation", "index": 567, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_568", "instruction": "pythonを用いて、col1.txtとcol2.txtを結合し、元のファイルの1列目と2列目をタブ区切りで並べたテキストファイルを作成せよ。確認にはpasteコマンドを用いよ。\n\nimport pandas as pd\n\n\ndf = pd.read_csv('ch02/popular-names.txt', sep='\\t', header=None)\ndf[0].to_csv('ch02/col1.txt', index=False, header=None)\ndf[1].to_csv('ch02/col2.txt', index=False, header=None)", "instances": [{"input": "", "output": "import pandas as pd\n\n\nc1 = pd.read_csv('ch02/col1.txt', header=None)\nc2 = pd.read_csv('ch02/col2.txt', header=None)\n\ndf = pd.concat([c1, c2], axis=1)\ndf.to_csv('ch02/ans13.txt', sep='\\t', index=False, header=None)\n\n\npaste ch02/col1.txt ch02/col2.txt >> ch02/ans13.txt"}], "task": "code_generation", "index": 568, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_569", "instruction": "pythonを用いて、自然数Nをコマンドライン引数などの手段で受け取り、入力のうち先頭のN行だけを表示せよ。確認にはheadコマンドを用いよ。", "instances": [{"input": "", "output": "import sys\nimport pandas as pd\n\n\nif len(sys.argv) == 1:\n    print('Set arg n, like \"python ch02/ans14.py 5\"')\nelse:\n    n = int(sys.argv[1])\n    df = pd.read_csv('ch02/popular-names.txt', sep='\\t', header=None)\n    print(df.head(n))\n\n\nhead -n $1 ch02/popular-names.txt"}], "task": "code_generation", "index": 569, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_570", "instruction": "pythonを用いて、自然数Nをコマンドライン引数などの手段で受け取り、入力のうち末尾のN行だけを表示せよ。確認にはtailコマンドを用いよ。", "instances": [{"input": "", "output": "import sys\nimport pandas as pd\n\n\nif len(sys.argv) == 1:\n    print('Set arg n, like \"python ch02/ans15.py 5\"')\nelse:\n    n = int(sys.argv[1])\n    df = pd.read_csv('ch02/popular-names.txt', sep='\\t', header=None)\n    print(df.tail(n))\n\n\ntail -n $1 ch02/popular-names.txt"}], "task": "code_generation", "index": 570, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_571", "instruction": "pythonを用いて、自然数Nをコマンドライン引数などの手段で受け取り、入力のファイルを行単位でN分割せよ。同様の処理をsplitコマンドで実現せよ。", "instances": [{"input": "", "output": "import sys\nimport pandas as pd\n\n\nif len(sys.argv) == 1:\n    print('Set arg n, like \"python ch02/ans15.py 5\"')\nelse:\n    n = int(sys.argv[1])\n    df = pd.read_csv('ch02/popular-names.txt', sep='\\t', header=None)\n    nrow = -(-len(df) // n)\n\n    for i in range(n):\n        df.loc[nrow * i:nrow * (i + 1)].to_csv(f'ch02/ans16_{i}', sep='\\t', index=False, header=None)\n\n\nn=`wc -l ch02/popular-names.txt | awk '{print $1}'`\nln=`expr $n / $1`\nsplit -l $ln ch02/popular-names.txt ch02/ans16_"}], "task": "code_generation", "index": 571, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_572", "instruction": "pythonを用いて、1列目の文字列の種類（異なる文字列の集合）を求めよ。確認にはcut、 sort、 uniqコマンドを用いよ。", "instances": [{"input": "", "output": "import pandas as pd\n\n\ndf = pd.read_csv('ch02/popular-names.txt', sep='\\t', header=None)\nprint(df[0].unique())\n\n\ncut -f1 -d$'\\t' ch02/popular-names.txt | LANG=C sort | uniq"}], "task": "code_generation", "index": 572, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_573", "instruction": "pythonを用いて、各行を3コラム目の数値の逆順で整列せよ（注意: 各行の内容は変更せずに並び替えよ）。確認にはsortコマンドを用いよ（この問題はコマンドで実行した時の結果と合わなくてもよい）。", "instances": [{"input": "", "output": "import pandas as pd\n\n\ndf = pd.read_csv('ch02/popular-names.txt', sep='\\t', header=None)\nprint(df.sort_values(2, ascending=False))\n\n\nsort -r -k 3 -t $'\\t' ch02/popular-names.txt"}], "task": "code_generation", "index": 573, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_574", "instruction": "pythonを用いて、各行の1列目の文字列の出現頻度を求め、その高い順に並べて表示せよ。確認にはcut、uniq、sortコマンドを用いよ。", "instances": [{"input": "", "output": "import pandas as pd\n\n\ndf = pd.read_csv('ch02/popular-names.txt', sep='\\t', header=None)\nprint(df[0].value_counts())\n\n\ncut -f1 -d$'\\t' ch02/popular-names.txt | LANG=C sort | uniq -c | sort -r -k 2 -t ' '"}], "task": "code_generation", "index": 574, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_575", "instruction": "pythonを用いて、Wikipedia記事のJSONファイルを読み込み、「イギリス」に関する記事本文を表示せよ。問題21-29では、ここで抽出した記事本文に対して実行せよ。", "instances": [{"input": "", "output": "import pandas as pd\n\n\ndf = pd.read_json('ch03/jawiki-country.json.gz', lines=True)\nuk_text = df.query('title==\"イギリス\"')['text'].values[0]\nprint(uk_text)"}], "task": "code_generation", "index": 575, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_576", "instruction": "pythonを用いて、記事中でカテゴリ名を宣言している行を抽出せよ。", "instances": [{"input": "", "output": "import pandas as pd\n\n\ndf = pd.read_json('ch03/jawiki-country.json.gz', lines=True)\nuk_text = df.query('title==\"イギリス\"')['text'].values[0]\nuk_texts = uk_text.split('\\n')\nans = list(filter(lambda x: '[Category:' in x, uk_texts))\nprint(ans)"}], "task": "code_generation", "index": 576, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_577", "instruction": "pythonを用いて、記事のカテゴリ名を（行単位ではなく名前で）抽出せよ。", "instances": [{"input": "", "output": "import pandas as pd\n\n\ndf = pd.read_json('ch03/jawiki-country.json.gz', lines=True)\nuk_text = df.query('title==\"イギリス\"')['text'].values[0]\nuk_texts = uk_text.split('\\n')\nans = list(filter(lambda x: '[Category:' in x, uk_texts))\nans = [a.replace('[[Category:', '').replace('|*', '').replace(']]', '') for a in ans]\nprint(ans)"}], "task": "code_generation", "index": 577, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_578", "instruction": "pythonを用いて、記事中に含まれるセクション名とそのレベル（例えば”== セクション名 ==”なら1）を表示せよ。", "instances": [{"input": "", "output": "import re\nimport pandas as pd\n\n\ndf = pd.read_json('ch03/jawiki-country.json.gz', lines=True)\nuk_text = df.query('title==\"イギリス\"')['text'].values[0]\nfor section in re.findall(r'(=+)([^=]+)\\1\\n', uk_text):\n    print(f'{section[1].strip()}\\t{len(section[0]) - 1}')"}], "task": "code_generation", "index": 578, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_579", "instruction": "pythonを用いて、記事から参照されているメディアファイルをすべて抜き出せ。", "instances": [{"input": "", "output": "import re\nimport pandas as pd\n\n\ndf = pd.read_json('ch03/jawiki-country.json.gz', lines=True)\nuk_text = df.query('title==\"イギリス\"')['text'].values[0]\nfor file in re.findall(r'\\[\\[(ファイル|File):([^]|]+?)(\\|.*?)+\\]\\]', uk_text):\n    print(file[1])"}], "task": "code_generation", "index": 579, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_580", "instruction": "pythonを用いて、記事中に含まれる「基礎情報」テンプレートのフィールド名と値を抽出し、辞書オブジェクトとして格納せよ。", "instances": [{"input": "", "output": "import re\nimport pandas as pd\n\n\ndf = pd.read_json('ch03/jawiki-country.json.gz', lines=True)\nuk_text = df.query('title==\"イギリス\"')['text'].values[0]\nuk_texts = uk_text.split('\\n')\n\npattern = re.compile('\\|(.+?)\\s=\\s*(.+)')\nans = {}\nfor line in uk_texts:\n    r = re.search(pattern, line)\n    if r:\n        ans[r[1]] = r[2]\nprint(ans)"}], "task": "code_generation", "index": 580, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_581", "instruction": "pythonを用いて、記事中に含まれる「基礎情報」テンプレートのフィールド名と値を抽出し、辞書オブジェクトとして格納する処理時に、テンプレートの値からMediaWikiの強調マークアップ（弱い強調、強調、強い強調のすべて）を除去してテキストに変換せよ。", "instances": [{"input": "", "output": "import re\nimport pandas as pd\n\n\ndef remove_stress(dc):\n    r = re.compile(\"'+\")\n    return {k: r.sub('', v) for k, v in dc.items()}\n\n\ndf = pd.read_json('ch03/jawiki-country.json.gz', lines=True)\nuk_text = df.query('title==\"イギリス\"')['text'].values[0]\nuk_texts = uk_text.split('\\n')\n\npattern = re.compile('\\|(.+?)\\s=\\s*(.+)')\nans = {}\nfor line in uk_texts:\n    r = re.search(pattern, line)\n    if r:\n        ans[r[1]] = r[2]\nprint(remove_stress(ans))"}], "task": "code_generation", "index": 581, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_582", "instruction": "pythonを用いて、記事中に含まれる「基礎情報」テンプレートのフィールド名と値を抽出し、辞書オブジェクトとして格納し、テンプレートの値からMediaWikiの強調マークアップ（弱い強調、強調、強い強調のすべて）を除去してテキストに変換する処理時に、テンプレートの値からMediaWikiの内部リンクマークアップを除去し、テキストに変換せよ。", "instances": [{"input": "", "output": "import re\nimport pandas as pd\n\n\ndef remove_stress(dc):\n    r = re.compile(\"'+\")\n    return {k: r.sub('', v) for k, v in dc.items()}\n\n\ndef remove_inner_links(dc):\n    r = re.compile('\\[\\[(.+\\||)(.+?)\\]\\]')\n    return {k: r.sub(r'\\2', v) for k, v in dc.items()}\n\n\ndf = pd.read_json('ch03/jawiki-country.json.gz', lines=True)\nuk_text = df.query('title==\"イギリス\"')['text'].values[0]\nuk_texts = uk_text.split('\\n')\n\npattern = re.compile('\\|(.+?)\\s=\\s*(.+)')\nans = {}\nfor line in uk_texts:\n    r = re.search(pattern, line)\n    if r:\n        ans[r[1]] = r[2]\nprint(remove_inner_links(remove_stress(ans)))"}], "task": "code_generation", "index": 582, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_583", "instruction": "pythonを用いて、記事中に含まれる「基礎情報」テンプレートのフィールド名と値を抽出し、辞書オブジェクトとして格納し、テンプレートの値からMediaWikiの強調マークアップ（弱い強調、強調、強い強調のすべて）を除去してテキストに変換し、テンプレートの値からMediaWikiの内部リンクマークアップを除去し、テキストに変換する処理時に、テンプレートの値からMediaWikiマークアップを可能な限り除去し、国の基本情報を整形せよ。", "instances": [{"input": "", "output": "import re\nimport pandas as pd\n\n\ndef remove_stress(dc):\n    r = re.compile(\"'+\")\n    return {k: r.sub('', v) for k, v in dc.items()}\n\n\ndef remove_inner_links(dc):\n    r = re.compile('\\[\\[(.+\\||)(.+?)\\]\\]')\n    return {k: r.sub(r'\\2', v) for k, v in dc.items()}\n\n\ndef remove_mk(v):\n    r1 = re.compile(\"'+\")\n    r2 = re.compile('\\[\\[(.+\\||)(.+?)\\]\\]')\n    r3 = re.compile('\\{\\{(.+\\||)(.+?)\\}\\}')\n    r4 = re.compile('<\\s*?/*?\\s*?br\\s*?/*?\\s*>')\n    v = r1.sub('', v)\n    v = r2.sub(r'\\2', v)\n    v = r3.sub(r'\\2', v)\n    v = r4.sub('', v)\n    return v\n\n\ndf = pd.read_json('ch03/jawiki-country.json.gz', lines=True)\nuk_text = df.query('title==\"イギリス\"')['text'].values[0]\nuk_texts = uk_text.split('\\n')\n\npattern = re.compile('\\|(.+?)\\s=\\s*(.+)')\nans = {}\nfor line in uk_texts:\n    r = re.search(pattern, line)\n    if r:\n        ans[r[1]] = r[2]\n\nr = re.compile('\\[\\[(.+\\||)(.+?)\\]\\]')\nans = {k: r.sub(r'\\2', remove_mk(v)) for k, v in ans.items()}\nprint(remove_inner_links(remove_stress(ans)))"}], "task": "code_generation", "index": 583, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_584", "instruction": "pythonを用いて、テンプレートの内容を利用し、国旗画像のURLを取得せよ。", "instances": [{"input": "", "output": "import re\nimport requests\nimport pandas as pd\n\n\ndef remove_stress(dc):\n    r = re.compile(\"'+\")\n    return {k: r.sub('', v) for k, v in dc.items()}\n\n\ndef remove_inner_links(dc):\n    r = re.compile('\\[\\[(.+\\||)(.+?)\\]\\]')\n    return {k: r.sub(r'\\2', v) for k, v in dc.items()}\n\n\ndef remove_mk(v):\n    r1 = re.compile(\"'+\")\n    r2 = re.compile('\\[\\[(.+\\||)(.+?)\\]\\]')\n    r3 = re.compile('\\{\\{(.+\\||)(.+?)\\}\\}')\n    r4 = re.compile('<\\s*?/*?\\s*?br\\s*?/*?\\s*>')\n    v = r1.sub('', v)\n    v = r2.sub(r'\\2', v)\n    v = r3.sub(r'\\2', v)\n    v = r4.sub('', v)\n    return v\n\n\ndef get_url(dc):\n    url_file = dc['国旗画像'].replace(' ', '_')\n    url = 'https://commons.wikimedia.org/w/api.php?action=query&titles=File:' + url_file + '&prop=imageinfo&iiprop=url&format=json'\n    data = requests.get(url)\n    return re.search(r'\"url\":\"(.+?)\"', data.text).group(1)\n\n\ndf = pd.read_json('ch03/jawiki-country.json.gz', lines=True)\nuk_text = df.query('title==\"イギリス\"')['text'].values[0]\nuk_texts = uk_text.split('\\n')\n\npattern = re.compile('\\|(.+?)\\s=\\s*(.+)')\nans = {}\nfor line in uk_texts:\n    r = re.search(pattern, line)\n    if r:\n        ans[r[1]] = r[2]\n\nr = re.compile('\\[\\[(.+\\||)(.+?)\\]\\]')\nans = {k: r.sub(r'\\2', remove_mk(v)) for k, v in ans.items()}\nprint(get_url(remove_inner_links(remove_stress(ans))))"}], "task": "code_generation", "index": 584, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_585", "instruction": "pythonを用いて、形態素解析結果（neko.txt.mecab）を読み込むプログラムを実装せよ。ただし、各形態素は表層形（surface）、基本形（base）、品詞（pos）、品詞細分類1（pos1）をキーとするマッピング型に格納し、1文を形態素（マッピング型）のリストとして表現せよ。", "instances": [{"input": "", "output": "def parse_mecab(block):\n    res = []\n    for line in block.split('\\n'):\n        if line == '':\n            return res\n        (surface, attr) = line.split('\\t')\n        attr = attr.split(',')\n        lineDict = {\n            'surface': surface,\n            'base': attr[6],\n            'pos': attr[0],\n            'pos1': attr[1]\n        }\n        res.append(lineDict)\n\n\nfilename = 'ch04/neko.txt.mecab'\nwith open(filename, mode='rt', encoding='utf-8') as f:\n    blocks = f.read().split('EOS\\n')\nblocks = list(filter(lambda x: x != '', blocks))\nblocks = [parse_mecab(block) for block in blocks]\nprint(blocks[5])\n\n\nmecab < ch04/neko.txt > ch04/neko.txt.mecab"}], "task": "code_generation", "index": 585, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_586", "instruction": "pythonを用いて、動詞の表層形をすべて抽出せよ。", "instances": [{"input": "", "output": "def parse_mecab(block):\n    res = []\n    for line in block.split('\\n'):\n        if line == '':\n            return res\n        (surface, attr) = line.split('\\t')\n        attr = attr.split(',')\n        lineDict = {\n            'surface': surface,\n            'base': attr[6],\n            'pos': attr[0],\n            'pos1': attr[1]\n        }\n        res.append(lineDict)\n\n\ndef extract_surface(block):\n    res = list(filter(lambda x: x['pos'] == '動詞', block))\n    res = [r['surface'] for r in res]\n    return res\n\n\nfilename = 'ch04/neko.txt.mecab'\nwith open(filename, mode='rt', encoding='utf-8') as f:\n    blocks = f.read().split('EOS\\n')\nblocks = list(filter(lambda x: x != '', blocks))\nblocks = [parse_mecab(block) for block in blocks]\nans = [extract_surface(block) for block in blocks]\nprint(ans[5])"}], "task": "code_generation", "index": 586, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_587", "instruction": "pythonを用いて、動詞の基本形をすべて抽出せよ。", "instances": [{"input": "", "output": "def parse_mecab(block):\n    res = []\n    for line in block.split('\\n'):\n        if line == '':\n            return res\n        (surface, attr) = line.split('\\t')\n        attr = attr.split(',')\n        lineDict = {\n            'surface': surface,\n            'base': attr[6],\n            'pos': attr[0],\n            'pos1': attr[1]\n        }\n        res.append(lineDict)\n\n\ndef extract_base(block):\n    res = list(filter(lambda x: x['pos'] == '動詞', block))\n    res = [r['base'] for r in res]\n    return res\n\n\nfilename = 'ch04/neko.txt.mecab'\nwith open(filename, mode='rt', encoding='utf-8') as f:\n    blocks = f.read().split('EOS\\n')\nblocks = list(filter(lambda x: x != '', blocks))\nblocks = [parse_mecab(block) for block in blocks]\nans = [extract_base(block) for block in blocks]\nprint(ans[5])"}], "task": "code_generation", "index": 587, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_588", "instruction": "pythonを用いて、2つの名詞が「の」で連結されている名詞句を抽出せよ。", "instances": [{"input": "", "output": "def parse_mecab(block):\n    res = []\n    for line in block.split('\\n'):\n        if line == '':\n            return res\n        (surface, attr) = line.split('\\t')\n        attr = attr.split(',')\n        lineDict = {\n            'surface': surface,\n            'base': attr[6],\n            'pos': attr[0],\n            'pos1': attr[1]\n        }\n        res.append(lineDict)\n\n\ndef extract_a_no_b(block):\n    res = []\n    for i in range(1, len(block) - 1):\n        if block[i - 1]['pos'] == '名詞' and block[i]['base'] == 'の' and block[i + 1]['pos'] == '名詞':\n            res.append(block[i - 1]['surface'] + block[i]['surface'] + block[i + 1]['surface'])\n    return res\n\n\nfilename = 'ch04/neko.txt.mecab'\nwith open(filename, mode='rt', encoding='utf-8') as f:\n    blocks = f.read().split('EOS\\n')\nblocks = list(filter(lambda x: x != '', blocks))\nblocks = [parse_mecab(block) for block in blocks]\nans = [extract_a_no_b(block) for block in blocks]\nprint(ans)"}], "task": "code_generation", "index": 588, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_589", "instruction": "pythonを用いて、文章中に出現する単語とその出現頻度を求め、出現頻度の高い順に並べよ。", "instances": [{"input": "", "output": "def parse_mecab(block):\n    res = []\n    for line in block.split('\\n'):\n        if line == '':\n            return res\n        (surface, attr) = line.split('\\t')\n        attr = attr.split(',')\n        lineDict = {\n            'surface': surface,\n            'base': attr[6],\n            'pos': attr[0],\n            'pos1': attr[1]\n        }\n        res.append(lineDict)\n\n\ndef extract_noun_noun(block):\n    res = []\n    tmp = []\n    for b in block:\n        if b['pos'] == '名詞':\n            tmp.append(b['surface'])\n        elif len(tmp) >= 2:\n            res.append(''.join(tmp))\n            tmp = []\n        else:\n            tmp = []\n    return res\n\n\nfilename = 'ch04/neko.txt.mecab'\nwith open(filename, mode='rt', encoding='utf-8') as f:\n    blocks = f.read().split('EOS\\n')\nblocks = list(filter(lambda x: x != '', blocks))\nblocks = [parse_mecab(block) for block in blocks]\nans = [extract_noun_noun(block) for block in blocks]\nprint(ans)"}], "task": "code_generation", "index": 589, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_590", "instruction": "pythonを用いて、文章中に出現する単語とその出現頻度を求め、出現頻度の高い順に並べよ。", "instances": [{"input": "", "output": "from collections import defaultdict\n\n\ndef parse_mecab(block):\n    res = []\n    for line in block.split('\\n'):\n        if line == '':\n            return res\n        (surface, attr) = line.split('\\t')\n        attr = attr.split(',')\n        lineDict = {\n            'surface': surface,\n            'base': attr[6],\n            'pos': attr[0],\n            'pos1': attr[1]\n        }\n        res.append(lineDict)\n\n\ndef extract_words(block):\n    return [b['base'] + '_' + b['pos'] + '_' + b['pos1'] for b in block]\n\n\nfilename = 'ch04/neko.txt.mecab'\nwith open(filename, mode='rt', encoding='utf-8') as f:\n    blocks = f.read().split('EOS\\n')\nblocks = list(filter(lambda x: x != '', blocks))\nblocks = [parse_mecab(block) for block in blocks]\nwords = [extract_words(block) for block in blocks]\nd = defaultdict(int)\nfor word in words:\n    for w in word:\n        d[w] += 1\nans = sorted(d.items(), key=lambda x: x[1], reverse=True)\nprint(ans)"}], "task": "code_generation", "index": 590, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_591", "instruction": "pythonを用いて、出現頻度が高い10語とその出現頻度をグラフ（例えば棒グラフなど）で表示せよ。", "instances": [{"input": "", "output": "from collections import defaultdict\nimport matplotlib.pyplot as plt\nimport japanize_matplotlib\n\n\ndef parse_mecab(block):\n    res = []\n    for line in block.split('\\n'):\n        if line == '':\n            return res\n        (surface, attr) = line.split('\\t')\n        attr = attr.split(',')\n        lineDict = {\n            'surface': surface,\n            'base': attr[6],\n            'pos': attr[0],\n            'pos1': attr[1]\n        }\n        res.append(lineDict)\n\n\ndef extract_words(block):\n    return [b['base'] + '_' + b['pos'] + '_' + b['pos1'] for b in block]\n\n\nfilename = 'ch04/neko.txt.mecab'\nwith open(filename, mode='rt', encoding='utf-8') as f:\n    blocks = f.read().split('EOS\\n')\nblocks = list(filter(lambda x: x != '', blocks))\nblocks = [parse_mecab(block) for block in blocks]\nwords = [extract_words(block) for block in blocks]\nd = defaultdict(int)\nfor word in words:\n    for w in word:\n        d[w] += 1\nans = sorted(d.items(), key=lambda x: x[1], reverse=True)[:10]\nlabels = [a[0] for a in ans]\nvalues = [a[1] for a in ans]\n\nplt.figure(figsize=(15, 8))\nplt.barh(labels, values)\nplt.savefig('ch04/ans36.png')"}], "task": "code_generation", "index": 591, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_592", "instruction": "pythonを用いて、「猫」とよく共起する（共起頻度が高い）10語とその出現頻度をグラフ（例えば棒グラフなど）で表示せよ。", "instances": [{"input": "", "output": "from collections import defaultdict\nimport matplotlib.pyplot as plt\nimport japanize_matplotlib\n\n\ndef parse_mecab(block):\n    res = []\n    for line in block.split('\\n'):\n        if line == '':\n            return res\n        (surface, attr) = line.split('\\t')\n        attr = attr.split(',')\n        lineDict = {\n            'surface': surface,\n            'base': attr[6],\n            'pos': attr[0],\n            'pos1': attr[1]\n        }\n        res.append(lineDict)\n\n\ndef extract_base(block):\n    return [b['base'] for b in block]\n\n\nfilename = 'ch04/neko.txt.mecab'\nwith open(filename, mode='rt', encoding='utf-8') as f:\n    blocks = f.read().split('EOS\\n')\nblocks = list(filter(lambda x: x != '', blocks))\nblocks = [parse_mecab(block) for block in blocks]\nwords = [extract_base(block) for block in blocks]\nwords = list(filter(lambda x: '猫' in x, words))\nd = defaultdict(int)\nfor word in words:\n    for w in word:\n        if w != '猫':\n            d[w] += 1\nans = sorted(d.items(), key=lambda x: x[1], reverse=True)[:10]\nlabels = [a[0] for a in ans]\nvalues = [a[1] for a in ans]\nplt.figure(figsize=(8, 8))\nplt.barh(labels, values)\nplt.savefig('ch04/ans37.png')"}], "task": "code_generation", "index": 592, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_593", "instruction": "pythonを用いて、単語の出現頻度のヒストグラムを描け。ただし、横軸は出現頻度を表し、1から単語の出現頻度の最大値までの線形目盛とする。縦軸はx軸で示される出現頻度となった単語の異なり数（種類数）である。", "instances": [{"input": "", "output": "from collections import defaultdict\nimport matplotlib.pyplot as plt\n\n\ndef parse_mecab(block):\n    res = []\n    for line in block.split('\\n'):\n        if line == '':\n            return res\n        (surface, attr) = line.split('\\t')\n        attr = attr.split(',')\n        lineDict = {\n            'surface': surface,\n            'base': attr[6],\n            'pos': attr[0],\n            'pos1': attr[1]\n        }\n        res.append(lineDict)\n\n\ndef extract_words(block):\n    return [b['base'] + '_' + b['pos'] + '_' + b['pos1'] for b in block]\n\n\nfilename = 'ch04/neko.txt.mecab'\nwith open(filename, mode='rt', encoding='utf-8') as f:\n    blocks = f.read().split('EOS\\n')\nblocks = list(filter(lambda x: x != '', blocks))\nblocks = [parse_mecab(block) for block in blocks]\nwords = [extract_words(block) for block in blocks]\nd = defaultdict(int)\nfor word in words:\n    for w in word:\n        d[w] += 1\nans = d.values()\nplt.figure(figsize=(8, 8))\nplt.hist(ans, bins=100)\nplt.savefig('ch04/ans38.png')"}], "task": "code_generation", "index": 593, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_594", "instruction": "pythonを用いて、単語の出現頻度順位を横軸、その出現頻度を縦軸として、両対数グラフをプロットせよ。", "instances": [{"input": "", "output": "import math\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\n\n\ndef parse_mecab(block):\n    res = []\n    for line in block.split('\\n'):\n        if line == '':\n            return res\n        (surface, attr) = line.split('\\t')\n        attr = attr.split(',')\n        lineDict = {\n            'surface': surface,\n            'base': attr[6],\n            'pos': attr[0],\n            'pos1': attr[1]\n        }\n        res.append(lineDict)\n\n\ndef extract_words(block):\n    return [b['base'] + '_' + b['pos'] + '_' + b['pos1'] for b in block]\n\n\nfilename = 'ch04/neko.txt.mecab'\nwith open(filename, mode='rt', encoding='utf-8') as f:\n    blocks = f.read().split('EOS\\n')\nblocks = list(filter(lambda x: x != '', blocks))\nblocks = [parse_mecab(block) for block in blocks]\nwords = [extract_words(block) for block in blocks]\nd = defaultdict(int)\nfor word in words:\n    for w in word:\n        d[w] += 1\nans = sorted(d.items(), key=lambda x: x[1], reverse=True)\nranks = [math.log(r + 1) for r in range(len(ans))]\nvalues = [math.log(a[1]) for a in ans]\nplt.figure(figsize=(8, 8))\nplt.scatter(ranks, values)\nplt.savefig('ch04/ans39.png')"}], "task": "code_generation", "index": 594, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_595", "instruction": "pythonを用いて、形態素を表すクラスMorphを実装せよ。このクラスは表層形（surface）、基本形（base）、品詞（pos）、品詞細分類1（pos1）をメンバ変数に持つこととする。さらに、係り受け解析の結果（ai.ja.txt.parsed）を読み込み、各文をMorphオブジェクトのリストとして表現し、冒頭の説明文の形態素列を表示せよ。", "instances": [{"input": "", "output": "class Morph:\n    def __init__(self, dc):\n        self.surface = dc['surface']\n        self.base = dc['base']\n        self.pos = dc['pos']\n        self.pos1 = dc['pos1']\n\n\ndef parse_cabocha(block):\n    res = []\n    for line in block.split('\\n'):\n        if line == '':\n            return res\n        elif line[0] == '*':\n            continue\n        (surface, attr) = line.split('\\t')\n        attr = attr.split(',')\n        lineDict = {\n            'surface': surface,\n            'base': attr[6],\n            'pos': attr[0],\n            'pos1': attr[1]\n        }\n        res.append(Morph(lineDict))\n\n\nfilename = 'ch05/ai.ja.txt.cabocha'\nwith open(filename, mode='rt', encoding='utf-8') as f:\n    blocks = f.read().split('EOS\\n')\nblocks = list(filter(lambda x: x != '', blocks))\nblocks = [parse_cabocha(block) for block in blocks]\nfor m in blocks[2]:\n    print(vars(m))"}], "task": "code_generation", "index": 595, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_596", "instruction": "pythonを用いて、形態素を表すクラスMorphと文節を表すクラスChunkを実装せよ。このクラスは形態素（Morphオブジェクト）のリスト（morphs）、係り先文節インデックス番号（dst）、係り元文節インデックス番号のリスト（srcs）をメンバ変数に持つこととする。さらに、入力テキストの係り受け解析結果を読み込み、１文をChunkオブジェクトのリストとして表現し、冒頭の説明文の文節の文字列と係り先を表示せよ。本章の残りの問題では、ここで作ったプログラムを活用せよ。", "instances": [{"input": "", "output": "class Morph:\n    def __init__(self, dc):\n        self.surface = dc['surface']\n        self.base = dc['base']\n        self.pos = dc['pos']\n        self.pos1 = dc['pos1']\n\n\nclass Chunk:\n    def __init__(self, morphs, dst):\n        self.morphs = morphs    # 形態素（Morphオブジェクト）のリスト\n        self.dst = dst          # 係り先文節インデックス番号\n        self.srcs = []          # 係り元文節インデックス番号のリスト\n\n\ndef parse_cabocha(block):\n    def check_create_chunk(tmp):\n        if len(tmp) > 0:\n            c = Chunk(tmp, dst)\n            res.append(c)\n            tmp = []\n        return tmp\n\n    res = []\n    tmp = []\n    dst = None\n    for line in block.split('\\n'):\n        if line == '':\n            tmp = check_create_chunk(tmp)\n        elif line[0] == '*':\n            dst = line.split(' ')[2].rstrip('D')\n            tmp = check_create_chunk(tmp)\n        else:\n            (surface, attr) = line.split('\\t')\n            attr = attr.split(',')\n            lineDict = {\n                'surface': surface,\n                'base': attr[6],\n                'pos': attr[0],\n                'pos1': attr[1]\n            }\n            tmp.append(Morph(lineDict))\n\n    for i, r in enumerate(res):\n        res[int(r.dst)].srcs.append(i)\n    return res\n\n\nfilename = 'ch05/ai.ja.txt.cabocha'\nwith open(filename, mode='rt', encoding='utf-8') as f:\n    blocks = f.read().split('EOS\\n')\nblocks = list(filter(lambda x: x != '', blocks))\nblocks = [parse_cabocha(block) for block in blocks]\nfor m in blocks[7]:\n    print([mo.surface for mo in m.morphs], m.dst, m.srcs)"}], "task": "code_generation", "index": 596, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_597", "instruction": "pythonを用いて、係り元の文節と係り先の文節のテキストをタブ区切り形式ですべて抽出せよ。ただし、句読点などの記号は出力しないようにせよ。", "instances": [{"input": "", "output": "class Morph:\n    def __init__(self, dc):\n        self.surface = dc['surface']\n        self.base = dc['base']\n        self.pos = dc['pos']\n        self.pos1 = dc['pos1']\n\n\nclass Chunk:\n    def __init__(self, morphs, dst):\n        self.morphs = morphs    # 形態素（Morphオブジェクト）のリスト\n        self.dst = dst          # 係り先文節インデックス番号\n        self.srcs = []          # 係り元文節インデックス番号のリスト\n\n\ndef parse_cabocha(block):\n    def check_create_chunk(tmp):\n        if len(tmp) > 0:\n            c = Chunk(tmp, dst)\n            res.append(c)\n            tmp = []\n        return tmp\n\n    res = []\n    tmp = []\n    dst = None\n    for line in block.split('\\n'):\n        if line == '':\n            tmp = check_create_chunk(tmp)\n        elif line[0] == '*':\n            dst = line.split(' ')[2].rstrip('D')\n            tmp = check_create_chunk(tmp)\n        else:\n            (surface, attr) = line.split('\\t')\n            attr = attr.split(',')\n            lineDict = {\n                'surface': surface,\n                'base': attr[6],\n                'pos': attr[0],\n                'pos1': attr[1]\n            }\n            tmp.append(Morph(lineDict))\n\n    for i, r in enumerate(res):\n        res[int(r.dst)].srcs.append(i)\n    return res\n\n\nfilename = 'ch05/ai.ja.txt.cabocha'\nwith open(filename, mode='rt', encoding='utf-8') as f:\n    blocks = f.read().split('EOS\\n')\nblocks = list(filter(lambda x: x != '', blocks))\nblocks = [parse_cabocha(block) for block in blocks]\n\nfor b in blocks:\n    for m in b:\n        if int(m.dst) > -1:\n            print(''.join([mo.surface if mo.pos != '記号' else '' for mo in m.morphs]),\n                  ''.join([mo.surface if mo.pos != '記号' else '' for mo in b[int(m.dst)].morphs]), sep='\\t')"}], "task": "code_generation", "index": 597, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_598", "instruction": "pythonを用いて、名詞を含む文節が、動詞を含む文節に係るとき、これらをタブ区切り形式で抽出せよ。ただし、句読点などの記号は出力しないようにせよ。", "instances": [{"input": "", "output": "class Morph:\n    def __init__(self, dc):\n        self.surface = dc['surface']\n        self.base = dc['base']\n        self.pos = dc['pos']\n        self.pos1 = dc['pos1']\n\n\nclass Chunk:\n    def __init__(self, morphs, dst):\n        self.morphs = morphs    # 形態素（Morphオブジェクト）のリスト\n        self.dst = dst          # 係り先文節インデックス番号\n        self.srcs = []          # 係り元文節インデックス番号のリスト\n\n\ndef parse_cabocha(block):\n    def check_create_chunk(tmp):\n        if len(tmp) > 0:\n            c = Chunk(tmp, dst)\n            res.append(c)\n            tmp = []\n        return tmp\n\n    res = []\n    tmp = []\n    dst = None\n    for line in block.split('\\n'):\n        if line == '':\n            tmp = check_create_chunk(tmp)\n        elif line[0] == '*':\n            dst = line.split(' ')[2].rstrip('D')\n            tmp = check_create_chunk(tmp)\n        else:\n            (surface, attr) = line.split('\\t')\n            attr = attr.split(',')\n            lineDict = {\n                'surface': surface,\n                'base': attr[6],\n                'pos': attr[0],\n                'pos1': attr[1]\n            }\n            tmp.append(Morph(lineDict))\n\n    for i, r in enumerate(res):\n        res[int(r.dst)].srcs.append(i)\n    return res\n\n\nfilename = 'ch05/ai.ja.txt.cabocha'\nwith open(filename, mode='rt', encoding='utf-8') as f:\n    blocks = f.read().split('EOS\\n')\nblocks = list(filter(lambda x: x != '', blocks))\nblocks = [parse_cabocha(block) for block in blocks]\n\nfor b in blocks:\n    for m in b:\n        if int(m.dst) > -1:\n            pre_text = ''.join([mo.surface if mo.pos != '記号' else '' for mo in m.morphs])\n            pre_pos = [mo.pos for mo in m.morphs]\n            post_text = ''.join([mo.surface if mo.pos != '記号' else '' for mo in b[int(m.dst)].morphs])\n            post_pos = [mo.pos for mo in b[int(m.dst)].morphs]\n            if '名詞' in pre_pos and '動詞' in post_pos:\n                print(pre_text, post_text, sep='\\t')"}], "task": "code_generation", "index": 598, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_599", "instruction": "pythonを用いて、与えられた文の係り受け木を有向グラフとして可視化せよ。可視化には、Graphviz等を用いるとよい。", "instances": [{"input": "", "output": "import pydot\n\n\nclass Morph:\n    def __init__(self, dc):\n        self.surface = dc['surface']\n        self.base = dc['base']\n        self.pos = dc['pos']\n        self.pos1 = dc['pos1']\n\n\nclass Chunk:\n    def __init__(self, morphs, dst):\n        self.morphs = morphs    # 形態素（Morphオブジェクト）のリスト\n        self.dst = dst          # 係り先文節インデックス番号\n        self.srcs = []          # 係り元文節インデックス番号のリスト\n\n\ndef parse_cabocha(block):\n    def check_create_chunk(tmp):\n        if len(tmp) > 0:\n            c = Chunk(tmp, dst)\n            res.append(c)\n            tmp = []\n        return tmp\n\n    res = []\n    tmp = []\n    dst = None\n    for line in block.split('\\n'):\n        if line == '':\n            tmp = check_create_chunk(tmp)\n        elif line[0] == '*':\n            dst = line.split(' ')[2].rstrip('D')\n            tmp = check_create_chunk(tmp)\n        else:\n            (surface, attr) = line.split('\\t')\n            attr = attr.split(',')\n            lineDict = {\n                'surface': surface,\n                'base': attr[6],\n                'pos': attr[0],\n                'pos1': attr[1]\n            }\n            tmp.append(Morph(lineDict))\n\n    for i, r in enumerate(res):\n        res[int(r.dst)].srcs.append(i)\n    return res\n\n\nfilename = 'ch05/ai.ja.txt.cabocha'\nwith open(filename, mode='rt', encoding='utf-8') as f:\n    blocks = f.read().split('EOS\\n')\nblocks = list(filter(lambda x: x != '', blocks))\nblocks = [parse_cabocha(block) for block in blocks]\n\npairs = []\ntarget = blocks[7]\nfor m in target:\n    if int(m.dst) > -1:\n        pre_text = ''.join([mo.surface if mo.pos != '記号' else '' for mo in m.morphs])\n        post_text = ''.join([mo.surface if mo.pos != '記号' else '' for mo in target[int(m.dst)].morphs])\n        pairs.append([pre_text, post_text])\n\nprint(pairs)\ng = pydot.graph_from_edges(pairs)\ng.write_png('ch05/ans44.png', prog='dot')"}], "task": "code_generation", "index": 599, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_600", "instruction": "pythonを用いて、今回用いている文章をコーパスと見なし、日本語の述語が取りうる格を調査したい。 動詞を述語、動詞に係っている文節の助詞を格と考え、述語と格をタブ区切り形式で出力せよ。 ただし、出力は以下の仕様を満たすようにせよ。\n\n・動詞を含む文節において、最左の動詞の基本形を述語とする\n・述語に係る助詞を格とする\n・述語に係る助詞（文節）が複数あるときは、すべての助詞をスペース区切りで辞書順に並べる\n「ジョン・マッカーシーはAIに関する最初の会議で人工知能という用語を作り出した。」という例文を考える。 この文は「作り出す」という１つの動詞を含み、「作り出す」に係る文節は「ジョン・マッカーシーは」、「会議で」、「用語を」であると解析された場合は、次のような出力になるはずである。\n \n```\n作り出す\tで は を\nこのプログラムの出力をファイルに保存し、以下の事項をUNIXコマンドを用いて確認せよ。\n```\n\n・コーパス中で頻出する述語と格パターンの組み合わせ\n・「行う」「なる」「与える」という動詞の格パターン（コーパス中で出現頻度の高い順に並べよ）", "instances": [{"input": "", "output": "class Morph:\n    def __init__(self, dc):\n        self.surface = dc['surface']\n        self.base = dc['base']\n        self.pos = dc['pos']\n        self.pos1 = dc['pos1']\n\n\nclass Chunk:\n    def __init__(self, morphs, dst):\n        self.morphs = morphs    # 形態素（Morphオブジェクト）のリスト\n        self.dst = dst          # 係り先文節インデックス番号\n        self.srcs = []          # 係り元文節インデックス番号のリスト\n\n\ndef parse_cabocha(block):\n    def check_create_chunk(tmp):\n        if len(tmp) > 0:\n            c = Chunk(tmp, dst)\n            res.append(c)\n            tmp = []\n        return tmp\n\n    res = []\n    tmp = []\n    dst = None\n    for line in block.split('\\n'):\n        if line == '':\n            tmp = check_create_chunk(tmp)\n        elif line[0] == '*':\n            dst = line.split(' ')[2].rstrip('D')\n            tmp = check_create_chunk(tmp)\n        else:\n            (surface, attr) = line.split('\\t')\n            attr = attr.split(',')\n            lineDict = {\n                'surface': surface,\n                'base': attr[6],\n                'pos': attr[0],\n                'pos1': attr[1]\n            }\n            tmp.append(Morph(lineDict))\n\n    for i, r in enumerate(res):\n        res[int(r.dst)].srcs.append(i)\n    return res\n\n\nfilename = 'ch05/ai.ja.txt.cabocha'\nwith open(filename, mode='rt', encoding='utf-8') as f:\n    blocks = f.read().split('EOS\\n')\nblocks = list(filter(lambda x: x != '', blocks))\nblocks = [parse_cabocha(block) for block in blocks]\n\nfor b in blocks:\n    for m in b:\n        if len(m.srcs) > 0:\n            pre_morphs = [b[int(s)].morphs for s in m.srcs]\n            pre_morphs = [list(filter(lambda x: '助詞' in x.pos, pm)) for pm in pre_morphs]\n            pre_surface = [[p.surface for p in pm] for pm in pre_morphs]\n            pre_surface = list(filter(lambda x: x != [], pre_surface))\n            pre_surface = [p[0] for p in pre_surface]\n            post_base = [mo.base for mo in m.morphs]\n            post_pos = [mo.pos for mo in m.morphs]\n            if len(pre_surface) > 0 and '動詞' in post_pos:\n                print(post_base[0], ' '.join(pre_surface), sep='\\t')"}], "task": "code_generation", "index": 600, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_601", "instruction": "pythonを用いて、45のプログラムを改変し、述語と格パターンに続けて項（述語に係っている文節そのもの）をタブ区切り形式で出力せよ。45の仕様に加えて、以下の仕様を満たすようにせよ。\n\n・項は述語に係っている文節の単語列とする（末尾の助詞を取り除く必要はない）\n・述語に係る文節が複数あるときは、助詞と同一の基準・順序でスペース区切りで並べる\n「ジョン・マッカーシーはAIに関する最初の会議で人工知能という用語を作り出した。」という例文を考える。 この文は「作り出す」という１つの動詞を含み、「作り出す」に係る文節は「ジョン・マッカーシーは」、「会議で」、「用語を」であると解析された場合は、次のような出力になるはずである。\n\n```\n作り出す\tで は を\t会議で ジョンマッカーシーは 用語を\n```", "instances": [{"input": "", "output": "class Morph:\n    def __init__(self, dc):\n        self.surface = dc['surface']\n        self.base = dc['base']\n        self.pos = dc['pos']\n        self.pos1 = dc['pos1']\n\n\nclass Chunk:\n    def __init__(self, morphs, dst):\n        self.morphs = morphs    # 形態素（Morphオブジェクト）のリスト\n        self.dst = dst          # 係り先文節インデックス番号\n        self.srcs = []          # 係り元文節インデックス番号のリスト\n\n\ndef parse_cabocha(block):\n    def check_create_chunk(tmp):\n        if len(tmp) > 0:\n            c = Chunk(tmp, dst)\n            res.append(c)\n            tmp = []\n        return tmp\n\n    res = []\n    tmp = []\n    dst = None\n    for line in block.split('\\n'):\n        if line == '':\n            tmp = check_create_chunk(tmp)\n        elif line[0] == '*':\n            dst = line.split(' ')[2].rstrip('D')\n            tmp = check_create_chunk(tmp)\n        else:\n            (surface, attr) = line.split('\\t')\n            attr = attr.split(',')\n            lineDict = {\n                'surface': surface,\n                'base': attr[6],\n                'pos': attr[0],\n                'pos1': attr[1]\n            }\n            tmp.append(Morph(lineDict))\n\n    for i, r in enumerate(res):\n        res[int(r.dst)].srcs.append(i)\n    return res\n\n\nfilename = 'ch05/ai.ja.txt.cabocha'\nwith open(filename, mode='rt', encoding='utf-8') as f:\n    blocks = f.read().split('EOS\\n')\nblocks = list(filter(lambda x: x != '', blocks))\nblocks = [parse_cabocha(block) for block in blocks]\n\nfor b in blocks:\n    for m in b:\n        if len(m.srcs) > 0:\n            pre_morphs = [b[int(s)].morphs for s in m.srcs]\n            pre_morphs_filtered = [list(filter(lambda x: '助詞' in x.pos, pm)) for pm in pre_morphs]\n            pre_surface = [[p.surface for p in pm] for pm in pre_morphs_filtered]\n            pre_surface = list(filter(lambda x: x != [], pre_surface))\n            pre_surface = [p[0] for p in pre_surface]\n            post_base = [mo.base for mo in m.morphs]\n            post_pos = [mo.pos for mo in m.morphs]\n            if len(pre_surface) > 0 and '動詞' in post_pos:\n                pre_text = list(filter(lambda x: '助詞' in [p.pos for p in x], pre_morphs))\n                pre_text = [''.join([p.surface for p in pt]) for pt in pre_text]\n                print(post_base[0], ' '.join(pre_surface), ' '.join(pre_text), sep='\\t')"}], "task": "code_generation", "index": 601, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_602", "instruction": "pythonを用いて、動詞のヲ格にサ変接続名詞が入っている場合のみに着目したい。46のプログラムを以下の仕様を満たすように改変せよ。\n\n・「サ変接続名詞+を（助詞）」で構成される文節が動詞に係る場合のみを対象とする\n・述語は「サ変接続名詞+を+動詞の基本形」とし、文節中に複数の動詞があるときは、最左の動詞を用いる\n・述語に係る助詞（文節）が複数あるときは、すべての助詞をスペース区切りで辞書順に並べる\n・述語に係る文節が複数ある場合は、すべての項をスペース区切りで並べる（助詞の並び順と揃えよ）\n例えば「また、自らの経験を元に学習を行う強化学習という手法もある。」という文から、以下の出力が得られるはずである。\n\n```\n学習を行う\tに を\t元に 経験を\n```", "instances": [{"input": "", "output": "class Morph:\n    def __init__(self, dc):\n        self.surface = dc['surface']\n        self.base = dc['base']\n        self.pos = dc['pos']\n        self.pos1 = dc['pos1']\n\n\nclass Chunk:\n    def __init__(self, morphs, dst):\n        self.morphs = morphs    # 形態素（Morphオブジェクト）のリスト\n        self.dst = dst          # 係り先文節インデックス番号\n        self.srcs = []          # 係り元文節インデックス番号のリスト\n\n\ndef parse_cabocha(block):\n    def check_create_chunk(tmp):\n        if len(tmp) > 0:\n            c = Chunk(tmp, dst)\n            res.append(c)\n            tmp = []\n        return tmp\n\n    res = []\n    tmp = []\n    dst = None\n    for line in block.split('\\n'):\n        if line == '':\n            tmp = check_create_chunk(tmp)\n        elif line[0] == '*':\n            dst = line.split(' ')[2].rstrip('D')\n            tmp = check_create_chunk(tmp)\n        else:\n            (surface, attr) = line.split('\\t')\n            attr = attr.split(',')\n            lineDict = {\n                'surface': surface,\n                'base': attr[6],\n                'pos': attr[0],\n                'pos1': attr[1]\n            }\n            tmp.append(Morph(lineDict))\n\n    for i, r in enumerate(res):\n        res[int(r.dst)].srcs.append(i)\n    return res\n\n\nfilename = 'ch05/ai.ja.txt.cabocha'\nwith open(filename, mode='rt', encoding='utf-8') as f:\n    blocks = f.read().split('EOS\\n')\nblocks = list(filter(lambda x: x != '', blocks))\nblocks = [parse_cabocha(block) for block in blocks]\n\nwith open('ch05/ans47.txt', mode='w') as f:\n    for b in blocks:\n        for i, m in enumerate(b):\n            if 'サ変接続' in [s.pos1 for s in m.morphs] and 'を' in [s.surface for s in m.morphs] and i + 1 < len(b) and b[i + 1].morphs[0].pos == '動詞':\n                text = ''.join([s.surface for s in m.morphs]) + b[i + 1].morphs[0].base\n                if len(m.srcs) > 0:\n                    pre_morphs = [b[int(s)].morphs for s in m.srcs]\n                    pre_morphs_filtered = [list(filter(lambda x: '助詞' in x.pos, pm)) for pm in pre_morphs]\n                    pre_surface = [[p.surface for p in pm] for pm in pre_morphs_filtered]\n                    pre_surface = list(filter(lambda x: x != [], pre_surface))\n                    pre_surface = [p[0] for p in pre_surface]\n                    pre_text = list(filter(lambda x: '助詞' in [p.pos for p in x], pre_morphs))\n                    pre_text = [''.join([p.surface for p in pt]) for pt in pre_text]\n                    if len(pre_surface) > 0:\n                        f.writelines('\\t'.join([text, ' '.join(pre_surface), ' '.join(pre_text)]))\n                        f.write('\\n')"}], "task": "code_generation", "index": 602, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_603", "instruction": "pythonを用いて、文中のすべての名詞を含む文節に対し、その文節から構文木の根に至るパスを抽出せよ。 ただし、構文木上のパスは以下の仕様を満たすものとする。\n\n・各文節は（表層形の）形態素列で表現する\n・パスの開始文節から終了文節に至るまで、各文節の表現を” -> “で連結する\n\n「ジョン・マッカーシーはAIに関する最初の会議で人工知能という用語を作り出した。」という例文を考える。 CaboChaを係り受け解析に用いた場合、次のような出力が得られると思われる。\n\n```\nジョンマッカーシーは -> 作り出した\nAIに関する -> 最初の -> 会議で -> 作り出した\n最初の -> 会議で -> 作り出した\n会議で -> 作り出した\n人工知能という -> 用語を -> 作り出した\n用語を -> 作り出した\n```\n\nKNPを係り受け解析に用いた場合、次のような出力が得られると思われる。\n\n```\nジョンマッカーシーは -> 作り出した\nＡＩに -> 関する -> 会議で -> 作り出した\n会議で -> 作り出した\n人工知能と -> いう -> 用語を -> 作り出した\n用語を -> 作り出した\n```", "instances": [{"input": "", "output": "class Morph:\n    def __init__(self, dc):\n        self.surface = dc['surface']\n        self.base = dc['base']\n        self.pos = dc['pos']\n        self.pos1 = dc['pos1']\n\n\nclass Chunk:\n    def __init__(self, morphs, dst):\n        self.morphs = morphs    # 形態素（Morphオブジェクト）のリスト\n        self.dst = dst          # 係り先文節インデックス番号\n        self.srcs = []          # 係り元文節インデックス番号のリスト\n\n\ndef parse_cabocha(block):\n    def check_create_chunk(tmp):\n        if len(tmp) > 0:\n            c = Chunk(tmp, dst)\n            res.append(c)\n            tmp = []\n        return tmp\n\n    res = []\n    tmp = []\n    dst = None\n    for line in block.split('\\n'):\n        if line == '':\n            tmp = check_create_chunk(tmp)\n        elif line[0] == '*':\n            dst = line.split(' ')[2].rstrip('D')\n            tmp = check_create_chunk(tmp)\n        else:\n            (surface, attr) = line.split('\\t')\n            attr = attr.split(',')\n            lineDict = {\n                'surface': surface,\n                'base': attr[6],\n                'pos': attr[0],\n                'pos1': attr[1]\n            }\n            tmp.append(Morph(lineDict))\n\n    for i, r in enumerate(res):\n        res[int(r.dst)].srcs.append(i)\n    return res\n\n\nfilename = 'ch05/ai.ja.txt.cabocha'\nwith open(filename, mode='rt', encoding='utf-8') as f:\n    blocks = f.read().split('EOS\\n')\nblocks = list(filter(lambda x: x != '', blocks))\nblocks = [parse_cabocha(block) for block in blocks]\n\nfor b in blocks:\n    for m in b:\n        text = []\n        if '名詞' in [s.pos for s in m.morphs] and int(m.dst) != -1:\n            current_chunk = m\n            text.append(''.join([m.surface for m in current_chunk.morphs]))\n            next_chunk = b[int(current_chunk.dst)]\n            while int(current_chunk.dst) != -1:\n                text.append(''.join([m.surface for m in next_chunk.morphs]))\n                current_chunk = next_chunk\n                next_chunk = b[int(next_chunk.dst)]\n            print(*text, sep=' -> ')"}], "task": "code_generation", "index": 603, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_604", "instruction": "pythonを用いて、文中のすべての名詞句のペアを結ぶ最短係り受けパスを抽出せよ。ただし、名詞句ペアの文節番号がiとj（i<j）のとき、係り受けパスは以下の仕様を満たすものとする。\n\n・パスは開始文節から終了文節に至るまでの各文節の表現（表層形の形態素列）を” -> “で連結して表現する\n・文節iとjに含まれる名詞句はそれぞれ、XとYに置換する\nまた、係り受けパスの形状は、以下の2通りが考えられる。\n・文節iから構文木の根に至る経路上に文節jが存在する場合: 文節iから文節jのパスを表示\n・上記以外で、文節iと文節jから構文木の根に至る経路上で共通の文節kで交わる場合: 文節iから文節kに至る直前のパスと文節jから文節kに至る直前までのパス、文節kの内容を” | “で連結して表示\n\n「ジョン・マッカーシーはAIに関する最初の会議で人工知能という用語を作り出した。」という例文を考える。 CaboChaを係り受け解析に用いた場合、次のような出力が得られると思われる。\n\n```\nXは | Yに関する -> 最初の -> 会議で | 作り出した\nXは | Yの -> 会議で | 作り出した\nXは | Yで | 作り出した\nXは | Yという -> 用語を | 作り出した\nXは | Yを | 作り出した\nXに関する -> Yの\nXに関する -> 最初の -> Yで\nXに関する -> 最初の -> 会議で | Yという -> 用語を | 作り出した\nXに関する -> 最初の -> 会議で | Yを | 作り出した\nXの -> Yで\nXの -> 会議で | Yという -> 用語を | 作り出した\nXの -> 会議で | Yを | 作り出した\nXで | Yという -> 用語を | 作り出した\nXで | Yを | 作り出した\nXという -> Yを\n```\n\nKNPを係り受け解析に用いた場合、次のような出力が得られると思われる。\n\n```\nXは | Yに -> 関する -> 会議で | 作り出した。\nXは | Yで | 作り出した。\nXは | Yと -> いう -> 用語を | 作り出した。\nXは | Yを | 作り出した。\nXに -> 関する -> Yで\nXに -> 関する -> 会議で | Yと -> いう -> 用語を | 作り出した。\nXに -> 関する -> 会議で | Yを | 作り出した。\nXで | Yと -> いう -> 用語を | 作り出した。\nXで | Yを | 作り出した。\nXと -> いう -> Yを\n```", "instances": [{"input": "", "output": "文中のすべての名詞句のペアを結ぶ最短係り受けパスを抽出せよ．ただし，名詞句ペアの文節番号がi\nとj\n（i<j\n）のとき，係り受けパスは以下の仕様を満たすものとする．\n\n問題48と同様に，パスは開始文節から終了文節に至るまでの各文節の表現（表層形の形態素列）を” -> “で連結して表現する\n文節i\nとj\nに含まれる名詞句はそれぞれ，XとYに置換する\nまた，係り受けパスの形状は，以下の2通りが考えられる．\n\n文節i\nから構文木の根に至る経路上に文節j\nが存在する場合: 文節i\nから文節j\nのパスを表示\n上記以外で，文節i\nと文節j\nから構文木の根に至る経路上で共通の文節k\nで交わる場合: 文節i\nから文節k\nに至る直前のパスと文節j\nから文節k\nに至る直前までのパス，文節k\nの内容を” | “で連結して表示\n「ジョン・マッカーシーはAIに関する最初の会議で人工知能という用語を作り出した。」という例文を考える． CaboChaを係り受け解析に用いた場合，次のような出力が得られると思われる．\n\nXは | Yに関する -> 最初の -> 会議で | 作り出した\nXは | Yの -> 会議で | 作り出した\nXは | Yで | 作り出した\nXは | Yという -> 用語を | 作り出した\nXは | Yを | 作り出した\nXに関する -> Yの\nXに関する -> 最初の -> Yで\nXに関する -> 最初の -> 会議で | Yという -> 用語を | 作り出した\nXに関する -> 最初の -> 会議で | Yを | 作り出した\nXの -> Yで\nXの -> 会議で | Yという -> 用語を | 作り出した\nXの -> 会議で | Yを | 作り出した\nXで | Yという -> 用語を | 作り出した\nXで | Yを | 作り出した\nXという -> Yを\nKNPを係り受け解析に用いた場合，次のような出力が得られると思われる．\n\nXは | Yに -> 関する -> 会議で | 作り出した。\nXは | Yで | 作り出した。\nXは | Yと -> いう -> 用語を | 作り出した。\nXは | Yを | 作り出した。\nXに -> 関する -> Yで\nXに -> 関する -> 会議で | Yと -> いう -> 用語を | 作り出した。\nXに -> 関する -> 会議で | Yを | 作り出した。\nXで | Yと -> いう -> 用語を | 作り出した。\nXで | Yを | 作り出した。\nXと -> いう -> Yを"}], "task": "code_generation", "index": 604, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_605", "instruction": "pythonを用いて、News Aggregator Data Setをダウンロードし、以下の要領で学習データ（train.txt）、検証データ（valid.txt）、評価データ（test.txt）を作成せよ。\n\n1.ダウンロードしたzipファイルを解凍し、readme.txtの説明を読む。\n2.情報源（publisher）が”Reuters”, “Huffington Post”, “Businessweek”, “Contactmusic.com”, “Daily Mail”の事例（記事）のみを抽出する。\n3.抽出された事例をランダムに並び替える。\n4.抽出された事例の80%を学習データ、残りの10%ずつを検証データと評価データに分割し、それぞれtrain.txt、valid.txt、test.txtというファイル名で保存する。ファイルには、１行に１事例を書き出すこととし、カテゴリ名と記事見出しのタブ区切り形式とせよ（このファイルは後に問題70で再利用する）。\n\n学習データと評価データを作成したら、各カテゴリの事例数を確認せよ。", "instances": [{"input": "", "output": "import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n\nnewsCorpora = pd.read_table('ch06/NewsAggregatorDataset/newsCorpora.csv', header=None)\nnewsCorpora.columns = ['ID', 'TITLE', 'URL', 'PUBLISHER', 'CATEGORY', 'STORY', 'HOSTNAME', 'TIMESTAMP']\nnewsCorpora = newsCorpora[newsCorpora['PUBLISHER'].isin(\n    ['Reuters', 'Huffington Post', 'Businessweek', 'Contactmusic.com', 'Daily Mail'])].sample(frac=1, random_state=0)\n\nX = newsCorpora[['TITLE', 'CATEGORY']].copy()\nX['CATEGORY'] = X['CATEGORY'].map({'b': 0, 'e': 1, 't': 2, 'm': 3})\ny = newsCorpora['CATEGORY']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=0)\nX_valid, X_test, y_valid, y_test = train_test_split(X_test, y_test, test_size=0.5, stratify=y_test, random_state=0)\n\nX_train.to_csv('ch06/train2.txt', sep='\\t', index=False, header=None)\nX_valid.to_csv('ch06/valid2.txt', sep='\\t', index=False, header=None)\nX_test.to_csv('ch06/test2.txt', sep='\\t', index=False, header=None)"}], "task": "code_generation", "index": 605, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_606", "instruction": "pythonを用いて、学習データ、検証データ、評価データから特徴量を抽出し、それぞれtrain.feature.txt、valid.feature.txt、test.feature.txtというファイル名で保存せよ。 なお、カテゴリ分類に有用そうな特徴量は各自で自由に設計せよ。記事の見出しを単語列に変換したものが最低限のベースラインとなるであろう。", "instances": [{"input": "", "output": "import joblib\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n\nX_train = pd.read_table('ch06/train.txt', header=None)\nX_valid = pd.read_table('ch06/valid.txt', header=None)\nX_test = pd.read_table('ch06/test.txt', header=None)\nuse_cols = ['TITLE', 'CATEGORY']\nX_train.columns = use_cols\nX_valid.columns = use_cols\nX_test.columns = use_cols\nX_train['TMP'] = 'train'\nX_valid['TMP'] = 'valid'\nX_test['TMP'] = 'test'\n\ndata = pd.concat([X_train, X_valid, X_test]).reset_index(drop=True)\nvectorizer = CountVectorizer(token_pattern=u'(?u)\\\\b\\\\w+\\\\b')\nbag = vectorizer.fit_transform(data['TITLE'])\ndata = pd.concat([data, pd.DataFrame(bag.toarray())], axis=1)\n\njoblib.dump(vectorizer.vocabulary_, 'ch06/vocabulary_.joblib')\n\nX_train = data.query('TMP==\"train\"').drop(use_cols + ['TMP'], axis=1)\nX_valid = data.query('TMP==\"valid\"').drop(use_cols + ['TMP'], axis=1)\nX_test = data.query('TMP==\"test\"').drop(use_cols + ['TMP'], axis=1)\n\nX_train.to_csv('ch06/train.feature.txt', sep='\\t', index=False, header=None)\nX_valid.to_csv('ch06/valid.feature.txt', sep='\\t', index=False, header=None)\nX_test.to_csv('ch06/test.feature.txt', sep='\\t', index=False, header=None)"}], "task": "code_generation", "index": 606, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_607", "instruction": "pythonを用いて、学習データを用いて、ロジスティック回帰モデルを学習せよ。", "instances": [{"input": "", "output": "import pandas as pd\nimport joblib\nfrom sklearn.linear_model import LogisticRegression\n\n\nX_train = pd.read_table('ch06/train.feature.txt', header=None)\ny_train = pd.read_table('ch06/train.txt', header=None)[1]\n\nclf = LogisticRegression(penalty='l2', solver='sag', random_state=0)\nclf.fit(X_train, y_train)\njoblib.dump(clf, 'ch06/model.joblib')"}], "task": "code_generation", "index": 607, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_608", "instruction": "pythonを用いて、学習データから作成したロジスティック回帰モデルを用い、与えられた記事見出しからカテゴリとその予測確率を計算するプログラムを実装せよ。", "instances": [{"input": "", "output": "import pandas as pd\nfrom sklearn.linear_model import LogisticRegression\n\n\nX_train = pd.read_table('ch06/train.feature.txt', header=None)\ny_train = pd.read_table('ch06/train.txt', header=None)[1]\n\nclf = LogisticRegression(penalty='l2', solver='sag', random_state=0)\nclf.fit(X_train, y_train)\ny_train = clf.predict(X_train)"}], "task": "code_generation", "index": 608, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_609", "instruction": "pythonを用いて、学習データから作成したロジスティック回帰モデルの正解率を、学習データおよび評価データ上で計測せよ。", "instances": [{"input": "", "output": "import pandas as pd\nimport joblib\nfrom sklearn.metrics import accuracy_score\n\n\nX_train = pd.read_table('ch06/train.feature.txt', header=None)\nX_test = pd.read_table('ch06/test.feature.txt', header=None)\ny_train = pd.read_table('ch06/train.txt', header=None)[1]\ny_test = pd.read_table('ch06/test.txt', header=None)[1]\n\nclf = joblib.load('ch06/model.joblib')\n\nprint(f'train acc: {accuracy_score(y_train, clf.predict(X_train))}')\nprint(f'test acc: {accuracy_score(y_test, clf.predict(X_test))}')"}], "task": "code_generation", "index": 609, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_610", "instruction": "pythonを用いて、学習データから作成したロジスティック回帰モデルの混同行列（confusion matrix）を、学習データおよび評価データ上で作成せよ。", "instances": [{"input": "", "output": "import pandas as pd\nimport joblib\nfrom sklearn.metrics import confusion_matrix\n\n\nX_train = pd.read_table('ch06/train.feature.txt', header=None)\nX_test = pd.read_table('ch06/test.feature.txt', header=None)\ny_train = pd.read_table('ch06/train.txt', header=None)[1]\ny_test = pd.read_table('ch06/test.txt', header=None)[1]\n\nclf = joblib.load('ch06/model.joblib')\n\nprint(f'train confusion matrix:\\n {confusion_matrix(y_train, clf.predict(X_train))}')\nprint(f'test confusion matrix:\\n {confusion_matrix(y_test, clf.predict(X_test))}')"}], "task": "code_generation", "index": 610, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_611", "instruction": "pythonを用いて、学習データから作成したロジスティック回帰モデルの適合率、再現率、F1スコアを、評価データ上で計測せよ。カテゴリごとに適合率、再現率、F1スコアを求め、カテゴリごとの性能をマイクロ平均（micro-average）とマクロ平均（macro-average）で統合せよ。", "instances": [{"input": "", "output": "import pandas as pd\nimport joblib\nfrom sklearn.metrics import recall_score, precision_score, f1_score\n\n\nX_train = pd.read_table('ch06/train.feature.txt', header=None)\nX_test = pd.read_table('ch06/test.feature.txt', header=None)\ny_train = pd.read_table('ch06/train.txt', header=None)[1]\ny_test = pd.read_table('ch06/test.txt', header=None)[1]\n\nclf = joblib.load('ch06/model.joblib')\ny_pred = clf.predict(X_test)\n\nprint(f'test recall of None: {recall_score(y_test, y_pred, average=None)}')\nprint(f'test recall of micro: {recall_score(y_test, y_pred, average=\"micro\")}')\nprint(f'test recall of macro: {recall_score(y_test, y_pred, average=\"macro\")}')\nprint(f'test precision of None: {precision_score(y_test, y_pred, average=None)}')\nprint(f'test precision of micro: {precision_score(y_test, y_pred, average=\"micro\")}')\nprint(f'test precision of macro: {precision_score(y_test, y_pred, average=\"macro\")}')\nprint(f'test f1 of None: {f1_score(y_test, y_pred, average=None)}')\nprint(f'test f1 of micro: {f1_score(y_test, y_pred, average=\"micro\")}')\nprint(f'test f1 of macro: {f1_score(y_test, y_pred, average=\"macro\")}')"}], "task": "code_generation", "index": 611, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_612", "instruction": "pythonを用いて、学習データから作成したロジスティック回帰モデルの中で、重みの高い特徴量トップ10と、重みの低い特徴量トップ10を確認せよ。", "instances": [{"input": "", "output": "import joblib\n\n\nclf = joblib.load('ch06/model.joblib')\nvocabulary_ = joblib.load('ch06/vocabulary_.joblib')\ncoefs = clf.coef_\n\nfor c in coefs:\n    d = dict(zip(vocabulary_, c))\n    d_top = sorted(d.items(), key=lambda x: abs(x[1]), reverse=True)[:10]\n    print(d_top)\n    d_bottom = sorted(d.items(), key=lambda x: abs(x[1]), reverse=False)[:10]\n    print(d_bottom)"}], "task": "code_generation", "index": 612, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_613", "instruction": "pythonを用いて、ロジスティック回帰モデルを学習するとき、正則化パラメータを調整することで、学習時の過学習（overfitting）の度合いを制御できる。異なる正則化パラメータでロジスティック回帰モデルを学習し、学習データ、検証データ、および評価データ上の正解率を求めよ。実験の結果は、正則化パラメータを横軸、正解率を縦軸としたグラフにまとめよ。", "instances": [{"input": "", "output": "import matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\n\nX_train = pd.read_table('ch06/train.feature.txt', header=None)\nX_valid = pd.read_table('ch06/valid.feature.txt', header=None)\nX_test = pd.read_table('ch06/test.feature.txt', header=None)\ny_train = pd.read_table('ch06/train.txt', header=None)[1]\ny_valid = pd.read_table('ch06/valid.txt', header=None)[1]\ny_test = pd.read_table('ch06/test.txt', header=None)[1]\n\nC_candidate = [0.1, 1.0, 10, 100]\ntrain_acc = []\nvalid_acc = []\ntest_acc = []\n\nfor c in C_candidate:\n    clf = LogisticRegression(penalty='l2', solver='sag', random_state=0, C=c)\n    clf.fit(X_train, y_train)\n    train_acc.append(accuracy_score(y_train, clf.predict(X_train)))\n    valid_acc.append(accuracy_score(y_valid, clf.predict(X_valid)))\n    test_acc.append(accuracy_score(y_test, clf.predict(X_test)))\n\nplt.plot(C_candidate, train_acc, label='train acc')\nplt.plot(C_candidate, valid_acc, label='valid acc')\nplt.plot(C_candidate, test_acc, label='test acc')\nplt.legend()\nplt.savefig('ch06/ans58.png')"}], "task": "code_generation", "index": 613, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_614", "instruction": "pythonを用いて、学習アルゴリズムや学習パラメータを変えながら、カテゴリ分類モデルを学習せよ。検証データ上の正解率が最も高くなる学習アルゴリズム・パラメータを求めよ。また、その学習アルゴリズム・パラメータを用いたときの評価データ上の正解率を求めよ。", "instances": [{"input": "", "output": "import pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\n\nX_train = pd.read_table('ch06/train.feature.txt', header=None)\nX_valid = pd.read_table('ch06/valid.feature.txt', header=None)\nX_test = pd.read_table('ch06/test.feature.txt', header=None)\ny_train = pd.read_table('ch06/train.txt', header=None)[1]\ny_valid = pd.read_table('ch06/valid.txt', header=None)[1]\ny_test = pd.read_table('ch06/test.txt', header=None)[1]\n\ntest_acc = []\n\nC_candidate = [0.1, 1.0, 10, 100]\nfor c in C_candidate:\n    clf = LogisticRegression(penalty='l2', solver='sag', random_state=0, C=c)\n    clf.fit(X_train, y_train)\n    test_acc.append(accuracy_score(y_test, clf.predict(X_test)))\n\n\nmax_depth_candidate = [2, 4, 8, 16]\nfor m in max_depth_candidate:\n    clf = RandomForestClassifier(max_depth=m, random_state=0)\n    clf.fit(X_train, y_train)\n    test_acc.append(accuracy_score(y_test, clf.predict(X_test)))\n\nbestIndex = test_acc.index(max(test_acc))\nif bestIndex < 4:\n    bestAlg = 'LogisticRegression'\n    bestParam = f'C={C_candidate[bestIndex]}'\nelse:\n    bestAlg = 'RandomForestClassifier'\n    bestParam = f'max_depth={max_depth_candidate[bestIndex - 4]}'\n\nprint(bestAlg, bestParam)"}], "task": "code_generation", "index": 614, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_615", "instruction": "pythonを用いて、Toggle menu\n第7章: 単語ベクトル\n On this page\n60. 単語ベクトルの読み込みと表示\n61. 単語の類似度\n62. 類似度の高い単語10件\n63. 加法構成性によるアナロジー\n64. アナロジーデータでの実験\n65. アナロジータスクでの正解率\n66. WordSimilarity-353での評価\n67. k-meansクラスタリング\n68. Ward法によるクラスタリング\n69. t-SNEによる可視化\n単語の意味を実ベクトルで表現する単語ベクトル（単語埋め込み）に関して、以下の処理を行うプログラムを作成せよ。\n\n60. 単語ベクトルの読み込みと表示Permalink\nGoogle Newsデータセット（約1,000億単語）での学習済み単語ベクトル（300万単語・フレーズ、300次元）をダウンロードし、”United States”の単語ベクトルを表示せよ。ただし、”United States”は内部的には”United_States”と表現されていることに注意せよ。", "instances": [{"input": "", "output": "from gensim.models import KeyedVectors\n\n\nmodel = KeyedVectors.load_word2vec_format('ch07/GoogleNews-vectors-negative300.bin', binary=True)\nus = model['United_States']\nprint(us)"}], "task": "code_generation", "index": 615, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_616", "instruction": "pythonを用いて、“United States”と”U.S.”のコサイン類似度を計算せよ。", "instances": [{"input": "", "output": "import numpy as np\nfrom gensim.models import KeyedVectors\n\n\ndef cosSim(v1, v2):\n    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n\n\nmodel = KeyedVectors.load_word2vec_format('ch07/GoogleNews-vectors-negative300.bin', binary=True)\nus1 = model['United_States']\nus2 = model['U.S.']\nprint(cosSim(us1, us2))"}], "task": "code_generation", "index": 616, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_617", "instruction": "pythonを用いて、“United States”とコサイン類似度が高い10語と、その類似度を出力せよ。", "instances": [{"input": "", "output": "from gensim.models import KeyedVectors\n\n\nmodel = KeyedVectors.load_word2vec_format('ch07/GoogleNews-vectors-negative300.bin', binary=True)\nresult = model.most_similar(positive=['United_States'])\n\nfor i in range(10):\n    print(\"{}: {:.4f}\".format(*result[i]))"}], "task": "code_generation", "index": 617, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_618", "instruction": "pythonを用いて、“Spain”の単語ベクトルから”Madrid”のベクトルを引き、”Athens”のベクトルを足したベクトルを計算し、そのベクトルと類似度の高い10語とその類似度を出力せよ。", "instances": [{"input": "", "output": "import numpy as np\nfrom gensim.models import KeyedVectors\n\n\ndef cosSim(v1, v2):\n    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n\n\nmodel = KeyedVectors.load_word2vec_format('ch07/GoogleNews-vectors-negative300.bin', binary=True)\nresult = model.most_similar(positive=['Spain', 'Athens'], negative=['Madrid'])\n\nfor i in range(10):\n    print(\"{}: {:.4f}\".format(*result[i]))"}], "task": "code_generation", "index": 618, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_619", "instruction": "pythonを用いて、単語アナロジーの評価データをダウンロードし、vec(2列目の単語) - vec(1列目の単語) + vec(3列目の単語)を計算し、そのベクトルと類似度が最も高い単語と、その類似度を求めよ。求めた単語と類似度は、各事例の末尾に追記せよ。", "instances": [{"input": "", "output": "import pandas as pd\nfrom gensim.models import KeyedVectors\nfrom tqdm import tqdm\n\n\ndef culcSim(row):\n    global model\n    return pd.Series(list(model.most_similar(positive=[row['v2'], row['v3']], negative=[row['v1']])[0]))\n\n\ntqdm.pandas()\ndf = pd.read_csv('ch07/questions-words.txt', sep=' ')\ndf = df.reset_index()\ndf.columns = ['v1', 'v2', 'v3', 'v4']\ndf.dropna(inplace=True)\n\nmodel = KeyedVectors.load_word2vec_format('ch07/GoogleNews-vectors-negative300.bin', binary=True)\ndf[['simWord', 'simScore']] = df.progress_apply(culcSim, axis=1)\ndf.to_csv('ch07/ans64.txt', sep=' ', index=False, header=None)"}], "task": "code_generation", "index": 619, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_620", "instruction": "pythonを用いて、単語アナロジーの評価データをダウンロードし、vec(2列目の単語) - vec(1列目の単語) + vec(3列目の単語)を計算し、そのベクトルと類似度が最も高い単語と、その類似度を求めよ。求めた単語と類似度は、各事例の末尾に追記し、この実行結果を用い、意味的アナロジー（semantic analogy）と文法的アナロジー（syntactic analogy）の正解率を測定せよ。", "instances": [{"input": "", "output": "import pandas as pd\nfrom gensim.models import KeyedVectors\nfrom tqdm import tqdm\n\n\ndef culcSim(row):\n    global model\n    return pd.Series(list(model.most_similar(positive=[row['v2'], row['v3']], negative=[row['v1']])[0]))\n\n\ntqdm.pandas()\ndf = pd.read_csv('ch07/questions-words.txt', sep=' ')\ndf = df.reset_index()\ndf.columns = ['v1', 'v2', 'v3', 'v4']\ndf.dropna(inplace=True)\n\nmodel = KeyedVectors.load_word2vec_format('ch07/GoogleNews-vectors-negative300.bin', binary=True)\ndf[['simWord', 'simScore']] = df.progress_apply(culcSim, axis=1)\ndf.to_csv('ch07/ans64.txt', sep=' ', index=False, header=None)\n\n\ndf = pd.read_csv('ch07/ans64.txt', sep=' ', header=None)\nprint((df[3] == df[4]).sum() / len(df))"}], "task": "code_generation", "index": 620, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_621", "instruction": "pythonを用いて、The WordSimilarity-353 Test Collectionの評価データをダウンロードし、単語ベクトルにより計算される類似度のランキングと、人間の類似度判定のランキングの間のスピアマン相関係数を計算せよ。", "instances": [{"input": "", "output": "import pandas as pd\nimport numpy as np\nfrom gensim.models import KeyedVectors\nfrom tqdm import tqdm\n\n\ndef cosSim(v1, v2):\n    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n\n\ndef culcCosSim(row):\n    global model\n    w1v = model[row['Word 1']]\n    w2v = model[row['Word 2']]\n    return cosSim(w1v, w2v)\n\n\ntqdm.pandas()\nmodel = KeyedVectors.load_word2vec_format('ch07/GoogleNews-vectors-negative300.bin', binary=True)\ndf = pd.read_csv('ch07/wordsim353/combined.csv')\ndf['cosSim'] = df.progress_apply(culcCosSim, axis=1)\n\nprint(df[['Human (mean)', 'cosSim']].corr(method='spearman'))"}], "task": "code_generation", "index": 621, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_622", "instruction": "pythonを用いて、国名に関する単語ベクトルを抽出し、k-meansクラスタリングをクラスタ数k=5として実行せよ。", "instances": [{"input": "", "output": "import pandas as pd\nimport numpy as np\nfrom gensim.models import KeyedVectors\nfrom sklearn.cluster import KMeans\n\n\n# http://www.fao.org/countryprofiles/iso3list/en/\ncountry = pd.read_table('ch07/countries.tsv')\ncountry = country['Short name'].values\n\nmodel = KeyedVectors.load_word2vec_format('ch07/GoogleNews-vectors-negative300.bin', binary=True)\n\ncountryVec = []\ncountryName = []\nfor c in country:\n    if c in model.vocab:\n        countryVec.append(model[c])\n        countryName.append(c)\n\nX = np.array(countryVec)\nkm = KMeans(n_clusters=5, random_state=0)\ny_km = km.fit_predict(X)\nprint(y_km)"}], "task": "code_generation", "index": 622, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_623", "instruction": "pythonを用いて、国名に関する単語ベクトルに対し、Ward法による階層型クラスタリングを実行せよ。さらに、クラスタリング結果をデンドログラムとして可視化せよ。", "instances": [{"input": "", "output": "import pandas as pd\nimport numpy as np\nfrom gensim.models import KeyedVectors\nimport matplotlib.pyplot as plt\nfrom scipy.cluster.hierarchy import linkage, dendrogram\n\n\n# http://www.fao.org/countryprofiles/iso3list/en/\ncountry = pd.read_table('ch07/countries.tsv')\ncountry = country['Short name'].values\n\nmodel = KeyedVectors.load_word2vec_format('ch07/GoogleNews-vectors-negative300.bin', binary=True)\n\ncountryVec = []\ncountryName = []\nfor c in country:\n    if c in model.vocab:\n        countryVec.append(model[c])\n        countryName.append(c)\n\nX = np.array(countryVec)\nlinkage_result = linkage(X, method='ward', metric='euclidean')\nplt.figure(num=None, figsize=(16, 9), dpi=200, facecolor='w', edgecolor='k')\ndendrogram(linkage_result, labels=countryName)\nplt.show()"}], "task": "code_generation", "index": 623, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_624", "instruction": "pythonを用いて、ベクトル空間上の国名に関する単語ベクトルをt-SNEで可視化せよ。", "instances": [{"input": "", "output": "import pandas as pd\nimport numpy as np\nfrom gensim.models import KeyedVectors\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n\n\n# http://www.fao.org/countryprofiles/iso3list/en/\ncountry = pd.read_table('ch07/countries.tsv')\ncountry = country['Short name'].values\n\nmodel = KeyedVectors.load_word2vec_format('ch07/GoogleNews-vectors-negative300.bin', binary=True)\n\ncountryVec = []\ncountryName = []\nfor c in country:\n    if c in model.vocab:\n        countryVec.append(model[c])\n        countryName.append(c)\n\nX = np.array(countryVec)\ntsne = TSNE(random_state=0, n_iter=15000, metric='cosine')\nembs = tsne.fit_transform(X)\nplt.scatter(embs[:, 0], embs[:, 1])\nplt.show()"}], "task": "code_generation", "index": 624, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_625", "instruction": "pythonを用いて、手持ちの学習データ、検証データ、評価データを行列・ベクトルに変換したい。例えば、学習データについて、すべての事例$$x_i$$の特徴ベクトル$$\\boldsymbol{x}_i$$を並べた行列$$X$$と、正解ラベルを並べた行列（ベクトル）$$Y$$を作成したい。\n\n$$\nX = \\begin{pmatrix} \n  \\boldsymbol{x}_1 \\\\ \n  \\boldsymbol{x}_2 \\\\ \n  \\dots \\\\ \n  \\boldsymbol{x}_n \\\\ \n\\end{pmatrix} \\in \\mathbb{R}^{n \\times d},\nY = \\begin{pmatrix} \n  y_1 \\\\ \n  y_2 \\\\ \n  \\dots \\\\ \n  y_n \\\\ \n\\end{pmatrix} \\in \\mathbb{N}^{n}\n$$\n\nここで、$$n$$は学習データの事例数であり、$$\\boldsymbol{x}_i \\in \\mathbb{R}^d$$と$$y_i \\in \\mathbb{N}$$はそれぞれ、$$i \\in \\{1, \\dots, n\\}$$番目の事例の特徴量ベクトルと正解ラベルを表す。\nなお、今回は「ビジネス」「科学技術」「エンターテイメント」「健康」の4カテゴリ分類である。$$\\mathbb{N}_{<4}$$で$$4$$未満の自然数（$$0$$を含む）を表すことにすれば、任意の事例の正解ラベル$$y_i$$は$$y_i \\in \\mathbb{N}_{<4}$$で表現できる。\n以降では、ラベルの種類数を$$L$$で表す（今回の分類タスクでは$$L=4$$である）。\n\n$$i$$番目の事例の特徴ベクトル$$\\boldsymbol{x}_i$$は、次式で求める。\n\n$$\n\\boldsymbol{x}_i = \\frac{1}{T_i} \\sum_{t=1}^{T_i} \\mathrm{emb}(w_{i,t})\n$$\n\nここで、$$i$$番目の事例は$$T_i$$個の（記事見出しの）単語列$$(w_{i,1}, w_{i,2}, \\dots, w_{i,T_i})$$から構成され、$$\\mathrm{emb}(w) \\in \\mathbb{R}^d$$は単語$$w$$に対応する単語ベクトル（次元数は$$d$$）である。すなわち、$$i$$番目の事例の記事見出しを、その見出しに含まれる単語のベクトルの平均で表現したものが$$\\boldsymbol{x}_i$$である。今回は単語ベクトルとして、問題60でダウンロードしたものを用いればよい。$$300$$次元の単語ベクトルを用いたので、$$d=300$$である。\n\n$$i$$番目の事例のラベル$$y_i$$は、次のように定義する。\n\n$$\ny_i = \\begin{cases}\n0 & (\\mbox{記事}x_i\\mbox{が「ビジネス」カテゴリの場合}) \\\\\n1 & (\\mbox{記事}x_i\\mbox{が「科学技術」カテゴリの場合}) \\\\\n2 & (\\mbox{記事}x_i\\mbox{が「エンターテイメント」カテゴリの場合}) \\\\\n3 & (\\mbox{記事}x_i\\mbox{が「健康」カテゴリの場合}) \\\\\n\\end{cases}\n$$\n\nなお、カテゴリ名とラベルの番号が一対一で対応付いていれば、上式の通りの対応付けでなくてもよい。\n\n以上の仕様に基づき、以下の行列・ベクトルを作成し、ファイルに保存せよ。\n\n+ 学習データの特徴量行列: $$X_{\\rm train} \\in \\mathbb{R}^{N_t \\times d}$$\n+ 学習データのラベルベクトル: $$Y_{\\rm train} \\in \\mathbb{N}^{N_t}$$\n+ 検証データの特徴量行列: $$X_{\\rm valid} \\in \\mathbb{R}^{N_v \\times d}$$\n+ 検証データのラベルベクトル: $$Y_{\\rm valid} \\in \\mathbb{N}^{N_v}$$\n+ 評価データの特徴量行列: $$X_{\\rm test} \\in \\mathbb{R}^{N_e \\times d}$$\n+ 評価データのラベルベクトル: $$Y_{\\rm test} \\in \\mathbb{N}^{N_e}$$\n\nなお、$$N_t, N_v, N_e$$はそれぞれ、学習データの事例数、検証データの事例数、評価データの事例数である。\n\nここで、nは学習データの事例数であり、xi∈Rdとyi∈Nはそれぞれ、i∈{1,…,n}番目の事例の特徴量ベクトルと正解ラベルを表す。 なお、今回は「ビジネス」「科学技術」「エンターテイメント」「健康」の4カテゴリ分類である。N<4で4未満の自然数（0を含む）を表すことにすれば、任意の事例の正解ラベルyiはyi∈N<4で表現できる。 以降では、ラベルの種類数をL\nで表す（今回の分類タスクではL=4である）。\n\ni番目の事例はTi個の（記事見出しの）単語列(wi,1,wi,2,…,wi,Ti)から構成され、emb(w)∈Rd\nは単語wに対応する単語ベクトル（次元数はd）である。すなわち、i番目の事例の記事見出しを、その見出しに含まれる単語のベクトルの平均で表現したものがxiである。今回は単語ベクトルとして、問題60でダウンロードしたものを用いればよい。300次元の単語ベクトルを用いたので、d=300である。\n\ni 番目の事例のラベルyiは、次のように定義する。\n\n0(記事xiが「ビジネス」カテゴリの場合)\n1(記事xiが「科学技術」カテゴリの場合)\n2(記事xiが「エンターテイメント」カテゴリの場合)\n3(記事xiが「健康」カテゴリの場合)\n\nなお、カテゴリ名とラベルの番号が一対一で対応付いていれば、上式の通りの対応付けでなくてもよい。\n\n以下の行列・ベクトルを作成し、ファイルに保存せよ。\n\n学習データの特徴量行列、学習データのラベルベクトル、検証データの特徴量行列、検証データのラベルベクトル、評価データの特徴量行列、評価データのラベルベクトル\n\n+ 学習データの特徴量行列: $$X_{\\rm train} \\in \\mathbb{R}^{N_t \\times d}$$\n+ 学習データのラベルベクトル: $$Y_{\\rm train} \\in \\mathbb{N}^{N_t}$$\n+ 検証データの特徴量行列: $$X_{\\rm valid} \\in \\mathbb{R}^{N_v \\times d}$$\n+ 検証データのラベルベクトル: $$Y_{\\rm valid} \\in \\mathbb{N}^{N_v}$$\n+ 評価データの特徴量行列: $$X_{\\rm test} \\in \\mathbb{R}^{N_e \\times d}$$\n+ 評価データのラベルベクトル: $$Y_{\\rm test} \\in \\mathbb{N}^{N_e}$$", "instances": [{"input": "", "output": "import joblib\nimport numpy as np\nimport pandas as pd\nfrom gensim.models import KeyedVectors\nfrom tqdm import tqdm\n\n\ndef culcSwem(row):\n    global model\n    swem = [model[w] if w in model.vocab else np.zeros(shape=(model.vector_size,)) for w in row['TITLE'].split()]\n    swem = np.mean(np.array(swem), axis=0)\n    return swem\n\n\nX_train = pd.read_table('ch06/train.txt', header=None)\nX_valid = pd.read_table('ch06/valid.txt', header=None)\nX_test = pd.read_table('ch06/test.txt', header=None)\nuse_cols = ['TITLE', 'CATEGORY']\nn_train = len(X_train)\nn_valid = len(X_valid)\nn_test = len(X_test)\nX_train.columns = use_cols\nX_valid.columns = use_cols\nX_test.columns = use_cols\n\ndata = pd.concat([X_train, X_valid, X_test]).reset_index(drop=True)\n\ntqdm.pandas()\nmodel = KeyedVectors.load_word2vec_format('ch07/GoogleNews-vectors-negative300.bin', binary=True)\nswemVec = data.progress_apply(culcSwem, axis=1)\n\nX_train = np.array(list(swemVec.values)[:n_train])\nX_valid = np.array(list(swemVec.values)[n_train:n_train + n_valid])\nX_test = np.array(list(swemVec.values)[n_train + n_valid:])\njoblib.dump(X_train, 'ch08/X_train.joblib')\njoblib.dump(X_valid, 'ch08/X_valid.joblib')\njoblib.dump(X_test, 'ch08/X_test.joblib')\n\ny_data = data['CATEGORY'].map({'b': 0, 'e': 1, 't': 2, 'm': 3})\n\ny_train = y_data.values[:n_train]\ny_valid = y_data.values[n_train:n_train + n_valid]\ny_test = y_data.values[n_train + n_valid:]\n\njoblib.dump(y_train, 'ch08/y_train.joblib')\njoblib.dump(y_valid, 'ch08/y_valid.joblib')\njoblib.dump(y_test, 'ch08/y_test.joblib')"}], "task": "code_generation", "index": 625, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_626", "instruction": "pythonを用いて、以下の行列を読み込み、学習データについて以下の計算を実行せよ。\n\n$$\n\\hat{\\boldsymbol{y}}_1 = {\\rm softmax}(\\boldsymbol{x}_1 W), \\\\\n\\hat{Y} = {\\rm softmax}(X_{[1:4]} W)\n$$\n\nただし、$${\\rm softmax}$$はソフトマックス関数、$$X_{[1:4]} \\in \\mathbb{R}^{4 \\times d}$$は特徴ベクトル$$\\boldsymbol{x}_1, \\boldsymbol{x}_2, \\boldsymbol{x}_3, \\boldsymbol{x}_4$$を縦に並べた行列である。\n\n$$\nX_{[1:4]} = \\begin{pmatrix} \n  \\boldsymbol{x}_1 \\\\ \n  \\boldsymbol{x}_2 \\\\ \n  \\boldsymbol{x}_3 \\\\ \n  \\boldsymbol{x}_4 \\\\ \n\\end{pmatrix}\n$$\n\n行列$$W \\in \\mathbb{R}^{d \\times L}$$は単層ニューラルネットワークの重み行列で、ここではランダムな値で初期化すればよい（問題73以降で学習して求める）。なお、$$\\hat{\\boldsymbol{y}}_1 \\in \\mathbb{R}^L$$は未学習の行列$$W$$で事例$$x_1$$を分類したときに、各カテゴリに属する確率を表すベクトルである。\n同様に、$$\\hat{Y} \\in \\mathbb{R}^{n \\times L}$$は、学習データの事例$$x_1, x_2, x_3, x_4$$について、各カテゴリに属する確率を行列として表現している。\n\n+ 学習データの特徴量行列: $$X_{\\rm train} \\in \\mathbb{R}^{N_t \\times d}$$\n+ 学習データのラベルベクトル: $$Y_{\\rm train} \\in \\mathbb{N}^{N_t}$$\n+ 検証データの特徴量行列: $$X_{\\rm valid} \\in \\mathbb{R}^{N_v \\times d}$$\n+ 検証データのラベルベクトル: $$Y_{\\rm valid} \\in \\mathbb{N}^{N_v}$$\n+ 評価データの特徴量行列: $$X_{\\rm test} \\in \\mathbb{R}^{N_e \\times d}$$\n+ 評価データのラベルベクトル: $$Y_{\\rm test} \\in \\mathbb{N}^{N_e}$$", "instances": [{"input": "", "output": "import joblib\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\n\nX_train = joblib.load('ch08/X_train.joblib')\nX_train = torch.from_numpy(X_train.astype(np.float32)).clone()\n\nX = X_train[0:4]\n\nnet = nn.Sequential(nn.Linear(X.size()[1], 4), nn.Softmax(1))\ny_pred = net(X)\nprint(y_pred)"}], "task": "code_generation", "index": 626, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_627", "instruction": "学習データの事例$$x_1$$と事例集合$$x_1, x_2, x_3, x_4$$に対して、クロスエントロピー損失と、行列$$W$$に対する勾配を計算せよ。なお、ある事例$$x_i$$に対して損失は次式で計算される。\n\n$$\nl_i = - \\log [\\mbox{事例}x_i\\mbox{が}y_i\\mbox{に分類される確率}]\n$$\n\nただし、事例集合に対するクロスエントロピー損失は、その集合に含まれる各事例の損失の平均とする。", "instances": [{"input": "", "output": "import joblib\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\n\nX_train = joblib.load('ch08/X_train.joblib')\nX_train = torch.from_numpy(X_train.astype(np.float32)).clone()\n\nX = X_train[0:4]\n\nnet = nn.Sequential(nn.Linear(X.size()[1], 4), nn.Softmax(1))\ny_pred = net(X)\nprint(y_pred)"}], "task": "code_generation", "index": 627, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_628", "instruction": "pythonを用いて、確率的勾配降下法（SGD: Stochastic Gradient Descent）を用いて、行列W\nを学習せよ。なお、学習は適当な基準で終了させればよい（例えば「100エポックで終了」など）。", "instances": [{"input": "", "output": "import joblib\nimport numpy as np\nimport torch\nfrom torch import nn, optim\n\n\nX_train = joblib.load('ch08/X_train.joblib')\ny_train = joblib.load('ch08/y_train.joblib')\nX_train = torch.from_numpy(X_train.astype(np.float32)).clone()\ny_train = torch.from_numpy(y_train.astype(np.int64)).clone()\n\nX = X_train[0:4]\ny = y_train[0:4]\n\nnet = nn.Linear(X.size()[1], 4)\nloss_fn = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.01)\n\nlosses = []\n\nfor epoc in range(100):\n    optimizer.zero_grad()\n    y_pred = net(X)\n    loss = loss_fn(y_pred, y)\n    loss.backward()\n    optimizer.step()\n    losses.append(loss)\n\nprint(net.state_dict()['weight'])"}], "task": "code_generation", "index": 628, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_629", "instruction": "pythonを用いて、確率的勾配降下法（SGD: Stochastic Gradient Descent）を用いて、行列W\nを学習し、求めた行列を用いて学習データおよび評価データの事例を分類したとき、その正解率をそれぞれ求めよ。", "instances": [{"input": "", "output": "import joblib\nimport numpy as np\nimport torch\nfrom torch import nn, optim\n\n\nX_train = joblib.load('ch08/X_train.joblib')\ny_train = joblib.load('ch08/y_train.joblib')\nX_train = torch.from_numpy(X_train.astype(np.float32)).clone()\ny_train = torch.from_numpy(y_train.astype(np.int64)).clone()\n\nX_test = joblib.load('ch08/X_test.joblib')\ny_test = joblib.load('ch08/y_test.joblib')\nX_test = torch.from_numpy(X_test.astype(np.float32)).clone()\ny_test = torch.from_numpy(y_test.astype(np.int64)).clone()\n\nX = X_train[0:4]\ny = y_train[0:4]\n\nnet = nn.Linear(X.size()[1], 4)\nloss_fn = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.01)\n\nlosses = []\n\nfor epoc in range(100):\n    optimizer.zero_grad()\n    y_pred = net(X)\n    loss = loss_fn(y_pred, y)\n    loss.backward()\n    optimizer.step()\n    losses.append(loss)\n\n_, y_pred_train = torch.max(net(X), 1)\nprint((y_pred_train == y).sum().item() / len(y))\n\n_, y_pred_test = torch.max(net(X_test), 1)\nprint((y_pred_test == y_test).sum().item() / len(y_test))"}], "task": "code_generation", "index": 629, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_630", "instruction": "pythonを用いて、以下のコードを改変し、各エポックのパラメータ更新が完了するたびに、訓練データでの損失、正解率、検証データでの損失、正解率をグラフにプロットし、学習の進捗状況を確認できるようにせよ。\n\nimport joblib\nimport numpy as np\nimport torch\nfrom torch import nn, optim\n\n\nX_train = joblib.load('ch08/X_train.joblib')\ny_train = joblib.load('ch08/y_train.joblib')\nX_train = torch.from_numpy(X_train.astype(np.float32)).clone()\ny_train = torch.from_numpy(y_train.astype(np.int64)).clone()\n\nX = X_train[0:4]\ny = y_train[0:4]\n\nnet = nn.Linear(X.size()[1], 4)\nloss_fn = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.01)\n\nlosses = []\n\nfor epoc in range(100):\n    optimizer.zero_grad()\n    y_pred = net(X)\n    loss = loss_fn(y_pred, y)\n    loss.backward()\n    optimizer.step()\n    losses.append(loss)\n\nprint(net.state_dict()['weight'])", "instances": [{"input": "", "output": "import joblib\nimport numpy as np\nimport torch\nfrom torch import nn, optim\nimport matplotlib.pyplot as plt\n\n\nX_train = joblib.load('ch08/X_train.joblib')\ny_train = joblib.load('ch08/y_train.joblib')\nX_train = torch.from_numpy(X_train.astype(np.float32)).clone()\ny_train = torch.from_numpy(y_train.astype(np.int64)).clone()\n\nX_valid = joblib.load('ch08/X_valid.joblib')\ny_valid = joblib.load('ch08/y_valid.joblib')\nX_valid = torch.from_numpy(X_valid.astype(np.float32)).clone()\ny_valid = torch.from_numpy(y_valid.astype(np.int64)).clone()\n\nX_test = joblib.load('ch08/X_test.joblib')\ny_test = joblib.load('ch08/y_test.joblib')\nX_test = torch.from_numpy(X_test.astype(np.float32)).clone()\ny_test = torch.from_numpy(y_test.astype(np.int64)).clone()\n\nX = X_train[0:4]\ny = y_train[0:4]\n\nnet = nn.Linear(X.size()[1], 4)\nloss_fn = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.01)\n\ntrain_losses = []\nvalid_losses = []\ntrain_accs = []\nvalid_accs = []\n\nfor epoc in range(100):\n    optimizer.zero_grad()\n    y_pred = net(X)\n    loss = loss_fn(y_pred, y)\n    loss.backward()\n    optimizer.step()\n\n    train_losses.append(loss)\n    valid_losses.append(loss_fn(net(X_valid), y_valid))\n\n    _, y_pred_train = torch.max(net(X), 1)\n    train_accs.append((y_pred_train == y).sum().item() / len(y))\n    _, y_pred_valid = torch.max(net(X_valid), 1)\n    valid_accs.append((y_pred_valid == y_valid).sum().item() / len(y_valid))\n\nplt.plot(train_losses, label='train loss')\nplt.plot(valid_losses, label='valid loss')\nplt.legend()\nplt.show()\n\nplt.plot(train_accs, label='train acc')\nplt.plot(valid_accs, label='valid acc')\nplt.legend()\nplt.show()"}], "task": "code_generation", "index": 630, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_631", "instruction": "pythonを用いて、以下のコードを改変し、各エポックのパラメータ更新が完了するたびに、チェックポイント（学習途中のパラメータ（重み行列など）の値や最適化アルゴリズムの内部状態）をファイルに書き出せ。\n\nimport joblib\nimport numpy as np\nimport torch\nfrom torch import nn, optim\nimport matplotlib.pyplot as plt\n\n\nX_train = joblib.load('ch08/X_train.joblib')\ny_train = joblib.load('ch08/y_train.joblib')\nX_train = torch.from_numpy(X_train.astype(np.float32)).clone()\ny_train = torch.from_numpy(y_train.astype(np.int64)).clone()\n\nX_valid = joblib.load('ch08/X_valid.joblib')\ny_valid = joblib.load('ch08/y_valid.joblib')\nX_valid = torch.from_numpy(X_valid.astype(np.float32)).clone()\ny_valid = torch.from_numpy(y_valid.astype(np.int64)).clone()\n\nX_test = joblib.load('ch08/X_test.joblib')\ny_test = joblib.load('ch08/y_test.joblib')\nX_test = torch.from_numpy(X_test.astype(np.float32)).clone()\ny_test = torch.from_numpy(y_test.astype(np.int64)).clone()\n\nX = X_train[0:4]\ny = y_train[0:4]\n\nnet = nn.Linear(X.size()[1], 4)\nloss_fn = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.01)\n\ntrain_losses = []\nvalid_losses = []\ntrain_accs = []\nvalid_accs = []\n\nfor epoc in range(100):\n    optimizer.zero_grad()\n    y_pred = net(X)\n    loss = loss_fn(y_pred, y)\n    loss.backward()\n    optimizer.step()\n\n    train_losses.append(loss)\n    valid_losses.append(loss_fn(net(X_valid), y_valid))\n\n    _, y_pred_train = torch.max(net(X), 1)\n    train_accs.append((y_pred_train == y).sum().item() / len(y))\n    _, y_pred_valid = torch.max(net(X_valid), 1)\n    valid_accs.append((y_pred_valid == y_valid).sum().item() / len(y_valid))\n\nplt.plot(train_losses, label='train loss')\nplt.plot(valid_losses, label='valid loss')\nplt.legend()\nplt.show()\n\nplt.plot(train_accs, label='train acc')\nplt.plot(valid_accs, label='valid acc')\nplt.legend()\nplt.show()", "instances": [{"input": "", "output": "import joblib\nimport numpy as np\nimport torch\nfrom torch import nn, optim\nimport matplotlib.pyplot as plt\n\n\nX_train = joblib.load('ch08/X_train.joblib')\ny_train = joblib.load('ch08/y_train.joblib')\nX_train = torch.from_numpy(X_train.astype(np.float32)).clone()\ny_train = torch.from_numpy(y_train.astype(np.int64)).clone()\n\nX_valid = joblib.load('ch08/X_valid.joblib')\ny_valid = joblib.load('ch08/y_valid.joblib')\nX_valid = torch.from_numpy(X_valid.astype(np.float32)).clone()\ny_valid = torch.from_numpy(y_valid.astype(np.int64)).clone()\n\nX_test = joblib.load('ch08/X_test.joblib')\ny_test = joblib.load('ch08/y_test.joblib')\nX_test = torch.from_numpy(X_test.astype(np.float32)).clone()\ny_test = torch.from_numpy(y_test.astype(np.int64)).clone()\n\nX = X_train[0:4]\ny = y_train[0:4]\n\nnet = nn.Linear(X.size()[1], 4)\nloss_fn = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.01)\n\ntrain_losses = []\nvalid_losses = []\ntrain_accs = []\nvalid_accs = []\n\nfor epoc in range(100):\n    optimizer.zero_grad()\n    y_pred = net(X)\n    loss = loss_fn(y_pred, y)\n    loss.backward()\n    optimizer.step()\n\n    joblib.dump(net.state_dict(), f'ch08/state_dict_{epoc}.joblib')\n\n    train_losses.append(loss)\n    valid_losses.append(loss_fn(net(X_valid), y_valid))\n\n    _, y_pred_train = torch.max(net(X), 1)\n    train_accs.append((y_pred_train == y).sum().item() / len(y))\n    _, y_pred_valid = torch.max(net(X_valid), 1)\n    valid_accs.append((y_pred_valid == y_valid).sum().item() / len(y_valid))\n\nplt.plot(train_losses, label='train loss')\nplt.plot(valid_losses, label='valid loss')\nplt.legend()\nplt.show()\n\nplt.plot(train_accs, label='train acc')\nplt.plot(valid_accs, label='valid acc')\nplt.legend()\nplt.show()"}], "task": "code_generation", "index": 631, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_632", "instruction": "pythonを用いて、以下のコードを改変し、B事例ごとに損失・勾配を計算し、行列Wの値を更新せよ（ミニバッチ化）。Bの値を1,2,4,8,…と変化させながら、1エポックの学習に要する時間を比較せよ。\n\nimport joblib\nimport numpy as np\nimport torch\nfrom torch import nn, optim\nimport matplotlib.pyplot as plt\n\n\nX_train = joblib.load('ch08/X_train.joblib')\ny_train = joblib.load('ch08/y_train.joblib')\nX_train = torch.from_numpy(X_train.astype(np.float32)).clone()\ny_train = torch.from_numpy(y_train.astype(np.int64)).clone()\n\nX_valid = joblib.load('ch08/X_valid.joblib')\ny_valid = joblib.load('ch08/y_valid.joblib')\nX_valid = torch.from_numpy(X_valid.astype(np.float32)).clone()\ny_valid = torch.from_numpy(y_valid.astype(np.int64)).clone()\n\nX_test = joblib.load('ch08/X_test.joblib')\ny_test = joblib.load('ch08/y_test.joblib')\nX_test = torch.from_numpy(X_test.astype(np.float32)).clone()\ny_test = torch.from_numpy(y_test.astype(np.int64)).clone()\n\nX = X_train[0:4]\ny = y_train[0:4]\n\nnet = nn.Linear(X.size()[1], 4)\nloss_fn = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.01)\n\ntrain_losses = []\nvalid_losses = []\ntrain_accs = []\nvalid_accs = []\n\nfor epoc in range(100):\n    optimizer.zero_grad()\n    y_pred = net(X)\n    loss = loss_fn(y_pred, y)\n    loss.backward()\n    optimizer.step()\n\n    joblib.dump(net.state_dict(), f'ch08/state_dict_{epoc}.joblib')\n\n    train_losses.append(loss)\n    valid_losses.append(loss_fn(net(X_valid), y_valid))\n\n    _, y_pred_train = torch.max(net(X), 1)\n    train_accs.append((y_pred_train == y).sum().item() / len(y))\n    _, y_pred_valid = torch.max(net(X_valid), 1)\n    valid_accs.append((y_pred_valid == y_valid).sum().item() / len(y_valid))\n\nplt.plot(train_losses, label='train loss')\nplt.plot(valid_losses, label='valid loss')\nplt.legend()\nplt.show()\n\nplt.plot(train_accs, label='train acc')\nplt.plot(valid_accs, label='valid acc')\nplt.legend()\nplt.show()", "instances": [{"input": "", "output": "import joblib\nimport numpy as np\nfrom tqdm import tqdm\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torch import nn, optim\nimport matplotlib.pyplot as plt\n\n\nX_train = joblib.load('ch08/X_train.joblib')\ny_train = joblib.load('ch08/y_train.joblib')\nX_train = torch.from_numpy(X_train.astype(np.float32)).clone()\ny_train = torch.from_numpy(y_train.astype(np.int64)).clone()\n\nX_valid = joblib.load('ch08/X_valid.joblib')\ny_valid = joblib.load('ch08/y_valid.joblib')\nX_valid = torch.from_numpy(X_valid.astype(np.float32)).clone()\ny_valid = torch.from_numpy(y_valid.astype(np.int64)).clone()\n\nX_test = joblib.load('ch08/X_test.joblib')\ny_test = joblib.load('ch08/y_test.joblib')\nX_test = torch.from_numpy(X_test.astype(np.float32)).clone()\ny_test = torch.from_numpy(y_test.astype(np.int64)).clone()\n\nX = X_train\ny = y_train\nds = TensorDataset(X, y)\n\nnet = nn.Linear(X.size()[1], 4)\nloss_fn = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.01)\n\nbatchSize = [1, 2, 4, 8]\n\nfor bs in batchSize:\n    loader = DataLoader(ds, batch_size=bs, shuffle=True)\n\n    train_losses = []\n    valid_losses = []\n    train_accs = []\n    valid_accs = []\n\n    for epoc in tqdm(range(100)):\n        train_running_loss = 0.0\n        valid_running_loss = 0.0\n\n        for xx, yy in loader:\n            y_pred = net(xx)\n            loss = loss_fn(y_pred, yy)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            train_running_loss += loss.item()\n            valid_running_loss += loss_fn(net(X_valid), y_valid).item()\n\n        joblib.dump(net.state_dict(), f'ch08/state_dict_{epoc}.joblib')\n\n        train_losses.append(train_running_loss)\n        valid_losses.append(valid_running_loss)\n\n        _, y_pred_train = torch.max(net(X), 1)\n        train_accs.append((y_pred_train == y).sum().item() / len(y))\n        _, y_pred_valid = torch.max(net(X_valid), 1)\n        valid_accs.append((y_pred_valid == y_valid).sum().item() / len(y_valid))\n\nplt.plot(train_losses, label='train loss')\nplt.plot(valid_losses, label='valid loss')\nplt.legend()\nplt.show()\n\nplt.plot(train_accs, label='train acc')\nplt.plot(valid_accs, label='valid acc')\nplt.legend()\nplt.show()"}], "task": "code_generation", "index": 632, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_633", "instruction": "pythonを用いて、以下のコードを改変し、GPU上で学習を実行せよ。\n\nimport joblib\nimport numpy as np\nfrom tqdm import tqdm\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torch import nn, optim\nimport matplotlib.pyplot as plt\n\n\nX_train = joblib.load('ch08/X_train.joblib')\ny_train = joblib.load('ch08/y_train.joblib')\nX_train = torch.from_numpy(X_train.astype(np.float32)).clone()\ny_train = torch.from_numpy(y_train.astype(np.int64)).clone()\n\nX_valid = joblib.load('ch08/X_valid.joblib')\ny_valid = joblib.load('ch08/y_valid.joblib')\nX_valid = torch.from_numpy(X_valid.astype(np.float32)).clone()\ny_valid = torch.from_numpy(y_valid.astype(np.int64)).clone()\n\nX_test = joblib.load('ch08/X_test.joblib')\ny_test = joblib.load('ch08/y_test.joblib')\nX_test = torch.from_numpy(X_test.astype(np.float32)).clone()\ny_test = torch.from_numpy(y_test.astype(np.int64)).clone()\n\nX = X_train\ny = y_train\nds = TensorDataset(X, y)\n\nnet = nn.Linear(X.size()[1], 4)\nloss_fn = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.01)\n\nbatchSize = [1, 2, 4, 8]\n\nfor bs in batchSize:\n    loader = DataLoader(ds, batch_size=bs, shuffle=True)\n\n    train_losses = []\n    valid_losses = []\n    train_accs = []\n    valid_accs = []\n\n    for epoc in tqdm(range(100)):\n        train_running_loss = 0.0\n        valid_running_loss = 0.0\n\n        for xx, yy in loader:\n            y_pred = net(xx)\n            loss = loss_fn(y_pred, yy)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            train_running_loss += loss.item()\n            valid_running_loss += loss_fn(net(X_valid), y_valid).item()\n\n        joblib.dump(net.state_dict(), f'ch08/state_dict_{epoc}.joblib')\n\n        train_losses.append(train_running_loss)\n        valid_losses.append(valid_running_loss)\n\n        _, y_pred_train = torch.max(net(X), 1)\n        train_accs.append((y_pred_train == y).sum().item() / len(y))\n        _, y_pred_valid = torch.max(net(X_valid), 1)\n        valid_accs.append((y_pred_valid == y_valid).sum().item() / len(y_valid))\n\nplt.plot(train_losses, label='train loss')\nplt.plot(valid_losses, label='valid loss')\nplt.legend()\nplt.show()\n\nplt.plot(train_accs, label='train acc')\nplt.plot(valid_accs, label='valid acc')\nplt.legend()\nplt.show()", "instances": [{"input": "", "output": "import joblib\nimport numpy as np\nfrom tqdm import tqdm\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torch import nn, optim\nimport matplotlib.pyplot as plt\n\n\nX_train = joblib.load('ch08/X_train.joblib')\ny_train = joblib.load('ch08/y_train.joblib')\nX_train = torch.from_numpy(X_train.astype(np.float32)).clone()\ny_train = torch.from_numpy(y_train.astype(np.int64)).clone()\n\nX_valid = joblib.load('ch08/X_valid.joblib')\ny_valid = joblib.load('ch08/y_valid.joblib')\nX_valid = torch.from_numpy(X_valid.astype(np.float32)).clone()\ny_valid = torch.from_numpy(y_valid.astype(np.int64)).clone()\n\nX_test = joblib.load('ch08/X_test.joblib')\ny_test = joblib.load('ch08/y_test.joblib')\nX_test = torch.from_numpy(X_test.astype(np.float32)).clone()\ny_test = torch.from_numpy(y_test.astype(np.int64)).clone()\n\nX = X_train\ny = y_train\nX = X.to('cuda:0')\ny = y.to('cuda:0')\nds = TensorDataset(X, y)\n\nnet = nn.Linear(X.size()[1], 4)\nnet = net.to('cuda:0')\nloss_fn = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.01)\n\nbatchSize = [1, 2, 4, 8]\n\nfor bs in batchSize:\n    loader = DataLoader(ds, batch_size=bs, shuffle=True)\n\n    train_losses = []\n    valid_losses = []\n    train_accs = []\n    valid_accs = []\n\n    for epoc in tqdm(range(100)):\n        train_running_loss = 0.0\n        valid_running_loss = 0.0\n\n        for xx, yy in loader:\n            y_pred = net(xx)\n            loss = loss_fn(y_pred, yy)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            train_running_loss += loss.item()\n            valid_running_loss += loss_fn(net(X_valid), y_valid).item()\n\n        joblib.dump(net.state_dict(), f'ch08/state_dict_{epoc}.joblib')\n\n        train_losses.append(train_running_loss)\n        valid_losses.append(valid_running_loss)\n\n        _, y_pred_train = torch.max(net(X), 1)\n        train_accs.append((y_pred_train == y).sum().item() / len(y))\n        _, y_pred_valid = torch.max(net(X_valid), 1)\n        valid_accs.append((y_pred_valid == y_valid).sum().item() / len(y_valid))\n\nplt.plot(train_losses, label='train loss')\nplt.plot(valid_losses, label='valid loss')\nplt.legend()\nplt.show()\n\nplt.plot(train_accs, label='train acc')\nplt.plot(valid_accs, label='valid acc')\nplt.legend()\nplt.show()"}], "task": "code_generation", "index": 633, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_634", "instruction": "pythonを用いて、以下のコードを改変し、バイアス項の導入や多層化など、ニューラルネットワークの形状を変更しながら、高性能なカテゴリ分類器を構築せよ。\n\nimport joblib\nimport numpy as np\nfrom tqdm import tqdm\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torch import nn, optim\nimport matplotlib.pyplot as plt\n\n\nX_train = joblib.load('ch08/X_train.joblib')\ny_train = joblib.load('ch08/y_train.joblib')\nX_train = torch.from_numpy(X_train.astype(np.float32)).clone()\ny_train = torch.from_numpy(y_train.astype(np.int64)).clone()\n\nX_valid = joblib.load('ch08/X_valid.joblib')\ny_valid = joblib.load('ch08/y_valid.joblib')\nX_valid = torch.from_numpy(X_valid.astype(np.float32)).clone()\ny_valid = torch.from_numpy(y_valid.astype(np.int64)).clone()\n\nX_test = joblib.load('ch08/X_test.joblib')\ny_test = joblib.load('ch08/y_test.joblib')\nX_test = torch.from_numpy(X_test.astype(np.float32)).clone()\ny_test = torch.from_numpy(y_test.astype(np.int64)).clone()\n\nX = X_train\ny = y_train\nX = X.to('cuda:0')\ny = y.to('cuda:0')\nds = TensorDataset(X, y)\n\nnet = nn.Linear(X.size()[1], 4)\nnet = net.to('cuda:0')\nloss_fn = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.01)\n\nbatchSize = [1, 2, 4, 8]\n\nfor bs in batchSize:\n    loader = DataLoader(ds, batch_size=bs, shuffle=True)\n\n    train_losses = []\n    valid_losses = []\n    train_accs = []\n    valid_accs = []\n\n    for epoc in tqdm(range(100)):\n        train_running_loss = 0.0\n        valid_running_loss = 0.0\n\n        for xx, yy in loader:\n            y_pred = net(xx)\n            loss = loss_fn(y_pred, yy)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            train_running_loss += loss.item()\n            valid_running_loss += loss_fn(net(X_valid), y_valid).item()\n\n        joblib.dump(net.state_dict(), f'ch08/state_dict_{epoc}.joblib')\n\n        train_losses.append(train_running_loss)\n        valid_losses.append(valid_running_loss)\n\n        _, y_pred_train = torch.max(net(X), 1)\n        train_accs.append((y_pred_train == y).sum().item() / len(y))\n        _, y_pred_valid = torch.max(net(X_valid), 1)\n        valid_accs.append((y_pred_valid == y_valid).sum().item() / len(y_valid))\n\nplt.plot(train_losses, label='train loss')\nplt.plot(valid_losses, label='valid loss')\nplt.legend()\nplt.show()\n\nplt.plot(train_accs, label='train acc')\nplt.plot(valid_accs, label='valid acc')\nplt.legend()\nplt.show()", "instances": [{"input": "", "output": "import joblib\nimport numpy as np\nfrom tqdm import tqdm\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torch import nn, optim\nimport matplotlib.pyplot as plt\n\n\nX_train = joblib.load('ch08/X_train.joblib')\ny_train = joblib.load('ch08/y_train.joblib')\nX_train = torch.from_numpy(X_train.astype(np.float32)).clone()\ny_train = torch.from_numpy(y_train.astype(np.int64)).clone()\n\nX_valid = joblib.load('ch08/X_valid.joblib')\ny_valid = joblib.load('ch08/y_valid.joblib')\nX_valid = torch.from_numpy(X_valid.astype(np.float32)).clone()\ny_valid = torch.from_numpy(y_valid.astype(np.int64)).clone()\n\nX_test = joblib.load('ch08/X_test.joblib')\ny_test = joblib.load('ch08/y_test.joblib')\nX_test = torch.from_numpy(X_test.astype(np.float32)).clone()\ny_test = torch.from_numpy(y_test.astype(np.int64)).clone()\n\nX = X_train\ny = y_train\nX = X.to('cuda:0')\ny = y.to('cuda:0')\nds = TensorDataset(X, y)\n\nnet = nn.Sequential(\n    nn.Linear(X.size()[1], 100),\n    nn.PReLU(),\n    nn.BatchNorm1d(100),\n    nn.Linear(100, 25),\n    nn.PReLU(),\n    nn.BatchNorm1d(25),\n    nn.Linear(25, 4)\n)\nnet = net.to('cuda:0')\nloss_fn = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.01)\n\nbatchSize = [64]\n\nfor bs in batchSize:\n    loader = DataLoader(ds, batch_size=bs, shuffle=True)\n\n    train_losses = []\n    valid_losses = []\n    train_accs = []\n    valid_accs = []\n\n    for epoc in tqdm(range(100)):\n        train_running_loss = 0.0\n        valid_running_loss = 0.0\n\n        for xx, yy in loader:\n            y_pred = net(xx)\n            loss = loss_fn(y_pred, yy)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            train_running_loss += loss.item()\n            valid_running_loss += loss_fn(net(X_valid), y_valid).item()\n\n        joblib.dump(net.state_dict(), f'ch08/state_dict_{epoc}.joblib')\n\n        train_losses.append(train_running_loss)\n        valid_losses.append(valid_running_loss)\n\n        _, y_pred_train = torch.max(net(X), 1)\n        train_accs.append((y_pred_train == y).sum().item() / len(y))\n        _, y_pred_valid = torch.max(net(X_valid), 1)\n        valid_accs.append((y_pred_valid == y_valid).sum().item() / len(y_valid))\n\nplt.plot(train_losses, label='train loss')\nplt.plot(valid_losses, label='valid loss')\nplt.legend()\nplt.show()\n\nplt.plot(train_accs, label='train acc')\nplt.plot(valid_accs, label='valid acc')\nplt.legend()\nplt.show()"}], "task": "code_generation", "index": 634, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_635", "instruction": "pythonを用いて、手持ちの学習データ中の単語にユニークなID番号を付与したい。学習データ中で最も頻出する単語に`1`、2番目に頻出する単語に`2`、……といった方法で、学習データ中で2回以上出現する単語にID番号を付与せよ。そして、与えられた単語列に対して、ID番号の列を返す関数を実装せよ。ただし、出現頻度が2回未満の単語のID番号はすべて`0`とせよ。", "instances": [{"input": "", "output": "from torchtext import data\n\n\nTEXT = data.Field(sequential=True, lower=True, batch_first=True)\nLABELS = data.Field(sequential=False, batch_first=True, use_vocab=False)\n\ntrain, valid, test = data.TabularDataset.splits(\n    path='ch06', train='train2.txt',\n    validation='valid2.txt', test='test2.txt', format='tsv',\n    fields=[('text', TEXT), ('labels', LABELS)])\n\nTEXT.build_vocab(train, min_freq=2)\nprint(TEXT.vocab.stoi)"}], "task": "code_generation", "index": 635, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_636", "instruction": "pythonを用いて、ID番号で表現された単語列$$\\boldsymbol{x} = (x_1, x_2, \\dots, x_T)$$がある。ただし、$$T$$は単語列の長さ、$$x_t \\in \\mathbb{R}^{V}$$は単語のID番号のone-hot表記である（$$V$$は単語の総数である）。再帰型ニューラルネットワーク（RNN: Recurrent Neural Network）を用い、単語列$$\\boldsymbol{x}$$からカテゴリ$$y$$を予測するモデルとして、次式を実装せよ。\n\n$$\n\\overrightarrow{h}_0 = 0, \\\\\n\\overrightarrow{h}_t = {\\rm \\overrightarrow{RNN}}(\\mathrm{emb}(x_t), \\overrightarrow{h}_{t-1}), \\\\\ny = {\\rm softmax}(W^{(yh)} \\overrightarrow{h}_T + b^{(y)})\n$$\n\nただし、$$\\mathrm{emb}(x) \\in \\mathbb{R}^{d_w}$$は単語埋め込み（単語のone-hot表記から単語ベクトルに変換する関数）、$$\\overrightarrow{h}_t \\in \\mathbb{R}^{d_h}$$は時刻$$t$$の隠れ状態ベクトル、$${\\rm \\overrightarrow{RNN}}(x,h)$$は入力$$x$$と前時刻の隠れ状態$$h$$から次状態を計算するRNNユニット、$$W^{(yh)} \\in \\mathbb{R}^{L \\times d_h}$$は隠れ状態ベクトルからカテゴリを予測するための行列、$$b^{(y)} \\in \\mathbb{R}^{L}$$はバイアス項である（$$d_w, d_h, L$$はそれぞれ、単語埋め込みの次元数、隠れ状態ベクトルの次元数、ラベル数である）。RNNユニット$${\\rm \\overrightarrow{RNN}}(x,h)$$には様々な構成が考えられるが、典型例として次式が挙げられる。\n\n$$\n{\\rm \\overrightarrow{RNN}}(x,h) = g(W^{(hx)} x + W^{(hh)}h + b^{(h)})\n$$\n\nただし、$$W^{(hx)} \\in \\mathbb{R}^{d_h \\times d_w}、W^{(hh)} \\in \\mathbb{R}^{d_h \\times d_h}, b^{(h)} \\in \\mathbb{R}^{d_h}$$はRNNユニットのパラメータ、$$g$$は活性化関数（例えば$$\\tanh$$やReLUなど）である。\n\nなお、この問題ではパラメータの学習を行わず、ランダムに初期化されたパラメータで$$y$$を計算するだけでよい。次元数などのハイパーパラメータは、$$d_w = 300, d_h=50$$など、適当な値に設定せよ（以降の問題でも同様である）。", "instances": [{"input": "", "output": "from torchtext import data\nimport torch\nfrom torch import nn\n\n\nclass RNN(nn.Module):\n    def __init__(self, num_embeddings,\n                 embedding_dim=50,\n                 hidden_size=50,\n                 output_size=1,\n                 num_layers=1,\n                 dropout=0.2):\n        super().__init__()\n        self.emb = nn.Embedding(num_embeddings, embedding_dim,\n                                padding_idx=0)\n        self.lstm = nn.LSTM(embedding_dim,\n                            hidden_size, num_layers,\n                            batch_first=True, dropout=dropout)\n        self.linear = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x, h0=None):\n        x = self.emb(x)\n        x, h = self.lstm(x, h0)\n        x = x[:, -1, :]\n        x = self.linear(x)\n        return x\n\n\nTEXT = data.Field(sequential=True, lower=True, batch_first=True)\nLABELS = data.Field(sequential=False, batch_first=True, use_vocab=False)\n\ntrain, val, test = data.TabularDataset.splits(\n    path='ch06', train='train2.txt',\n    validation='valid2.txt', test='test2.txt', format='tsv',\n    fields=[('TEXT', TEXT), ('LABEL', LABELS)])\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\ntrain_iter, val_iter, test_iter = data.Iterator.splits(\n    (train, val, test), batch_sizes=(64, 64, 64), device=device, repeat=False, sort=False)\n\nTEXT.build_vocab(train, min_freq=2)\nLABELS.build_vocab(train)\nmodel = RNN(len(TEXT.vocab.stoi) + 1, num_layers=2, output_size=4)\n\nfor epoch in range(1):\n    model.train()\n    for batch in train_iter:\n        x, y = batch.TEXT, batch.LABEL\n        y_pred = model(x)\n        print(y_pred)\n        print(y_pred.shape)"}], "task": "code_generation", "index": 636, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_637", "instruction": "pythonを用いて、確率的勾配降下法（SGD: Stochastic Gradient Descent）を用いて、問題81で構築したモデルを学習せよ。訓練データ上の損失と正解率、評価データ上の損失と正解率を表示しながらモデルを学習し、適当な基準（例えば10エポックなど）で終了させよ。", "instances": [{"input": "", "output": "import torch\nfrom torch import nn, optim\nfrom torchtext import data\nfrom catalyst.dl import SupervisedRunner\nfrom catalyst.dl.callbacks import AccuracyCallback\nfrom torch.utils.data import DataLoader\nfrom torchtext.data import Iterator\n\n\nclass BucketIteratorWrapper(DataLoader):\n    __initialized__ = False\n\n    def __init__(self, iterator: Iterator):\n        self.batch_size = iterator.batch_size\n        self.num_workers = 1\n        self.collate_fn = None\n        self.pin_memory = False\n        self.drop_last = False\n        self.timeout = 0\n        self.worker_init_fn = None\n        self.sampler = iterator\n        self.batch_sampler = iterator\n        self.__initialized__ = True\n\n    def __iter__(self):\n        return map(lambda batch: {\n            'features': batch.TEXT,\n            'targets': batch.LABEL,\n        }, self.batch_sampler.__iter__())\n\n    def __len__(self):\n        return len(self.batch_sampler)\n\n\nclass RNN(nn.Module):\n    def __init__(self, num_embeddings,\n                 embedding_dim=50,\n                 hidden_size=50,\n                 output_size=1,\n                 num_layers=1,\n                 dropout=0.2):\n        super().__init__()\n        self.emb = nn.Embedding(num_embeddings, embedding_dim,\n                                padding_idx=0)\n        self.lstm = nn.LSTM(embedding_dim,\n                            hidden_size, num_layers,\n                            batch_first=True, dropout=dropout)\n        self.linear = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x, h0=None):\n        x = self.emb(x)\n        x, h = self.lstm(x, h0)\n        x = x[:, -1, :]\n        x = self.linear(x)\n        return x\n\n\nTEXT = data.Field(sequential=True, lower=True, batch_first=True)\nLABELS = data.Field(sequential=False, batch_first=True, use_vocab=False)\n\ntrain, val, test = data.TabularDataset.splits(\n    path='ch06', train='train2.txt',\n    validation='valid2.txt', test='test2.txt', format='tsv',\n    fields=[('TEXT', TEXT), ('LABEL', LABELS)])\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\ntrain_iter, val_iter, test_iter = data.BucketIterator.splits(\n    (train, val, test), batch_sizes=(len(train), len(val), len(test)), device=device, repeat=False, sort=False)\n\ntrain_loader = BucketIteratorWrapper(train_iter)\nvalid_loader = BucketIteratorWrapper(val_iter)\nloaders = {\"train\": train_loader, \"valid\": valid_loader}\n\nTEXT.build_vocab(train, min_freq=2)\nLABELS.build_vocab(train)\n\nmodel = RNN(len(TEXT.vocab.stoi) + 1, num_layers=2, output_size=4)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\nrunner = SupervisedRunner()\n\nrunner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    loaders=loaders,\n    logdir=\"./logdir\",\n    callbacks=[AccuracyCallback(num_classes=4, accuracy_args=[1])],\n    num_epochs=10,\n    verbose=True,\n)"}], "task": "code_generation", "index": 637, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_638", "instruction": "pythonを用いて、以下のコードを改変し、$$B$$事例ごとに損失・勾配を計算して学習を行えるようにせよ（$$B$$の値は適当に選べ）。また、GPU上で学習を実行せよ。\n\nimport torch\nfrom torch import nn, optim\nfrom torchtext import data\nfrom catalyst.dl import SupervisedRunner\nfrom catalyst.dl.callbacks import AccuracyCallback\nfrom torch.utils.data import DataLoader\nfrom torchtext.data import Iterator\n\n\nclass BucketIteratorWrapper(DataLoader):\n    __initialized__ = False\n\n    def __init__(self, iterator: Iterator):\n        self.batch_size = iterator.batch_size\n        self.num_workers = 1\n        self.collate_fn = None\n        self.pin_memory = False\n        self.drop_last = False\n        self.timeout = 0\n        self.worker_init_fn = None\n        self.sampler = iterator\n        self.batch_sampler = iterator\n        self.__initialized__ = True\n\n    def __iter__(self):\n        return map(lambda batch: {\n            'features': batch.TEXT,\n            'targets': batch.LABEL,\n        }, self.batch_sampler.__iter__())\n\n    def __len__(self):\n        return len(self.batch_sampler)\n\n\nclass RNN(nn.Module):\n    def __init__(self, num_embeddings,\n                 embedding_dim=50,\n                 hidden_size=50,\n                 output_size=1,\n                 num_layers=1,\n                 dropout=0.2):\n        super().__init__()\n        self.emb = nn.Embedding(num_embeddings, embedding_dim,\n                                padding_idx=0)\n        self.lstm = nn.LSTM(embedding_dim,\n                            hidden_size, num_layers,\n                            batch_first=True, dropout=dropout)\n        self.linear = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x, h0=None):\n        x = self.emb(x)\n        x, h = self.lstm(x, h0)\n        x = x[:, -1, :]\n        x = self.linear(x)\n        return x\n\n\nTEXT = data.Field(sequential=True, lower=True, batch_first=True)\nLABELS = data.Field(sequential=False, batch_first=True, use_vocab=False)\n\ntrain, val, test = data.TabularDataset.splits(\n    path='ch06', train='train2.txt',\n    validation='valid2.txt', test='test2.txt', format='tsv',\n    fields=[('TEXT', TEXT), ('LABEL', LABELS)])\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\ntrain_iter, val_iter, test_iter = data.BucketIterator.splits(\n    (train, val, test), batch_sizes=(len(train), len(val), len(test)), device=device, repeat=False, sort=False)\n\ntrain_loader = BucketIteratorWrapper(train_iter)\nvalid_loader = BucketIteratorWrapper(val_iter)\nloaders = {\"train\": train_loader, \"valid\": valid_loader}\n\nTEXT.build_vocab(train, min_freq=2)\nLABELS.build_vocab(train)\n\nmodel = RNN(len(TEXT.vocab.stoi) + 1, num_layers=2, output_size=4)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\nrunner = SupervisedRunner()\n\nrunner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    loaders=loaders,\n    logdir=\"./logdir\",\n    callbacks=[AccuracyCallback(num_classes=4, accuracy_args=[1])],\n    num_epochs=10,\n    verbose=True,\n)", "instances": [{"input": "", "output": "import torch\nfrom torch import nn, optim\nfrom torchtext import data\nfrom catalyst.dl import SupervisedRunner\nfrom catalyst.dl.callbacks import AccuracyCallback\nfrom torch.utils.data import DataLoader\nfrom torchtext.data import Iterator\n\n\nclass BucketIteratorWrapper(DataLoader):\n    __initialized__ = False\n\n    def __init__(self, iterator: Iterator):\n        self.batch_size = iterator.batch_size\n        self.num_workers = 1\n        self.collate_fn = None\n        self.pin_memory = False\n        self.drop_last = False\n        self.timeout = 0\n        self.worker_init_fn = None\n        self.sampler = iterator\n        self.batch_sampler = iterator\n        self.__initialized__ = True\n\n    def __iter__(self):\n        return map(lambda batch: {\n            'features': batch.TEXT,\n            'targets': batch.LABEL,\n        }, self.batch_sampler.__iter__())\n\n    def __len__(self):\n        return len(self.batch_sampler)\n\n\nclass RNN(nn.Module):\n    def __init__(self, num_embeddings,\n                 embedding_dim=50,\n                 hidden_size=50,\n                 output_size=1,\n                 num_layers=1,\n                 dropout=0.2):\n        super().__init__()\n        self.emb = nn.Embedding(num_embeddings, embedding_dim,\n                                padding_idx=0)\n        self.lstm = nn.LSTM(embedding_dim,\n                            hidden_size, num_layers,\n                            batch_first=True, dropout=dropout)\n        self.linear = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x, h0=None):\n        x = self.emb(x)\n        x, h = self.lstm(x, h0)\n        x = x[:, -1, :]\n        x = self.linear(x)\n        return x\n\n\nTEXT = data.Field(sequential=True, lower=True, batch_first=True)\nLABELS = data.Field(sequential=False, batch_first=True, use_vocab=False)\n\ntrain, val, test = data.TabularDataset.splits(\n    path='ch06', train='train2.txt',\n    validation='valid2.txt', test='test2.txt', format='tsv',\n    fields=[('TEXT', TEXT), ('LABEL', LABELS)])\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\ntrain_iter, val_iter, test_iter = data.BucketIterator.splits(\n    (train, val, test), batch_sizes=(64, 64, 64), device=device, repeat=False, sort=False)\n\ntrain_loader = BucketIteratorWrapper(train_iter)\nvalid_loader = BucketIteratorWrapper(val_iter)\nloaders = {\"train\": train_loader, \"valid\": valid_loader}\n\nTEXT.build_vocab(train, min_freq=2)\nLABELS.build_vocab(train)\n\nmodel = RNN(len(TEXT.vocab.stoi) + 1, num_layers=2, output_size=4)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\nrunner = SupervisedRunner()\n\nrunner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    loaders=loaders,\n    logdir=\"./logdir\",\n    callbacks=[AccuracyCallback(num_classes=4, accuracy_args=[1])],\n    num_epochs=10,\n    verbose=True,\n)"}], "task": "code_generation", "index": 638, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_639", "instruction": "pythonを用いて、事前学習済みの単語ベクトル（例えば、Google Newsデータセット（約1,000億単語）での[学習済み単語ベクトル](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing)）で単語埋め込み$$\\mathrm{emb}(x)$$を初期化し、学習せよ。", "instances": [{"input": "", "output": "import torch\nfrom torch import nn, optim\nfrom torchtext import data\nfrom catalyst.dl import SupervisedRunner\nfrom catalyst.dl.callbacks import AccuracyCallback\nfrom torch.utils.data import DataLoader\nfrom torchtext.data import Iterator\nfrom gensim.models import KeyedVectors\n\n\nclass BucketIteratorWrapper(DataLoader):\n    __initialized__ = False\n\n    def __init__(self, iterator: Iterator):\n        self.batch_size = iterator.batch_size\n        self.num_workers = 1\n        self.collate_fn = None\n        self.pin_memory = False\n        self.drop_last = False\n        self.timeout = 0\n        self.worker_init_fn = None\n        self.sampler = iterator\n        self.batch_sampler = iterator\n        self.__initialized__ = True\n\n    def __iter__(self):\n        return map(lambda batch: {\n            'features': batch.TEXT,\n            'targets': batch.LABEL,\n        }, self.batch_sampler.__iter__())\n\n    def __len__(self):\n        return len(self.batch_sampler)\n\n\nclass RNN(nn.Module):\n    def __init__(self, num_embeddings,\n                 embedding_dim=300,\n                 hidden_size=300,\n                 output_size=1,\n                 num_layers=1,\n                 dropout=0.2):\n        super().__init__()\n        model = KeyedVectors.load_word2vec_format('ch07/GoogleNews-vectors-negative300.bin', binary=True)\n        weights = torch.FloatTensor(model.vectors)\n        self.emb = nn.Embedding.from_pretrained(weights)\n        self.lstm = nn.LSTM(embedding_dim,\n                            hidden_size, num_layers,\n                            batch_first=True, dropout=dropout)\n        self.linear = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x, h0=None):\n        x = self.emb(x)\n        x, h = self.lstm(x, h0)\n        x = x[:, -1, :]\n        x = self.linear(x)\n        return x\n\n\nTEXT = data.Field(sequential=True, lower=True, batch_first=True)\nLABELS = data.Field(sequential=False, batch_first=True, use_vocab=False)\n\ntrain, val, test = data.TabularDataset.splits(\n    path='ch06', train='train2.txt',\n    validation='valid2.txt', test='test2.txt', format='tsv',\n    fields=[('TEXT', TEXT), ('LABEL', LABELS)])\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\ntrain_iter, val_iter, test_iter = data.BucketIterator.splits(\n    (train, val, test), batch_sizes=(64, 64, 64), device=device, repeat=False, sort=False)\n\ntrain_loader = BucketIteratorWrapper(train_iter)\nvalid_loader = BucketIteratorWrapper(val_iter)\nloaders = {\"train\": train_loader, \"valid\": valid_loader}\n\nTEXT.build_vocab(train, min_freq=2)\nLABELS.build_vocab(train)\n\nmodel = RNN(len(TEXT.vocab.stoi) + 1, num_layers=2, output_size=4)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\nrunner = SupervisedRunner()\n\nrunner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    loaders=loaders,\n    logdir=\"./logdir\",\n    callbacks=[AccuracyCallback(num_classes=4, accuracy_args=[1])],\n    num_epochs=10,\n    verbose=True,\n)"}], "task": "code_generation", "index": 639, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_640", "instruction": "pythonを用いて、順方向と逆方向のRNNの両方を用いて入力テキストをエンコードし、モデルを学習せよ。\n\n$$\n\\overleftarrow{h}_{T+1} = 0, \\\\\n\\overleftarrow{h}_t = {\\rm \\overleftarrow{RNN}}(\\mathrm{emb}(x_t), \\overleftarrow{h}_{t+1}), \\\\\ny = {\\rm softmax}(W^{(yh)} [\\overrightarrow{h}_T; \\overleftarrow{h}_1] + b^{(y)})\n$$\n\nただし、$$\\overrightarrow{h}_t \\in \\mathbb{R}^{d_h}, \\overleftarrow{h}_t \\in \\mathbb{R}^{d_h}$$はそれぞれ、順方向および逆方向のRNNで求めた時刻$$t$$の隠れ状態ベクトル、$${\\rm \\overleftarrow{RNN}}(x,h)$$は入力$$x$$と次時刻の隠れ状態$$h$$から前状態を計算するRNNユニット、$$W^{(yh)} \\in \\mathbb{R}^{L \\times 2d_h}$$は隠れ状態ベクトルからカテゴリを予測するための行列、$$b^{(y)} \\in \\mathbb{R}^{L}$$はバイアス項である。また、$$[a; b]$$はベクトル$$a$$と$$b$$の連結を表す。\n\nさらに、双方向RNNを多層化して実験せよ。", "instances": [{"input": "", "output": "import torch\nfrom torch import nn, optim\nfrom torchtext import data\nfrom catalyst.dl import SupervisedRunner\nfrom catalyst.dl.callbacks import AccuracyCallback\nfrom torch.utils.data import DataLoader\nfrom torchtext.data import Iterator\nfrom gensim.models import KeyedVectors\n\n\nclass BucketIteratorWrapper(DataLoader):\n    __initialized__ = False\n\n    def __init__(self, iterator: Iterator):\n        self.batch_size = iterator.batch_size\n        self.num_workers = 1\n        self.collate_fn = None\n        self.pin_memory = False\n        self.drop_last = False\n        self.timeout = 0\n        self.worker_init_fn = None\n        self.sampler = iterator\n        self.batch_sampler = iterator\n        self.__initialized__ = True\n\n    def __iter__(self):\n        return map(lambda batch: {\n            'features': batch.TEXT,\n            'targets': batch.LABEL,\n        }, self.batch_sampler.__iter__())\n\n    def __len__(self):\n        return len(self.batch_sampler)\n\n\nclass RNN(nn.Module):\n    def __init__(self, num_embeddings,\n                 embedding_dim=300,\n                 hidden_size=300,\n                 output_size=1,\n                 num_layers=1,\n                 dropout=0.2):\n        super().__init__()\n        model = KeyedVectors.load_word2vec_format('ch07/GoogleNews-vectors-negative300.bin', binary=True)\n        weights = torch.FloatTensor(model.vectors)\n        self.emb = nn.Embedding.from_pretrained(weights)\n        self.lstm = nn.LSTM(embedding_dim,\n                            hidden_size, num_layers,\n                            batch_first=True, dropout=dropout, bidirectional=True)\n        self.linear = nn.Linear(hidden_size * 2, output_size)\n\n    def forward(self, x, h0=None):\n        x = self.emb(x)\n        x, h = self.lstm(x, h0)\n        x = x[:, -1, :]\n        x = self.linear(x)\n        return x\n\n\nTEXT = data.Field(sequential=True, lower=True, batch_first=True)\nLABELS = data.Field(sequential=False, batch_first=True, use_vocab=False)\n\ntrain, val, test = data.TabularDataset.splits(\n    path='ch06', train='train2.txt',\n    validation='valid2.txt', test='test2.txt', format='tsv',\n    fields=[('TEXT', TEXT), ('LABEL', LABELS)])\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\ntrain_iter, val_iter, test_iter = data.BucketIterator.splits(\n    (train, val, test), batch_sizes=(64, 64, 64), device=device, repeat=False, sort=False)\n\ntrain_loader = BucketIteratorWrapper(train_iter)\nvalid_loader = BucketIteratorWrapper(val_iter)\nloaders = {\"train\": train_loader, \"valid\": valid_loader}\n\nTEXT.build_vocab(train, min_freq=2)\nLABELS.build_vocab(train)\n\nmodel = RNN(len(TEXT.vocab.stoi) + 1, num_layers=2, output_size=4)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\nrunner = SupervisedRunner()\n\nrunner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    loaders=loaders,\n    logdir=\"./logdir\",\n    callbacks=[AccuracyCallback(num_classes=4, accuracy_args=[1])],\n    num_epochs=10,\n    verbose=True,\n)"}], "task": "code_generation", "index": 640, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_641", "instruction": "pythonを用いて、ID番号で表現された単語列$$\\boldsymbol{x} = (x_1, x_2, \\dots, x_T)$$がある。ただし、$$T$$は単語列の長さ、$$x_t \\in \\mathbb{R}^{V}$$は単語のID番号のone-hot表記である（$$V$$は単語の総数である）。畳み込みニューラルネットワーク（CNN: Convolutional Neural Network）を用い、単語列$$\\boldsymbol{x}$$からカテゴリ$$y$$を予測するモデルを実装せよ。\n\nただし、畳み込みニューラルネットワークの構成は以下の通りとする。\n\n+ 単語埋め込みの次元数: $$d_w$$\n+ 畳み込みのフィルターのサイズ: 3 トークン\n+ 畳み込みのストライド: 1 トークン\n+ 畳み込みのパディング: あり\n+ 畳み込み演算後の各時刻のベクトルの次元数: $$d_h$$\n+ 畳み込み演算後に最大値プーリング（max pooling）を適用し、入力文を$$d_h$$次元の隠れベクトルで表現\n\nすなわち、時刻$$t$$の特徴ベクトル$$p_t \\in \\mathbb{R}^{d_h}$$は次式で表される。\n\n$$\np_t = g(W^{(px)} [\\mathrm{emb}(x_{t-1}); \\mathrm{emb}(x_t); \\mathrm{emb}(x_{t+1})] + b^{(p)})\n$$\n\nただし、$$W^{(px)} \\in \\mathbb{R}^{d_h \\times 3d_w}, b^{(p)} \\in \\mathbb{R}^{d_h}$$はCNNのパラメータ、$$g$$は活性化関数（例えば$$\\tanh$$やReLUなど）、$$[a; b; c]$$はベクトル$$a, b, c$$の連結である。なお、行列$$W^{(px)}$$の列数が$$3d_w$$になるのは、3個のトークンの単語埋め込みを連結したものに対して、線形変換を行うためである。\n\n最大値プーリングでは、特徴ベクトルの次元毎に全時刻における最大値を取り、入力文書の特徴ベクトル$$c \\in \\mathbb{R}^{d_h}$$を求める。$$c[i]$$でベクトル$$c$$の$$i$$番目の次元の値を表すことにすると、最大値プーリングは次式で表される。\n\n$$\nc[i] = \\max_{1 \\leq t \\leq T} p_t[i]\n$$\n\n最後に、入力文書の特徴ベクトル$$c$$に行列$$W^{(yc)} \\in \\mathbb{R}^{L \\times d_h}$$とバイアス項$$b^{(y)} \\in \\mathbb{R}^{L}$$による線形変換とソフトマックス関数を適用し、カテゴリ$$y$$を予測する。\n\n$$\ny = {\\rm softmax}(W^{(yc)} c + b^{(y)})\n$$\n\nなお、この問題ではモデルの学習を行わず、ランダムに初期化された重み行列で$$y$$を計算するだけでよい。", "instances": [{"input": "", "output": "import torch\nfrom torch import nn, optim\nimport torch.nn.functional as F\nfrom torchtext import data\nfrom torch.utils.data import DataLoader\nfrom torchtext.data import Iterator\nfrom gensim.models import KeyedVectors\n\n\nclass BucketIteratorWrapper(DataLoader):\n    __initialized__ = False\n\n    def __init__(self, iterator: Iterator):\n        self.batch_size = iterator.batch_size\n        self.num_workers = 1\n        self.collate_fn = None\n        self.pin_memory = False\n        self.drop_last = False\n        self.timeout = 0\n        self.worker_init_fn = None\n        self.sampler = iterator\n        self.batch_sampler = iterator\n        self.__initialized__ = True\n\n    def __iter__(self):\n        return map(lambda batch: {\n            'features': batch.TEXT,\n            'targets': batch.LABEL,\n        }, self.batch_sampler.__iter__())\n\n    def __len__(self):\n        return len(self.batch_sampler)\n\n\nclass CNN(nn.Module):\n\n    def __init__(self, output_dim, kernel_num, kernel_sizes=[3, 4, 5], dropout=0.5, static=False):\n        super(CNN, self).__init__()\n\n        model = KeyedVectors.load_word2vec_format('ch07/GoogleNews-vectors-negative300.bin', binary=True)\n        weights = torch.FloatTensor(model.vectors)\n        self.embed = nn.Embedding.from_pretrained(weights)\n        self.convs1 = nn.ModuleList([nn.Conv2d(1, kernel_num, (k, self.embed.weight.shape[1])) for k in kernel_sizes])\n        self.dropout = nn.Dropout(dropout)\n        self.fc1 = nn.Linear(len(kernel_sizes) * kernel_num, output_dim)\n        self.static = static\n\n    def conv_and_pool(self, x, conv):\n        x = F.relu(conv(x)).squeeze(3)\n        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n        return x\n\n    def forward(self, x):\n        x = self.embed(x)\n\n        if self.static:\n            x = x.detach()\n\n        x = x.unsqueeze(1)\n        x = x.float()\n        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1]\n\n        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]\n\n        x = torch.cat(x, 1)\n        x = self.dropout(x)\n        logit = self.fc1(x)\n        return logit\n\n\nTEXT = data.Field(sequential=True, lower=True, batch_first=True)\nLABELS = data.Field(sequential=False, batch_first=True, use_vocab=False)\n\ntrain, val, test = data.TabularDataset.splits(\n    path='ch06', train='train2.txt',\n    validation='valid2.txt', test='test2.txt', format='tsv',\n    fields=[('TEXT', TEXT), ('LABEL', LABELS)])\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\ntrain_iter, val_iter, test_iter = data.BucketIterator.splits(\n    (train, val, test), batch_sizes=(64, 64, 64), device=device, repeat=False, sort=False)\n\ntrain_loader = BucketIteratorWrapper(train_iter)\nvalid_loader = BucketIteratorWrapper(val_iter)\nloaders = {\"train\": train_loader, \"valid\": valid_loader}\n\nTEXT.build_vocab(train, min_freq=2)\nLABELS.build_vocab(train)\nmodel = CNN(output_dim=4, kernel_num=3, kernel_sizes=[3, 4, 5], dropout=0.2)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\nfor epoch in range(1):\n    model.train()\n    for batch in train_iter:\n        x, y = batch.TEXT, batch.LABEL\n        y_pred = model(x)\n        print(y_pred)\n        print(y_pred.shape)"}], "task": "code_generation", "index": 641, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_642", "instruction": "pythonを用いて、確率的勾配降下法（SGD: Stochastic Gradient Descent）を用いて、問題86で構築したモデルを学習せよ。訓練データ上の損失と正解率、評価データ上の損失と正解率を表示しながらモデルを学習し、適当な基準（例えば10エポックなど）で終了させよ。", "instances": [{"input": "", "output": "import torch\nfrom torch import nn, optim\nimport torch.nn.functional as F\nfrom torchtext import data\nfrom catalyst.dl import SupervisedRunner\nfrom catalyst.dl.callbacks import AccuracyCallback\nfrom torch.utils.data import DataLoader\nfrom torchtext.data import Iterator\nfrom gensim.models import KeyedVectors\n\n\nclass BucketIteratorWrapper(DataLoader):\n    __initialized__ = False\n\n    def __init__(self, iterator: Iterator):\n        self.batch_size = iterator.batch_size\n        self.num_workers = 1\n        self.collate_fn = None\n        self.pin_memory = False\n        self.drop_last = False\n        self.timeout = 0\n        self.worker_init_fn = None\n        self.sampler = iterator\n        self.batch_sampler = iterator\n        self.__initialized__ = True\n\n    def __iter__(self):\n        return map(lambda batch: {\n            'features': batch.TEXT,\n            'targets': batch.LABEL,\n        }, self.batch_sampler.__iter__())\n\n    def __len__(self):\n        return len(self.batch_sampler)\n\n\nclass CNN(nn.Module):\n\n    def __init__(self, output_dim, kernel_num, kernel_sizes=[3, 4, 5], dropout=0.5, static=False):\n        super(CNN, self).__init__()\n\n        model = KeyedVectors.load_word2vec_format('ch07/GoogleNews-vectors-negative300.bin', binary=True)\n        weights = torch.FloatTensor(model.vectors)\n        self.embed = nn.Embedding.from_pretrained(weights)\n        self.convs1 = nn.ModuleList([nn.Conv2d(1, kernel_num, (k, self.embed.weight.shape[1])) for k in kernel_sizes])\n        self.dropout = nn.Dropout(dropout)\n        self.fc1 = nn.Linear(len(kernel_sizes) * kernel_num, output_dim)\n        self.static = static\n\n    def conv_and_pool(self, x, conv):\n        x = F.relu(conv(x)).squeeze(3)\n        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n        return x\n\n    def forward(self, x):\n        x = self.embed(x)\n\n        if self.static:\n            x = x.detach()\n\n        x = x.unsqueeze(1)\n        x = x.float()\n        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1]\n\n        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]\n\n        x = torch.cat(x, 1)\n        x = self.dropout(x)\n        logit = self.fc1(x)\n        return logit\n\n\nTEXT = data.Field(sequential=True, lower=True, batch_first=True)\nLABELS = data.Field(sequential=False, batch_first=True, use_vocab=False)\n\ntrain, val, test = data.TabularDataset.splits(\n    path='ch06', train='train2.txt',\n    validation='valid2.txt', test='test2.txt', format='tsv',\n    fields=[('TEXT', TEXT), ('LABEL', LABELS)])\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\ntrain_iter, val_iter, test_iter = data.BucketIterator.splits(\n    (train, val, test), batch_sizes=(64, 64, 64), device=device, repeat=False, sort=False)\n\ntrain_loader = BucketIteratorWrapper(train_iter)\nvalid_loader = BucketIteratorWrapper(val_iter)\nloaders = {\"train\": train_loader, \"valid\": valid_loader}\n\nTEXT.build_vocab(train, min_freq=2)\nLABELS.build_vocab(train)\nmodel = CNN(output_dim=4, kernel_num=3, kernel_sizes=[3, 4, 5], dropout=0.2)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\nrunner = SupervisedRunner()\n\nrunner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    loaders=loaders,\n    logdir=\"./logdir\",\n    callbacks=[AccuracyCallback(num_classes=4, accuracy_args=[1])],\n    num_epochs=10,\n    verbose=True,\n)"}], "task": "code_generation", "index": 642, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_643", "instruction": "pythonを用いて、以下のコードを改変し、ニューラルネットワークの形状やハイパーパラメータを調整しながら、高性能なカテゴリ分類器を構築せよ。\n\nimport torch\nfrom torch import nn, optim\nimport torch.nn.functional as F\nfrom torchtext import data\nfrom catalyst.dl import SupervisedRunner\nfrom catalyst.dl.callbacks import AccuracyCallback\nfrom torch.utils.data import DataLoader\nfrom torchtext.data import Iterator\nfrom gensim.models import KeyedVectors\n\n\nclass BucketIteratorWrapper(DataLoader):\n    __initialized__ = False\n\n    def __init__(self, iterator: Iterator):\n        self.batch_size = iterator.batch_size\n        self.num_workers = 1\n        self.collate_fn = None\n        self.pin_memory = False\n        self.drop_last = False\n        self.timeout = 0\n        self.worker_init_fn = None\n        self.sampler = iterator\n        self.batch_sampler = iterator\n        self.__initialized__ = True\n\n    def __iter__(self):\n        return map(lambda batch: {\n            'features': batch.TEXT,\n            'targets': batch.LABEL,\n        }, self.batch_sampler.__iter__())\n\n    def __len__(self):\n        return len(self.batch_sampler)\n\n\nclass CNN(nn.Module):\n\n    def __init__(self, output_dim, kernel_num, kernel_sizes=[3, 4, 5], dropout=0.5, static=False):\n        super(CNN, self).__init__()\n\n        model = KeyedVectors.load_word2vec_format('ch07/GoogleNews-vectors-negative300.bin', binary=True)\n        weights = torch.FloatTensor(model.vectors)\n        self.embed = nn.Embedding.from_pretrained(weights)\n        self.convs1 = nn.ModuleList([nn.Conv2d(1, kernel_num, (k, self.embed.weight.shape[1])) for k in kernel_sizes])\n        self.dropout = nn.Dropout(dropout)\n        self.fc1 = nn.Linear(len(kernel_sizes) * kernel_num, output_dim)\n        self.static = static\n\n    def conv_and_pool(self, x, conv):\n        x = F.relu(conv(x)).squeeze(3)\n        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n        return x\n\n    def forward(self, x):\n        x = self.embed(x)\n\n        if self.static:\n            x = x.detach()\n\n        x = x.unsqueeze(1)\n        x = x.float()\n        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1]\n\n        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]\n\n        x = torch.cat(x, 1)\n        x = self.dropout(x)\n        logit = self.fc1(x)\n        return logit\n\n\nTEXT = data.Field(sequential=True, lower=True, batch_first=True)\nLABELS = data.Field(sequential=False, batch_first=True, use_vocab=False)\n\ntrain, val, test = data.TabularDataset.splits(\n    path='ch06', train='train2.txt',\n    validation='valid2.txt', test='test2.txt', format='tsv',\n    fields=[('TEXT', TEXT), ('LABEL', LABELS)])\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\ntrain_iter, val_iter, test_iter = data.BucketIterator.splits(\n    (train, val, test), batch_sizes=(64, 64, 64), device=device, repeat=False, sort=False)\n\ntrain_loader = BucketIteratorWrapper(train_iter)\nvalid_loader = BucketIteratorWrapper(val_iter)\nloaders = {\"train\": train_loader, \"valid\": valid_loader}\n\nTEXT.build_vocab(train, min_freq=2)\nLABELS.build_vocab(train)\nmodel = CNN(output_dim=4, kernel_num=3, kernel_sizes=[3, 4, 5], dropout=0.2)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\nrunner = SupervisedRunner()\n\nrunner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    loaders=loaders,\n    logdir=\"./logdir\",\n    callbacks=[AccuracyCallback(num_classes=4, accuracy_args=[1])],\n    num_epochs=10,\n    verbose=True,\n)", "instances": [{"input": "", "output": "import torch\nfrom torch import nn, optim\nfrom torchtext import data\nfrom catalyst.dl import SupervisedRunner\nfrom catalyst.dl.callbacks import AccuracyCallback\nfrom torch.utils.data import DataLoader\nfrom torchtext.data import Iterator\nfrom gensim.models import KeyedVectors\n\n\nclass BucketIteratorWrapper(DataLoader):\n    __initialized__ = False\n\n    def __init__(self, iterator: Iterator):\n        self.batch_size = iterator.batch_size\n        self.num_workers = 1\n        self.collate_fn = None\n        self.pin_memory = False\n        self.drop_last = False\n        self.timeout = 0\n        self.worker_init_fn = None\n        self.sampler = iterator\n        self.batch_sampler = iterator\n        self.__initialized__ = True\n\n    def __iter__(self):\n        return map(lambda batch: {\n            'features': batch.TEXT,\n            'targets': batch.LABEL,\n        }, self.batch_sampler.__iter__())\n\n    def __len__(self):\n        return len(self.batch_sampler)\n\n\nclass RNN(nn.Module):\n    def __init__(self, num_embeddings,\n                 embedding_dim=300,\n                 hidden_size=300,\n                 output_size=1,\n                 num_layers=1,\n                 dropout=0.2):\n        super().__init__()\n        model = KeyedVectors.load_word2vec_format('ch07/GoogleNews-vectors-negative300.bin', binary=True)\n        weights = torch.FloatTensor(model.vectors)\n        self.emb = nn.Embedding.from_pretrained(weights)\n        self.lstm = nn.LSTM(embedding_dim,\n                            hidden_size, num_layers,\n                            batch_first=True, dropout=dropout, bidirectional=True)\n        self.linear = nn.Sequential(\n            nn.Linear(hidden_size * 2, 100),\n            nn.PReLU(),\n            nn.BatchNorm1d(100),\n            nn.Linear(100, output_size)\n        )\n\n    def forward(self, x, h0=None):\n        x = self.emb(x)\n        x, h = self.lstm(x, h0)\n        x = x[:, -1, :]\n        x = self.linear(x)\n        return x\n\n\nTEXT = data.Field(sequential=True, lower=True, batch_first=True)\nLABELS = data.Field(sequential=False, batch_first=True, use_vocab=False)\n\ntrain, val, test = data.TabularDataset.splits(\n    path='ch06', train='train2.txt',\n    validation='valid2.txt', test='test2.txt', format='tsv',\n    fields=[('TEXT', TEXT), ('LABEL', LABELS)])\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\ntrain_iter, val_iter, test_iter = data.BucketIterator.splits(\n    (train, val, test), batch_sizes=(64, 64, 64), device=device, repeat=False, sort=False)\n\ntrain_loader = BucketIteratorWrapper(train_iter)\nvalid_loader = BucketIteratorWrapper(val_iter)\nloaders = {\"train\": train_loader, \"valid\": valid_loader}\n\nTEXT.build_vocab(train, min_freq=2)\nLABELS.build_vocab(train)\n\nmodel = RNN(len(TEXT.vocab.stoi) + 1, num_layers=2, output_size=4)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\nrunner = SupervisedRunner()\n\nrunner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    loaders=loaders,\n    logdir=\"./logdir\",\n    callbacks=[AccuracyCallback(num_classes=4, accuracy_args=[1])],\n    num_epochs=10,\n    verbose=True,\n)"}], "task": "code_generation", "index": 643, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_644", "instruction": "pythonを用いて、事前学習済み言語モデル（例えば[BERT](https://github.com/google-research/bert)など）を出発点として、ニュース記事見出しをカテゴリに分類するモデルを構築せよ。", "instances": [{"input": "", "output": "from tqdm import tqdm\nimport torch\nfrom torch import optim\nfrom torchtext import data\nfrom transformers import BertForSequenceClassification\n\n\ndef eval_net(model, data_loader, device='cpu'):\n    model.eval()\n    ys = []\n    ypreds = []\n    for x, y, _ in data_loader:\n        with torch.no_grad():\n            loss, logit = model(input_ids=x, labels=y)\n            _, y_pred = torch.max(logit, 1)\n            ys.append(y)\n            ypreds.append(y_pred)\n    ys = torch.cat(ys)\n    ypreds = torch.cat(ypreds)\n    print(f'test acc: {(ys == ypreds).sum().item() / len(ys)}')\n    return\n\n\nTEXT = data.Field(sequential=True, lower=True, batch_first=True)\nLABELS = data.Field(sequential=False, batch_first=True, use_vocab=False)\n\ntrain, val, test = data.TabularDataset.splits(\n    path='ch06', train='train2.txt',\n    validation='valid2.txt', test='test2.txt', format='tsv',\n    fields=[('TEXT', TEXT), ('LABEL', LABELS)])\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\ntrain_iter, val_iter, test_iter = data.Iterator.splits(\n    (train, val, test), batch_sizes=(64, 64, 64), device=device, repeat=False, sort=False)\n\nTEXT.build_vocab(train, min_freq=2)\nLABELS.build_vocab(train)\n\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4)\nmodel = model.to(device)\n\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\nfor epoch in tqdm(range(10)):\n    losses = []\n    model.train()\n    for batch in train_iter:\n        x, y = batch.TEXT, batch.LABEL\n        loss, logit = model(input_ids=x, labels=y)\n        model.zero_grad()\n        loss.backward()\n        optimizer.step()\n        losses.append(loss.item())\n        _, y_pred_train = torch.max(logit, 1)\n    eval_net(model, test_iter, device)"}], "task": "code_generation", "index": 644, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_645", "instruction": "shellscriptを用いて、機械翻訳のデータセットをダウンロードせよ。訓練データ、開発データ、評価データを整形し、必要に応じてトークン化などの前処理を行うこと。ただし、この段階ではトークンの単位として形態素（日本語）および単語（英語）を採用せよ。", "instances": [{"input": "", "output": "onmt_preprocess -train_src data/kyoto-train.ja -train_tgt data/kyoto-train.en -valid_src data/kyoto-dev.ja -valid_tgt data/kyoto-dev.en -save_data data/data -src_vocab_size 10000 -tgt_vocab_size 10000\nonmt_train  \\\n  -data data/data  \\\n  -save_model data/demo-model  \\\n  -train_steps 100000  \\\n  -world_size 1  \\\n  -gpu_ranks 0"}], "task": "code_generation", "index": 645, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_646", "instruction": "shellscriptを用いて、手持ちのデータを用いて、ニューラル機械翻訳のモデルを学習せよ（ニューラルネットワークのモデルはTransformerやLSTMなど適当に選んでよい）。", "instances": [{"input": "", "output": "onmt_translate  \\\n  -model data/demo-model_step_100000.pt  \\\n  -src data/kyoto-test.ja  \\\n  -output pred.txt  \\\n  -replace_unk  \\\n  -verbose  \\\n  -gpu 0"}], "task": "code_generation", "index": 646, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_647", "instruction": "shellscriptを用いて、ニューラル機械翻訳モデルを用い、与えられた（任意の）日本語の文を英語に翻訳するプログラムを実装せよ。", "instances": [{"input": "", "output": "# https://forum.opennmt.net/t/simple-opennmt-py-rest-server/1392\nexport IP=\"0.0.0.0\"\nexport PORT=5000\nexport URL_ROOT=\"/translator\"\nexport CONFIG=\"./available_models/conf.json\"\nexport HOST=\"127.0.0.1\"\n\n# NOTE that these parameters are optionnal\n# here, we explicitely set to default values\npython server.py --ip $IP --port $PORT --url_root $URL_ROOT --config $CONFIG\n\n# curl http://$HOST:$PORT$URL_ROOT/models\n\n# curl -i -X POST -H \"Content-Type: application/json\" \\\n#     -d '[{\"src\": \"本日 は 晴天 なり\", \"id\": 100}]' \\\n#     http://$HOST:$PORT$URL_ROOT/translate | perl -Xpne 's/\\\\u([0-9a-fA-F]{4})/chr(hex($1))/eg'"}], "task": "code_generation", "index": 647, "liscence": "MIT", "source": "nlp_100_knocks"}
{"id": "amenokaku_1048", "instruction": "pythonを用いて、引数として2つの数値を取り、その中で最大の数値を返す関数max()を定義してなさい。なお、Pythonで利用可能なif-then-else構文を使用すること。", "instances": [{"input": "", "output": "def max1(m,n):\n    if m>n:\n        return m \n    else:\n        return n "}], "task": "code_generation", "index": 1048, "liscence": "MIT", "source": "python_for_begginers_solve_50_exercises"}
{"id": "amenokaku_1049", "instruction": "pythonを用いて、引数として3つの数値を取り、その中で最大の値を返す関数max_of_three()を定義しなさい。", "instances": [{"input": "", "output": "def max_of_three(a,b,c):\n    if a>b and a>c:\n        print a \n    elif b>c:\n        print b \n    else:\n        print c "}], "task": "code_generation", "index": 1049, "liscence": "MIT", "source": "python_for_begginers_solve_50_exercises"}
{"id": "amenokaku_1050", "instruction": "pythonを用いて、与えられたリスト、もしくは文字列の長さを計算する関数を定義しなさい。", "instances": [{"input": "", "output": "def length(x):\n    c=0\n    for _ in a:\n        c = c +1 \n    return c "}], "task": "code_generation", "index": 1050, "liscence": "MIT", "source": "python_for_begginers_solve_50_exercises"}
{"id": "amenokaku_1051", "instruction": "pythonを用いて、長さ1の文字列（すなわち文字）を受け取り、それが母音であればTrueを、そうでなければFalseを返す関数を書きなさい。", "instances": [{"input": "", "output": "def vowel(x):\n    if x in ('aeiou'):\n        return True\n    else:\n        return False"}], "task": "code_generation", "index": 1051, "liscence": "MIT", "source": "python_for_begginers_solve_50_exercises"}
{"id": "amenokaku_1052", "instruction": "pythonを用いて、テキストを \"rövarspråket\"（スウェーデン語で \"強盗語\"）に翻訳する関数translate()を書いてください。このとき、すべての子音を二重にして、その間に \"o \"を入れるようにして下さい。例えば、translate(\"this is fun\")は \"totohisos isos fofunon \"という文字列を返すはずです。", "instances": [{"input": "", "output": "def translate(x):\n    s = ''\n    for i in x:\n        if i not in ('aeiou'):\n            s += i + \"o\" + i\n        else:\n            s += i \n    print s "}], "task": "code_generation", "index": 1052, "liscence": "MIT", "source": "python_for_begginers_solve_50_exercises"}
{"id": "amenokaku_1053", "instruction": "pythonを用いて、関数sum()と関数multiply()を定義して下さい。例えば、sum([1, 2, 3, 4])\nは10を返し、multiply([1, 2, 3, 4]) は24を返します。", "instances": [{"input": "", "output": "def sum1(x):\n    c=0\n    for i in x:\n        c += i \n    return c\n\nprint sum1([1, 2, 3, 4])\n\n\ndef multiply(x):\n    c=1\n    for i in x:\n        c *= i \n    return c"}], "task": "code_generation", "index": 1053, "liscence": "MIT", "source": "python_for_begginers_solve_50_exercises"}
{"id": "amenokaku_1054", "instruction": "pythonを用いて、文字列の反転を計算する関数reverse()を定義しなさい。例えば、reverse(\"I am testing\")は \"gnitset ma I \"という文字列を返します。", "instances": [{"input": "", "output": "def reverse(x):\n    new =[]\n    for i in range(len(x))[::-1]:\n        new.append(x[i])\n    print ''.join(new)"}], "task": "code_generation", "index": 1054, "liscence": "MIT", "source": "python_for_begginers_solve_50_exercises"}
{"id": "amenokaku_1055", "instruction": "pythonを用いて、回文を認識する関数 is_palindrome()を定義する。例えば、is_palindrome(\"radar\") は真を返す。", "instances": [{"input": "", "output": "def is_palindrome(x):\n    if x == x[::-1]:\n        print True\n    else:\n        print False"}], "task": "code_generation", "index": 1055, "liscence": "MIT", "source": "python_for_begginers_solve_50_exercises"}
{"id": "amenokaku_1056", "instruction": "pythonを用いて、値(数値、文字列など)xと値のリストaを受け取り、xがaのメンバであれば真を、そうでなければ偽を返す関数is_member()を書きなさい。", "instances": [{"input": "", "output": "def is_member(x, a):\n    if x in a:\n        print True\n    else:\n        print False"}], "task": "code_generation", "index": 1056, "liscence": "MIT", "source": "python_for_begginers_solve_50_exercises"}
{"id": "amenokaku_1057", "instruction": "pythonを用いて、2つのリストを受け取り、少なくとも1つのメンバが共通していればTrueを、そうでなければFalseを返す関数overlapping()を定義しなさい。is_member()関数やin演算子を使ってもかまいませんが、練習のために、2つの入れ子になったforループを使って書いてください。", "instances": [{"input": "", "output": "def overlapping(m,n):\n    l1 =len(m)\n    l2 = len(n)    \n    for i in range(l1):\n        for j in range(l2):\n            if m[i]==n[j]:\n                status =1\n                break\n        else:\n            status =0\n    if status ==1:\n        print True\n    else:\n        print False"}], "task": "code_generation", "index": 1057, "liscence": "MIT", "source": "python_for_begginers_solve_50_exercises"}
{"id": "amenokaku_1058", "instruction": "pythonを用いて、整数 n と文字 c を受け取り、c:s からなる n 文字の文字列を返す関数 generate_n_chars() を定義しなさい。例えば、generate_n_chars(5, \"x\")の場合、文字列 \"xxxxx \"を返す。", "instances": [{"input": "", "output": "def generate_n_chars(n,x):\n    k=[]\n    for i in range(n):\n        k.append(x)\n    print ''.join(k)"}], "task": "code_generation", "index": 1058, "liscence": "MIT", "source": "python_for_begginers_solve_50_exercises"}
{"id": "amenokaku_1059", "instruction": "pythonを用いて、整数のリストを受け取り、スクリーンにヒストグラムを表示する関数histogram() を定義してください。例えば、histogram([4, 9, 7])は以下のように表示します：\n\n****\n*********\n*******", "instances": [{"input": "", "output": "def histogram(x):\n    for i in x:\n        print i * '*'"}], "task": "code_generation", "index": 1059, "liscence": "MIT", "source": "python_for_begginers_solve_50_exercises"}
{"id": "amenokaku_1060", "instruction": "pythonを用いて、数のリストを受け取り、最大の数を返す関数max_in_list()を書きましょう。", "instances": [{"input": "", "output": "def max1(l):\n    max =0\n    for i in range(len(l)-1):\n        if l[i] > l[i+1] and l[i]>max:\n            max = l[i]\n        elif l[i+1]>max:\n            max = l[i+1]\n    print max"}], "task": "code_generation", "index": 1060, "liscence": "MIT", "source": "python_for_begginers_solve_50_exercises"}
{"id": "amenokaku_1061", "instruction": "pythonを用いて、単語のリストを、対応する単語の長さを表す整数のリストにマップするプログラムを書きなさい。例えば、maps(['apple','orange','cat'])とすると[5, 6, 3]が出力として返ってくる。", "instances": [{"input": "", "output": "def maps(x):\n    k=[]\n    for i in x:\n        k.append(len(i))\n    print k"}], "task": "code_generation", "index": 1061, "liscence": "MIT", "source": "python_for_begginers_solve_50_exercises"}
{"id": "amenokaku_1062", "instruction": "pythonを用いて、単語のリストを受け取り、最も長い単語の長さを返す関数 find_longest_word() を書きなさい。", "instances": [{"input": "", "output": "def maps(x):\n    k=[]\n    for i in x:\n        k.append(len(i))\n    print max(k)"}], "task": "code_generation", "index": 1062, "liscence": "MIT", "source": "python_for_begginers_solve_50_exercises"}
{"id": "amenokaku_1063", "instruction": "{merry\": \"god\", \"christmas\": \"jul\", \"and\": \"och\", \"happy\": \"gott\", \"new\": \"nytt\", \"year\": \"ar\"} のように、小さな対訳辞書をPython辞書として表現し、それを使ってクリスマスカードを英語からスウェーデン語に翻訳してみましょう。すなわち、英語の単語のリストを受け取り、スウェーデン語の単語のリストを返す関数translate()を書いて下さい。", "instances": [{"input": "", "output": "def tanslate(x):\n    new=[]\n    d = {\"merry\":\"god\",\n          \"christmas\":\"jul\", \n          \"and\":\"och\", \n          \"happy\":\"gott\", \n          \"new\":\"nytt\", \n          \"year\":\"ar\"}\n    l = x.split(' ')\n    for i in l:\n        new.append(d[i])\n    print new \n    print ' '.join(new)      "}], "task": "code_generation", "index": 1063, "liscence": "MIT", "source": "python_for_begginers_solve_50_exercises"}
{"id": "amenokaku_1064", "instruction": "pythonを用いて、単語のリストと整数nを受け取り、nより長い単語のリストを返す関数filter_long_words()を書きなさい。", "instances": [{"input": "", "output": "def filter_long_words(x,n):\n    k=[]\n    for i in x:\n        if len(i)>n:\n            k.append(i)\n    print k"}], "task": "code_generation", "index": 1064, "liscence": "MIT", "source": "python_for_begginers_solve_50_exercises"}
{"id": "amenokaku_1065", "instruction": "\"99 Bottles of Beer\"は、アメリカとカナダの伝統的な歌です。覚えやすく、長い時間をかけて歌うことができるため、長旅の際によく歌われます。簡単な歌詞は以下の通りです。 \"99 bottles of beer on the wall, 99 bottles of beer.\nTake one down, pass it around, 98 bottles of beer on the wall.\"\n同じ歌詞が繰り返され、そのたびに瓶が1本ずつ減っていく。歌い手または歌い手たちがゼロになったとき、歌は完成します。この歌のすべての節を生成できるPythonプログラムを書きなさい。", "instances": [{"input": "", "output": "def song():\n    print '99 bottles of beer on the wall, 99 bottles of beer'\n    for i in range(99)[::-1]:\n        print 'Take one down, pass it around,' + str(i) +   ' bottles of beer on the wall.'"}], "task": "code_generation", "index": 1065, "liscence": "MIT", "source": "python_for_begginers_solve_50_exercises"}
{"id": "amenokaku_1066", "instruction": "pythonを用いて、次のようなフレーズの回文も受け付ける回文認識器を書いてください。\n\"Go hang a salami I'm a lasagna hog.\", \"Was it a rat I saw?\", \"Step on no pets\", \"Sit on a potato pan, Otis\", \"Lisa Bonet ate no basil\", \"Satan, oscillate my metallic sonatas\", \"I roamed under it as a tired nude Maori\", \"Rise to vote sir\", or the exclamation \"Dammit, I'm mad!\"\nなお、句読点、大文字、スペースは通常無視すること。", "instances": [{"input": "", "output": "def palindrome(x):\n    l=[]\n    for i in x:\n        if i.isalpha():\n            l.append(i.lower())\n    print ''.join(l)\n    if l==l[::-1]:\n        print 'palindrome'\n    else:\n        print 'Not a palindrome'      "}], "task": "code_generation", "index": 1066, "liscence": "MIT", "source": "python_for_begginers_solve_50_exercises"}
{"id": "amenokaku_1067", "instruction": "パングラムとは、例えば、\"The quick brown fox jumps over the lazy dog. \"のように英語のアルファベットのすべての文字を少なくとも一度は含む文のことである。 pythonを用いて、文がパングラムかどうかをチェックする関数を書きなさい。", "instances": [{"input": "", "output": "def pangram(x):\n    for i in string.letters[:26]:\n        if i in x.lower():\n            status =1\n        else:\n            print 'not a pangram'\n            status =0\n            break\n        \n    if status==1:\n        print 'pangram'  "}], "task": "code_generation", "index": 1067, "liscence": "MIT", "source": "python_for_begginers_solve_50_exercises"}
{"id": "amenokaku_1068", "instruction": "pythonを用いて、文字列を受け取り、その中に含まれる文字の頻度リストを作成する関数 char_freq() を書きなさい。頻度リストはPython辞書として表現します。char_freq(\"abbabcbdbabdbababcbcbab\")のようにしてみてください。", "instances": [{"input": "", "output": "def char_freq(x):\n    d ={}\n    for i in x:\n        d[i] = d.get(i,0) +1\n    print d"}], "task": "code_generation", "index": 1068, "liscence": "MIT", "source": "python_for_begginers_solve_50_exercises"}
{"id": "amenokaku_1069", "instruction": "暗号学においてシーザー暗号とは、平文の各文字をアルファベットのある一定数下の文字に置き換えるという、非常に単純な暗号化技術であす。例えば、3をシフトすると、AはDに、BはEに、といった具合である。この方法はジュリアス・シーザーにちなんで命名されました。 ROT-13（\"rotate by 13 places\"）は、シフトが13のシーザー暗号の例として広く使われています。Pythonでは、ROT-13のキーは以下の辞書で表すことができます。\n\n key = {'a':'n', 'b':'o', 'c':'p', 'd':'q', 'e':'r', 'f'： s', 'g':'t', 'h':'u', 'i':'v', 'j':'w', 'k':'x', 'l':'y', 'm':'z', 'n':'a', 'o':'b', 'p':'c', 'q':'d', 'r':'e', 's':'f', 't':'g', 'u'： H', 'V':'I', 'W':'J', 'X':'K', 'Y':'L', 'Z':'M', 'A':'N', 'B':'O', 'C':'P', 'D':'Q', 'E':'R', 'F':'S', 'G':'T', 'H':'U', 'I':'V', 'J':'W', 'K'： x', 'l':'y', 'm':'z', 'n':'a', 'o':'b', 'p':'c', 'q':'d', 'r':'e', 's':'f', 't':'g', 'u':'h', 'v':'i', 'w':'j', 'x':'k', 'y':'l', 'z':'m' }. \n\npythonを用いて、ROT-13のエンコーダ/デコーダを実装して下さい。それができたら、次の秘密メッセージを読むことができるようになります：\n\n  Pnrfne pvcure? V zhpu cersre Pnrfne fnynq！\n\n英語には26の文字があるので、ROT-13プログラムは英語で書かれたテキストのエンコードとデコードの両方ができることに注意してください。", "instances": [{"input": "", "output": "def rot_decoder(x):\n    new =[]\n    d = {'a':'n', 'b':'o', 'c':'p', 'd':'q', 'e':'r', 'f':'s', 'g':'t', 'h':'u', \n       'i':'v', 'j':'w', 'k':'x', 'l':'y', 'm':'z', 'n':'a', 'o':'b', 'p':'c', \n       'q':'d', 'r':'e', 's':'f', 't':'g', 'u':'h', 'v':'i', 'w':'j', 'x':'k',\n       'y':'l', 'z':'m', 'A':'N', 'B':'O', 'C':'P', 'D':'Q', 'E':'R', 'F':'S', \n       'G':'T', 'H':'U', 'I':'V', 'J':'W', 'K':'X', 'L':'Y', 'M':'Z', 'N':'A', \n       'O':'B', 'P':'C', 'Q':'D', 'R':'E', 'S':'F', 'T':'G', 'U':'H', 'V':'I', \n       'W':'J', 'X':'K', 'Y':'L', 'Z':'M'}\n    for i in x:\n        new.append(d.get(i,i))\n    print ''.join(new)\n    \nrot_decoder('Pnrfne pvcure? V zhpu cersre Pnrfne fnynq!')\n\n# Our decoder function can also encode the message since we have 26 characters. But in case if isn't you can use below strategy. \ndef rot_encoder(x):\n    key_inverse = { v:k for k,v in d.items()} \n    for i in x:\n        new.append(d.get(i,i))\n    print ''.join(new)"}], "task": "code_generation", "index": 1069, "liscence": "MIT", "source": "python_for_begginers_solve_50_exercises"}
{"id": "amenokaku_1070", "instruction": "pythonを用いて、単純なスペル修正をする関数correct()を定義し、文字列を受け取って以下を確認しなさい。\n\t1) 2回以上出現するスペース文字を1回に圧縮する。\n\t2) ピリオドの後に直接文字が続く場合、ピリオドの後に余分なスペースを挿入する。\n例えば correct(\"This is very funny and cool.Indeed!\") は、\"This is very funny and cool. Indeed!\"を返します。", "instances": [{"input": "", "output": "import re\n\ndef correct(x):\n    x =x.replace('.', '. ')\n    x = re.sub(' +', ' ', x)\n    print x    \n\ncorrect (\"This   is  very funny  and           cool.Indeed!\")"}], "task": "code_generation", "index": 1070, "liscence": "MIT", "source": "python_for_begginers_solve_50_exercises"}
{"id": "amenokaku_1071", "instruction": "英語の三人称単数動詞の形は、不定詞の語幹に付加される接尾辞-sによって区別される。 簡単なルールを以下に示す： 動詞の末尾がyの場合、それを削除してiesを加える 動詞の末尾が o、ch、s、sh、x、z の場合、esを加える デフォルトではsを加えるだけである。ここでは不定詞形の動詞を与えるとその三人称単数形を返す関数 make_3sg_form()をpythonを用いて定義して下さい。try、brush、run、fixといった単語を使って関数をテストして下さい。ただし、ルールは発見的なものであり、すべてのケースで機能するとは思わないで下さい。ヒント 文字列メソッド endswith() をチェックして下さい。", "instances": [{"input": "", "output": "def make_3sg_form(x):\n    if x.endswith('y'):\n        x = x[:-1] + 'ies'\n    elif x.endswith( ('o', 'ch', 's', 'sh', 'x', 'z')):\n        x += 'es'\n    else:\n        x += 's'\n    print x"}], "task": "code_generation", "index": 1071, "liscence": "MIT", "source": "python_for_begginers_solve_50_exercises"}
{"id": "amenokaku_1072", "instruction": "英語では、現在分詞は無限の形に接尾辞-ingをつけることで形成されます（go -> going）。  簡単な発見的規則は以下の通りです（動詞が e で終わる場合、e を捨てて ing を加える（例外でない場合：be, see, flee, knee など）。動詞が ie で終わる場合、ie を y に変えて ing を加える 子音-母音-子音で構成される。単語の場合、最後の文字を二重にしてから ing を加える デフォルトでは ing を加える）。ここでは、不定詞形の動詞が与えられたときにその現在分詞形を返す関数 make_ing_form() をpythonを用いて定義しなさい。また、lie、see、move、hugなどの単語を使って関数をテストしてください。 ただし、このような単純なルールがすべてのケースで機能わけではありません。", "instances": [{"input": "", "output": "def make_ing_form(x):\n    if x.endswith('e')  and not x.endswith('ie') and not x.endswith('ee') and not len(x)==2:\n        x = x[:-1] + 'ing'\n    elif x.endswith('ie'):\n        x = x[:-2] + 'ying'\n    elif len(x)==3 and x[-2] in ('aeiou') and x[-1] not in ('aeiou') and x[-3] not in ('aeiou'):\n        x += x[-1] + 'ing'\n    else:\n        x += 'ing'\n    print x\n\nmake_ing_form('flee')"}], "task": "code_generation", "index": 1072, "liscence": "MIT", "source": "python_for_begginers_solve_50_exercises"}
{"id": "amenokaku_1073", "instruction": "pythonを用いて、関数 reduce() を使って、数値のリストを受け取り、最大のものを返す関数 max_in_list() を書きなさい。", "instances": [{"input": "", "output": "def max_in_list(l):\n    largest = reduce( lambda x,y: max(x,y) , l)\n    return largest \n    \nl = [1,2,3,78,34,90,36,9]\n\nprint max_in_list(l)"}], "task": "code_generation", "index": 1073, "liscence": "MIT", "source": "python_for_begginers_solve_50_exercises"}
{"id": "amenokaku_1074", "instruction": "pythonを用いて、単語のリストを、対応する単語の長さを表す整数のリストにマップするプログラムを書きなさい。なお、これを3通りの方法で書きなさい。\n\n1) forループを使う\n2) 高階関数 map() を使う\n3) リスト内包を使う", "instances": [{"input": "", "output": "#1\ndef maps(x):\n    k=[]\n    for i in x:\n        k.append(len(i))\n    print k\n\n#2\nl = ['apple', 'orange', 'cat']\nprint map( lambda x : len(x), l)\n\n#3\nprint [ len(i) for i in l]"}], "task": "code_generation", "index": 1074, "liscence": "MIT", "source": "python_for_begginers_solve_50_exercises"}
{"id": "amenokaku_1075", "instruction": "pythonを用いて、単語のリストを受け取り、最も長い単語の長さを返す関数 find_longest_word() を書きなさい。高階の関数だけを使いなさい。", "instances": [{"input": "", "output": "def find_longest_word(words):\n    return max(map(len, words))\n\n# 使用例\nwords = [\"apple\", \"banana\", \"date\"]\nprint(find_longest_word(words))  # 6 と表示される (banana の長さ)"}], "task": "code_generation", "index": 1075, "liscence": "MIT", "source": "python_for_begginers_solve_50_exercises"}
{"id": "amenokaku_1076", "instruction": "pythonを用いて、高階関数filter()を使って、関数filter_long_words()を書きなさい。\nこの関数は、単語のリストと整数nを受け取り、nより長い単語のリストを返します。", "instances": [{"input": "", "output": "n=2\nx = ['abc','b','adfadfasd']\nprint filter(lambda x: len(x)>n, x)"}], "task": "code_generation", "index": 1076, "liscence": "MIT", "source": "python_for_begginers_solve_50_exercises"}
{"id": "amenokaku_1077", "instruction": "{merry\": \"god\", \"christmas\": \"jul\", \"and\": \"och\", \"happy\": \"gott\", \"new\": \"nytt\", \"year\": \"ar\"} のように小さな対訳辞書をPython辞書として表現し、それを使ってクリスマスカードを英語からスウェーデン語に翻訳してみましょう。高次関数 map() を使って、英語の単語のリストを受け取り、スウェーデン語の単語のリストを返す関数 translate() をpythonを用いて書いてください。", "instances": [{"input": "", "output": "def translate(x):\n    d ={\"merry\":\"god\", \n        \"christmas\":\"jul\",\n        \"and\":\"och\", \n        \"happy\":\"gott\",\n        \"new\":\"nytt\",\n        \"year\":\"ar\"}\n    l = x.split()\n    print map ( lambda x: d[x], l)"}], "task": "code_generation", "index": 1077, "liscence": "MIT", "source": "python_for_begginers_solve_50_exercises"}
{"id": "amenokaku_1078", "instruction": "pythonを用いて、高階関数 map()、filter()、reduce() を実装しなさい。", "instances": [{"input": "", "output": "def map1(f, l):\n    new=[]\n    for i in l:\n        new.append(f(i))\n    return new\n        \n    \ndef filter1(f, l):\n    new =[]\n    for i in l:\n        if f(i) == True:\n            new.append(i)\n    return new\n        \ndef reduce1(f, l):\n    new = l[0]\n    for i in l[1:]:\n        new = f(new, i)\n    return new"}], "task": "code_generation", "index": 1078, "liscence": "MIT", "source": "python_for_begginers_solve_50_exercises"}
{"id": "amenokaku_1079", "instruction": "pythonを用いて、ユーザーからファイル名を受け取り、各行を読み、回文であればその行をスクリーンに表示する回文認識器を書きなさい。", "instances": [{"input": "", "output": "def palindrome1(x):\n    for i in open(x).read().split('\\n'):\n        if i==i[::-1]:\n            print i + \" is palindrome\"\n\ndef palindrome2(x):\n    for i in open(x).readlines():\n        i = i.rstrip()\n        if i==i[::-1]:\n            print i + \" is palindrome\""}], "task": "code_generation", "index": 1079, "liscence": "MIT", "source": "python_for_begginers_solve_50_exercises"}
{"id": "amenokaku_1080", "instruction": "ウィキペディアによると、セモルトニラップとは、異なる単語やフレーズを逆に綴った単語やフレーズのことである（「セモルトニラップ」はそれ自体、「回文」を逆に綴ったものである）。 ここではpythonを用いて、ユーザーからファイル名(単語のリストを指す)を受け取り、セモルドニラップである単語のペアをすべて見つけて画面に表示するセモルドニラップ判定関数を書いてください。たとえば、\"stressed \"と \"desserts \"が単語リストの一部であれば、出力には \"stressed desserts \"というペアが含まれるはずである。ちなみに、それぞれのペアはそれ自体で回文を形成します。", "instances": [{"input": "", "output": "def semordnilap(x):\n    f = open(x).read()\n    words = f.split('\\n') \n    while words:\n        a = words[0]\n        words.remove(a)\n        if a[::-1] in words:\n            print a + ' and ' + a[::-1] + ' are semordnilap'"}], "task": "code_generation", "index": 1080, "liscence": "MIT", "source": "python_for_begginers_solve_50_exercises"}
{"id": "amenokaku_1081", "instruction": "pythonを用いて、char_freq_table()関数を書き、実行して、ユーザーからファイル名を受け取り、そのファイルに含まれる文字の頻度表を作成し、ソートされ、きれいにフォーマットされた文字頻度表をスクリーンに表示しなさい。", "instances": [{"input": "", "output": "example = __import__('21')\n\ndef char_freq_table():\n    n = raw_input('Please enter a file name: ')\n    example.char_freq(open(n).read())"}], "task": "code_generation", "index": 1081, "liscence": "MIT", "source": "python_for_begginers_solve_50_exercises"}
{"id": "amenokaku_1082", "instruction": "国際民間航空機関(ICAO)のアルファベットは、英語のアルファベットに発音記号を割り当てている(アルファはA、ブラボーはBなど)。これは、特に航行や人の安全が重要な場合に、母国語に関係なく無線や電話で音声メッセージを送受信する人が、重要な文字(と数字)の組み合わせを発音して理解できるようにするためである。以下は、ICAOアルファベットの1つのバージョンをカバーするPython辞書です： \n\nd = {'a':'alfa', 'b':'bravo', 'c':'charlie', 'd':'delta', 'e':'echo', 'f':'foxtrot', 'g':'golf', 'h':'hotel', 'i':'india', 'j':'juliett', 'k':'kilo', 'l':'lima', 'm':'mike', 'n'： 'november', 'o':'oscar', 'p':'papa', 'q':'quebec', 'r':'romeo', 's':'sierra', 't':'tango', 'u':'uniform', 'v':'victor', 'w':'whiskey', 'x':'x-ray', 'y':'yankee', 'z':'zulu'}. \n\nここでは、任意のテキスト（すなわち任意の文字列）をICAOの話し言葉に翻訳できる手続きspeak_ICAO()をpythonを用いて書きなさい。少なくとも2つのライブラリをインポートする必要があります（osとtimeです）。 os.system('say'+msg)においてmsgは発話される文字列です（UNIX/LinuxやWindowsでは、似たようなものがあるかもしれません）。発話されるテキストとは別に、関数はさらに2つのパラメータを受け取る必要があります（発話されるICAO単語の間のポーズの長さを示すfloatと、発話されるICAO単語の間のポーズの長さを示すfloatです）。", "instances": [{"input": "", "output": "import pyttsx\n\ndef speak_ICAO(x):\n    d = {'a':'alfa', 'b':'bravo', 'c':'charlie', 'd':'delta', 'e':'echo', 'f':'foxtrot',\n     'g':'golf', 'h':'hotel', 'i':'india', 'j':'juliett', 'k':'kilo', 'l':'lima',\n     'm':'mike', 'n':'november', 'o':'oscar', 'p':'papa', 'q':'quebec', 'r':'romeo',\n     's':'sierra', 't':'tango', 'u':'uniform', 'v':'victor', 'w':'whiskey', \n     'x':'x-ray', 'y':'yankee', 'z':'zulu'}\n    engine = pyttsx.init()\n    for i in x.lower():\n        engine.say(d.get(i,i))\n        engine.runAndWait()"}], "task": "code_generation", "index": 1082, "liscence": "MIT", "source": "python_for_begginers_solve_50_exercises"}
{"id": "amenokaku_1083", "instruction": "ハパックス・レゴメノン（しばしばハパックスと略される）とは、ある言語の文字記録、ある著者の作品、または1つのテキストのいずれかに一度だけ出現する単語のことである。テキストのファイル名を与えると、そのすべてのハパックスを返す関数をpythonを用いて書いててください。", "instances": [{"input": "", "output": "def hapax(x):\n    d={}\n    f = open(x)\n    for word in re.findall('\\w+', f.read().lower()):\n        d[word] = d.get(word,0) + 1\n    f.close()\n    print [ k for k, v in d.items() if v==1]"}], "task": "code_generation", "index": 1083, "liscence": "MIT", "source": "python_for_begginers_solve_50_exercises"}
{"id": "amenokaku_1084", "instruction": "pythonを用いて、テキストファイルが与えられたら、元のファイルのすべての行に1からnまでの番号を付けた新しいテキストファイルを作成するコードを書きなさい。", "instances": [{"input": "", "output": "f = open('sample.txt')\n\nf_out = open('sample_out.txt', 'w')\n\ncount =1\nfor i in f:\n    print i\n    f_out.write(str(count) + '. ' + i)\n    count +=1\n    \nf.close()\nf_out.close()"}], "task": "code_generation", "index": 1084, "liscence": "MIT", "source": "python_for_begginers_solve_50_exercises"}
{"id": "amenokaku_1085", "instruction": "pythonを用いて、ファイルに格納されたテキストの平均単語長を計算するプログラムを書きなさい（すなわち、テキスト中の単語トークンの長さの合計を単語トークンの数で割ったもの）。", "instances": [{"input": "", "output": "import re\n\ndef avg_length(x):\n    count = 0.0\n    f = open(x)\n    words = re.findall('\\w+', f.read())\n    for word in words:\n        count += len(word)\n    return count/len(words)"}], "task": "code_generation", "index": 1085, "liscence": "MIT", "source": "python_for_begginers_solve_50_exercises"}
{"id": "amenokaku_1086", "instruction": "pythonを用いて、1から20までの数字がランダムに選ばれる「数字当てゲーム」をプレイできるプログラムを書きなさい。", "instances": [{"input": "", "output": "# Method 1 - using while loop\nimport random\n\nname = raw_input(\"Hello! What is your name?\\n\")\nprint \"Well, \" + name + \", I am thinking of a number between 1 and 20\"\nno = random.randint(1,20)\nguess = int(raw_input(\"Take a guess\\n\"))\ncount =1\n\nwhile guess != no:    \n    if guess < no:\n        print \"Your guess is too low.\"                 \n    if guess > no:\n        print \"Your guess is too high\"\n    count +=1\n    guess = int(raw_input(\"Take a guess\\n\"))\n\nprint \"Good job, %s! You guessed my number in %d guesses!\" % (name ,count)\n\n\n#Method 2 using recursion. Here global makes count available to local scope of your function.\nimport random\n\ndef check():\n    global count  \n    guess = int(raw_input(\"Take a guess\\n\"))\n    if guess == no:\n        print \"Good job, %s! You guessed my number in %d guesses!\" %(name,count)\n    if guess < no:\n        print \"Your guess is too low.\"\n        count +=1        \n        check()          \n    if guess > no:\n        print \"Your guess is too high\"\n        count +=1\n        check()\n\nname = raw_input(\"Hello! What is your name?\\n\")\nprint \"Well, \" + name + \", I am thinking of a number between 1 and 20\"\nno = random.randint(1,20)\nprint no\nglobal count\ncount  =1\ncheck()"}], "task": "code_generation", "index": 1086, "liscence": "MIT", "source": "python_for_begginers_solve_50_exercises"}
{"id": "amenokaku_1087", "instruction": "アナグラムとは言葉遊びの一種で、単語やフレーズの文字を並べ替えて、元の文字をすべて一度だけ使って新しい単語やフレーズを作ることである。次のようなPythonプログラムを書きなさい。\n  1) 与えられた単語リストから単語wをランダムに選ぶ、 \n  2) w をランダムに並べ替える（つまり w のアナグラムを作る）、 \n  3) アナグラムをユーザーに提示する。\n  4) ユーザーに元の単語を推測させる対話型ループに入る。\n  プログラムの実行例は次のようになる：\n\n>>> import anagram\nColour word anagram: onwbr\nGuess the colour word!\nblack\nGuess the colour word!\nbrown\nCorrect!", "instances": [{"input": "", "output": "import random\nimport itertools\n\ndef anagram(x):\n    l=[]\n    word = random.choice(x)\n    anagrams = itertools.permutations(word)\n    for i in anagrams:\n        l.append(''.join(i))\n        anagram = random.choice(l)    #again randomly choose the anagram otherwise it would always contain the last permutation  \n    print 'Colour word anagram: %s' %anagram\n    guess = raw_input('Guess the colour word!\\n')\n    while guess!=word:\n            guess = raw_input('Guess the colour word!\\n')\n    print 'Correct!'"}], "task": "code_generation", "index": 1087, "liscence": "MIT", "source": "python_for_begginers_solve_50_exercises"}
{"id": "amenokaku_1088", "instruction": "lingoのゲームでは、5文字の隠された単語があります。ゲームの目的は \n推測によってこの単語を見つけ、2種類のヒントを受け取ることです。 \n\n    1) 正体も位置も完全に正しい文字\n    2) 単語の中に確かに存在するが、間違った位置に置かれた文字\n\npythonを用いて、lingoで遊べるプログラムを書きなさい。1)の意味で正しい文字をマークするには角括弧を使い、2)の意味で正しい文字をマークするには普通の括弧を使う。\n\n 例えば、プログラムが「虎」という単語を隠していると仮定すると次のように対話できるはずである。\n\n>>> import lingo\nsnake\nClue: snak(e)\nfiest\nClue: f[i](e)s(t)\ntimes\nClue: [t][i]m[e]s\ntiger\nClue: [t][i][g][e][r]", "instances": [{"input": "", "output": "def lingo(x):\n    n = raw_input('Please input your five letter guess!\\n')\n    \n    while n != x:\n        output =''\n        for index, i in enumerate(n):\n            if i in x:\n                if n[index]==x[index]:\n                    output += '[' + i + ']'\n                else:\n                    output += '(' + i + ')'\n            else:\n                output += i \n        print 'Clue:' + output  \n        n = raw_input('Please input your five letter guess!\\n') \n    print 'Success'    \n            \nlingo('snake')"}], "task": "code_generation", "index": 1088, "liscence": "MIT", "source": "python_for_begginers_solve_50_exercises"}
{"id": "amenokaku_1089", "instruction": "文分割関数は、テキストを文に分割することができるプログラムである。\n文分割のための標準的なヒューリスティック・セットには、以下のルールが含まれる（ただし、これらに限定されるものではない）。文の区切りは\".\"（ピリオド）、\"?\"、\"!\"のいずれかで行われる。\n\n1. ピリオドに続く空白と小文字は文の境界ではない。\n2. ピリオドの後に空白がなく数字が続くものは文の境界ではない。\n3. ピリオドの後に空白があり、その後に大文字が続くが、その前に短いタイトルのリストがあるものは文の区切りではない。\n   タイトルの例としては、Mr.、Mrs.、Dr.などがある。\n4. 空白が隣接していない文字のシーケンスに含まれるピリオドは、文の境界ではありません(例えば、www.aptex.com や e.g）。\n5. ピリオドに続くある種の句読点（特にカンマとそれ以上のピリオド）は、おそらく文の境界ではない。\n\nここでテキストファイルの名前を与えると、その内容を各文章を別々の行に書くことができるプログラムを書きなさい。\n以下の短い文章でプログラムをテストしてください：\n\nMr. Smith bought cheapsite.com for 1.5 million dollars, i.e. he paid a lot for it. Did he mind? \nAdam Jones Jr. thinks he didn't. In any case, this isn't true... Well, with a probability of .9 it isn't. \n\n結果は以下のようになるはずです。\n\nMr. Smith bought cheapsite.com for 1.5 million dollars, i.e. he paid a lot for it.\nDid he mind?\nAdam Jones Jr. thinks he didn't.\nIn any case, this isn't true...\nWell, with a probability of .9 it isn't.", "instances": [{"input": "", "output": "import re \n\ntext = \"Mr. Smith bought cheapsite.com for 1.5 million dollars, i.e. he paid a lot for it. \\\nDid he mind? Adam Jones Jr. thinks he didn't. In any case, \\\nthis isn't true... Well, with a probability of .9 it isn't.\"\n\n# Method 1\nfor i in re.findall(r'[A-Z][a-z]+\\.?.*?[.?!](?= [A-Z]|$)', text):\n    print i\n\nprint '*'*80\n\n#Method 2 - using verbose mode. \nfor i in re.findall(r'''\n\t\t\t\t\t\t[A-Z][a-z]+\\.?   # Starts with Capital includes (Mr., Mrs.)\n                        .*?              # followed by anything\n                        [.?!]            # ends with a (.)(?)(!)\n                        (?=\\s[A-Z]|$)    # is followed by whitespace and a capital letter\n                     ''', text, re.X):\n    print i\n\nprint '*'*80 \n\n#Method 3\nfor i in re.split(r'(?<=[^Mr|Mrs|Dr][.?!])\\s(?=[A-Z])', text):\n    print i"}], "task": "code_generation", "index": 1089, "liscence": "MIT", "source": "python_for_begginers_solve_50_exercises"}
{"id": "amenokaku_1090", "instruction": "アナグラムとは言葉遊びの一種で、単語や語句の文字を並べ替えた結果、元の文字をすべて正確に一度だけ使って、新しい単語や語句を作り出すものである。 http://www.puzzlers.org/pub/wordlists/unixdict.txt の単語リストを使って、同じ文字を共有し、その中に最も多くの単語を含む単語の集合を見つけるpythonプログラムを書きなさい。", "instances": [{"input": "", "output": "import urllib\nimport time\n\n\ndef anagram():\n    start = time.clock()\n    \n    f = urllib.urlopen('http://www.puzzlers.org/pub/wordlists/unixdict.txt')\n    words = set(f.read().split('\\n'))    \n    d = { ''.join(sorted(i)):[] for i in words}\n    \n    for i in words:\n        d[''.join(sorted(i))].append(i)    \n    max_len = max( len(v) for v in d.values())    \n    \n    for v in d.values():\n        if len(v)==max_len:\n            print v \n            \n    end = time.clock()\n    print end - start "}], "task": "code_generation", "index": 1090, "liscence": "MIT", "source": "python_for_begginers_solve_50_exercises"}
{"id": "amenokaku_1091", "instruction": "N個の開始括弧(\"[\")とN個の終了括弧(\"]\")を持つ文字列を、任意の順序で生成しなさい。\n生成された文字列が整合しているかどうか、つまり、開括弧と閉括弧のペア（その順序で）だけで構成されているかどうかをpythonを用いて判定しなさい。\n\n例：\n\n   []        OK   ][        NOT OK\n   [][]      OK   ][][      NOT OK\n   [[][]]    OK   []][[]    NOT OK", "instances": [{"input": "", "output": "def find_balanced(n):\n    count_left, count_right = 0,0\n    s = [random.choice(['[',']']) for i in range(int(n))]\n    for i in s:\n        if count_left == count_right:\n            if i ==']':\n                return ''.join(s) + ' not balanced'\n        if i =='[':\n            count_left +=1\n        else:\n            count_right +=1\n    if count_left == count_right:\n        return ''.join(s) + '  balanced'\n    else:\n        return ''.join(s) + ' not balanced'"}], "task": "code_generation", "index": 1091, "liscence": "MIT", "source": "python_for_begginers_solve_50_exercises"}
{"id": "amenokaku_1092", "instruction": "ある子供向けゲームでは、特定のカテゴリーの単語から始める。参加者は順番に単語を言うが、その単語は前の単語の最後の文字で始まらなければならない。 一度言った単語を繰り返すことはできない。相手がそのカテゴリーの単語を出せなかった場合は、ゲームから脱落する。例えば、「動物」をカテゴリーとすると以下となる。\n\nChild 1: dog \nChild 2: goldfish\nChild 1: hippopotamus\nChild 2: snake\n\npythonを用いて、ウィキペディアのポケモン一覧から抜粋した70の英語のポケモン名から、後続の名前が先行する名前の最後の文字で始まる、可能な限り数の多いポケモン名の並びを生成しなさい。ポケモンではない名前は繰り返さないです。\n\naudino bagon baltoy banette bidoof braviary bronzor carracosta charmeleon\ncresselia croagunk darmanitan deino emboar emolga exeggcute gabite\ngirafarig gulpin haxorus heatmor heatran ivysaur jellicent jumpluff kangaskhan\nkricketune landorus ledyba loudred lumineon lunatone machamp magnezone mamoswine\nnosepass petilil pidgeotto pikachu pinsir poliwrath poochyena porygon2\nporygonz registeel relicanth remoraid rufflet sableye scolipede scrafty seaking\nsealeo silcoon simisear snivy snorlax spoink starly tirtouga trapinch treecko\ntyrogue vigoroth vulpix wailord wartortle whismur wingull yamask", "instances": [{"input": "", "output": "# Method 1\ndef find(chain):\n    last_character = chain[-1][-1]\n    options = d[last_character] - set(chain)\n    \n    if not options:\n        return chain\n    else:\n        return max( (find(chain+[i]) for i in options), key=len)            \n    \n \nstart = time.clock()   \n\nd = defaultdict(set)       \nfor word in pokemon:\n    d[word[0]].add(word)\n\nprint max( (find([word]) for word in pokemon), key=len)\n\nend = time.clock()\nprint end - start\n\n\n\n# Method 2 try bottom down approach\ndef find2(chain):\n    first_character = chain[0][0]\n    options = d[first_character] -set(chain)\n    \n    if not options:\n        return chain\n    else:\n        return max( (find2([i]+ chain) for i in options), key=len)\n    \nstart = time.clock()\nd = defaultdict(set)\nfor word in pokemon:\n    d[word[-1]].add(word)\n      \nprint max( (find2([word]) for word in pokemon), key=len)\nend = time.clock()\nprint end - start \n\n\n\n# Method 3 - Using loop instead of generator expression\ndef find(chain):\n    l=[]\n    last_character = chain[-1][-1]\n    options = d[last_character] - set(chain)\n    \n    if not options:\n        return chain\n    else:\n        for i in options:\n            l.append(find(chain+[i]))\n        return max(l, key=len)\n            \n        #return [ find(chain+[i]) f=or i in options]        \n        \npokemon = set(pokemon)\n\nd = defaultdict(set)       \nfor word in pokemon:\n    d[word[0]].add(word)\n\nprint max( [find([word]) for word in pokemon], key=len)\n\n\n\n\n\n# Just try it out to have a better understanding how return plays an important role in recursion\n\ndef find(chain):\n    last_character = chain[-1][-1]\n    options = d[last_character] - set(chain)\n    \n    if not options:\n        return chain\n    else:\n        for i in options:\n            find(chain+[i])\n    #to understand importance of return, here once we are in else block nothing is returned       \n    \n    \npokemon = set(pokemon)\n\nd = defaultdict(set)       \nfor word in pokemon:\n    d[word[0]].add(word)\n\nprint [find([word]) for word in pokemon]"}], "task": "code_generation", "index": 1092, "liscence": "MIT", "source": "python_for_begginers_solve_50_exercises"}
{"id": "amenokaku_1093", "instruction": "オルタネート（alternate）とは、アルファベットを厳密な順序で交互に並べ、元の単語と同じ順序で使用することで、少なくとも他の2つの単語を構成する単語のことである。すべての文字が使われなければならないが、小さい単語は必ずしも同じ長さである必要ではありません。 例えば、7文字の単語で、2文字目以降がすべて使われると、4文字の単語と3文字の単語ができます。以下に2つの例を挙げます：\n\n    \"board\": makes \"bad\" and \"or\".\n    \"waists\": makes \"wit\" and \"ass\".\n\nhttp://www.puzzlers.org/pub/wordlists/unixdict.txt にある単語リストを使って、リスト内の各単語を調べ、2文字目以降の文字を使って2つのより小さい単語を作ろうとするpythonプログラムを書きなさい。小さい単語もリストのメンバーでなければなりません。 上記の方法で単語を画面に表示して下さい。", "instances": [{"input": "", "output": "import urllib2\nimport time\n\ndata = urllib2.urlopen('http://www.puzzlers.org/pub/wordlists/unixdict.txt').read()\nwords_list = set(data.split('\\n')) #Try chaging set to list \"list(data.split('\\n'))\" and notice the difference in timings. \n\nstart = time.clock()\nfor line in words_list:\n    total =[]\n    alternate, alternate2 = '',''\n    for i in range(0,len(line),2):\n        alternate = alternate + line[i]\n    if alternate in words_list and len(alternate)>2:\n        total.append(alternate)        \n    for i in range(1,len(line),2):\n        alternate2 = alternate + line[i]\n    if alternate2 in words_list and len(alternate2)>2:\n        total.append(alternate2)\n    if len(total)==2:\n        print \"%s: makes %s and %s\" %(line,alternate, alternate2)\nend = time.clock()\nprint end-start"}], "task": "code_generation", "index": 1093, "liscence": "MIT", "source": "python_for_begginers_solve_50_exercises"}
{"id": "amenokaku_1094", "instruction": "pythonを用いて、Gmailユーザーにメールを送信するスクリプトを作りなさい。最後にそれを爆弾に変換しなさい。このスクリプトは、ユーザーに50通のメールを送ることができれば爆弾と認定されます。", "instances": [{"input": "", "output": "import smtplib\n\ndef send_email():\n    # Enter your login credentials below\n    username = 'sendanonymous90'\n    password = '*******'\n    FROM = 'sendanonymous90@gmail.com'\n    TO =  'receivetestmail@gmail.com '\n    SUBJECT = 'TEST'\n    TEXT = 'Hi there, you have received a lottery of 1000$.'\n    \n    message  = \"\"\"From: %s\\nTo: %s\\nSubject: %s\\n\\n%s\"\"\" %(FROM, TO, SUBJECT, TEXT)    \n    \n    server = smtplib.SMTP(\"smtp.gmail.com\", 587)\n    server.starttls()\n    server.login(username, password)\n    \n    count =0\n    for i in range(500):\n        server.sendmail(FROM, TO, message)\n        print 'Success'\n        count +=1\n        print count \n\n    server.close()\n    \n\nsend_email()"}], "task": "code_generation", "index": 1094, "liscence": "MIT", "source": "python_for_begginers_solve_50_exercises"}
{"id": "amenokaku_1095", "instruction": "pythonを用いて、タイムライン上の投稿にコメントをつけるスクリプトを作りなさい。このスクリプトは、1つの投稿に50回コメントすることができれば爆弾と認定します。", "instances": [{"input": "", "output": "import requests\nimport json\n\n#replace with your access token, to be found at https://developers.facebook.com/tools/explorer\naccesstoken = 'CAACEdEose0cBAEP7zZCdStYw0TYVpnviOFbZB6XuZAEZCdbZBWjAqZAE2s8QVJW646QZB7u3nAxipKIjKhtZAmsmRSUAZCSV731ZAhrIBvTKxpFRsW4yUoSt1R7TnUnmcb83TYBcY2u3KDbSVZB2gtBdZBQ0E4zPw22vAk1ms9gjwvl3yIjCTYQ4tuh30WARyuhBXiSJH20CbrZBUUwZDZD'\n\nquery = 'SELECT post_id , created_time FROM stream where source_id = me()'\nd = { 'access_token': accesstoken, 'q':query}\n\nbase_url = 'https://graph.facebook.com'\nr = requests.get(base_url + \"/fql\", params=d)\nresult = json.loads(r.text)\nposts = result['data']\npost1 = posts[0]\n\ncomments_url = base_url + '/{}/comments'.format(post1['post_id'])\n\ncount =1\nfor i in range(50):\n    msg = {'access_token': accesstoken, 'message': 'Bomb' + str(count)}\n    requests.post(comments_url, data = msg)\n    count+=1\n    print count"}], "task": "code_generation", "index": 1095, "liscence": "MIT", "source": "python_for_begginers_solve_50_exercises"}
{"id": "amenokaku_1096", "instruction": "pythonを用いて、YouTubeのビデオをダウンロードするスクリプトを作ってください。ビデオは将来の使用のために保存しておきます。", "instances": [{"input": "", "output": "from urllib2 import urlopen\nfrom urlparse import unquote\nimport re\n\nurl ='https://www.youtube.com/watch?v=Wt1wFf53e2o'\nresponse = urlopen(url)\ncontent = response.read()\ndirect_urls = re.findall(r'\"url_encoded_fmt_stream_map\":.*?url=(https.*?)(,|\\\\u0026)', content)\n\ndirect_url = unquote(direct_urls[0][0])\n\nd = urlopen(direct_url)\nf = open('Test_video.avi', 'wb')\n\n#This will save the video to your default path. To change path use os.chdir(path)\n#Also note this will read the whole file into the memory and than write. We did this to make it words shortest script.\n#Instead read a chunk and write till complete. \nf.write(d.read())\n\nd.close()\nf.close()"}], "task": "code_generation", "index": 1096, "liscence": "MIT", "source": "python_for_begginers_solve_50_exercises"}
{"id": "amenokaku_2341", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "ProdutoAdmin ( admin . ModelAdmin ) :\n    list_display = ( \"str\" , \"str\" , \"str\" )\n    list_filter = [ \"str\" , \"str\" ]\n    search_fields = [ \"str\" ]\n    def produto_grupo_display ( self , obj ) :\n        grupo = Grupo . objects . get ( pk = obj . grupo_id )\n        return grupo . nome_grupo\n    produto_grupo_display . short_description = \"str\"\n", "output": "class ProdutoAdmin ( admin . ModelAdmin ) :\n    list_display = ( \"str\" , \"str\" , \"str\" )\n    list_filter = [ \"str\" , \"str\" ]\n    search_fields = [ \"str\" ]\n    def produto_grupo_display ( self , obj ) :\n        grupo = Grupo . objects . get ( pk = obj . grupo_id )\n        return grupo . nome_grupo\n    produto_grupo_display . short_description = \"str\"\n"}], "task": "code_fix", "index": 2341, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_2887", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "class HouseForm ( ModelForm ) :\n    captcha = ReCaptchaField ( )\n    class Meta :\n        model = House exclude = [ ]", "output": "class HouseForm ( ModelForm ) :\n    captcha = ReCaptchaField ( )\n    class Meta :\n        model = House\n        exclude = [ ]\n"}], "task": "code_fix", "index": 2887, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_2783", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "def main ) :\n    with open \"str\" , \"str\" ) as inputFile :\n        data = json . load ( inputFile )\n", "output": "def main ( ) :\n    with open ( \"str\" , \"str\" ) as inputFile :\n        data = json . load ( inputFile )\n"}], "task": "code_fix", "index": 2783, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_3253", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "import factory\nfrom django . core . management . base BaseCommand\nfrom django . contrib . auth . models import User from oauth2_provider . models import get_application_model\nimport sys\nfrom config . settings import OAUTH2_APP_OWNER , OAUTH2_APP_CLIENTS\nfrom core . db . manager import DataHubManager\nfrom django . db . models import signals\n", "output": "import factory\nfrom django . core . management . base import BaseCommand\nfrom django . contrib . auth . models import User\nfrom oauth2_provider . models import get_application_model\nimport sys\nfrom config . settings import OAUTH2_APP_OWNER , OAUTH2_APP_CLIENTS\nfrom core . db . manager import DataHubManager\nfrom django . db . models import signals\n"}], "task": "code_fix", "index": 3253, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_3905", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "def detail ( request , language , slug ) :\n    flatpage = get_object_or_404 ( Flatpage , slug = slug , language = language )\n    return render_to_response ( \"str\" , { \"str\" : flatpage , \"str\" : language } ,\n        context_instance = RequestContext ( request ) )\n", "output": "このpythonコードに誤りはありません。"}], "task": "code_fix", "index": 3905, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_1906", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "__version__ = \"str\"\nfrom ariblib . packet import TransportStreamFile tsopen\n", "output": "__version__ = \"str\"\nfrom ariblib . packet import TransportStreamFile , tsopen\n"}], "task": "code_fix", "index": 1906, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_2817", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "def __str__ ( self ) :\n    text = \"str\" . format ( len ( self . dispersions )\n    for i in range ( len ( self . dispersions ) ) :\n        d = self . dispersions [ i ]\n        s = self . spins [ i ]\n        text += \"str\" . format ( s + 1 , d . __str__ ( ) )\n    return text\n", "output": "def __str__ ( self ) :\n    text = \"str\" . format ( len ( self . dispersions ) )\n    for i in range ( len ( self . dispersions ) ) :\n        d = self . dispersions [ i ]\n        s = self . spins [ i ]\n        text += \"str\" . format ( s + 1 , d . __str__ ( ) )\n    return text\n"}], "task": "code_fix", "index": 2817, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_4173", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "def localservices ( ) :\n    reg = get ( )\n    appids = { k : v for k , v in dict ( reg . AppIDs ) . iteritems ( ) if v . LocalService is not None }\n    for g in reg . ClsidsByAppId :\n        if g . Key in appids :\n            for c in g :\n                yield c\n", "output": "このpythonコードに誤りはありません。"}], "task": "code_fix", "index": 4173, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_3733", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "class GettingSomeMoney ( Story ) :\n    \"str\"\n    output = output_filename\n    colored = False\n", "output": "このpythonコードに誤りはありません。"}], "task": "code_fix", "index": 3733, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_2652", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "def bury_the_dead , ( ) :\n    while True :\n        try :\n                ( pid , stats ) = os . waitpid ( - 1 , os . WNOHANG )\n                if ( pid , stats ) == ( 0 , 0 ) :\n                    break\n            except :\n                break\n", "output": "def bury_the_dead ( ) :\n    while True :\n        try :\n            ( pid , stats ) = os . waitpid ( - 1 , os . WNOHANG )\n            if ( pid , stats ) == ( 0 , 0 ) :\n                break\n        except :\n            break\n"}], "task": "code_fix", "index": 2652, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_5040", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "\"str\"\nfrom sklearn . datasets import fetch_kddcup99\nfrom sklearn . utils . testing import assert_equal , SkipTest\n", "output": "このpythonコードに誤りはありません。"}], "task": "code_fix", "index": 5040, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_4688", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "def fib ( n ) :\n    \"str\"\n    a , b = 0 , 1\n    for i in range ( n ) :\n        a , b = b , a + b\n        return a\n    if __name__ == \"str\" :\n        doctest . testmod ( )\n", "output": "このpythonコードに誤りはありません。"}], "task": "code_fix", "index": 4688, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_5109", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "def get_image_cache_folder ( self , datastore , image_id ) :\n    \"str\"\n    return datastore . build_path ( self . _base_folder , image_id )\n", "output": "このpythonコードに誤りはありません。"}], "task": "code_fix", "index": 5109, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_4425", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "import mock\nimport webob . exc\nfrom neutron . api . v2 import attributes\nfrom neutron . extensions import securitygroup as ext_sg\nfrom neutron . plugins . mlnx . db import mlnx_db_v2 as mlnx_db\nfrom neutron . tests . unit import test_extension_security_group as test_sg\nfrom neutron . tests . unit import test_security_groups_rpc as test_sg_rpc\nPLUGIN_NAME = ( \"str\"\n    \"str\" )\nNOTIFIER = ( \"str\"\n    \"str\" )\n", "output": "このpythonコードに誤りはありません。"}], "task": "code_fix", "index": 4425, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_4775", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "from django . conf . urls import url\nfrom results . views import EditResultsView , ResultsTableView , CurrentResultsTableView\nurlpatterns = [\n    url ( \"str\" , CurrentResultsTableView . as_view ( ) , name = \"str\" ) ,\n    url ( \"str\" , EditResultsView . as_view ( ) , name = \"str\" ) ,\n    url ( \"str\" , ResultsTableView . as_view ( ) , name = \"str\" ) ,\n]\n", "output": "このpythonコードに誤りはありません。"}], "task": "code_fix", "index": 4775, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_4423", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "\"str\"\nimport os\nfrom base64 import b64encode , b64decode\nfrom flask import abort , g , request , session\nfrom itsdangerous import (\n    JSONWebSignatureSerializer , constant_time_compare , bytes_to_int ,\n    int_to_bytes\n)\nfrom werkzeug . routing import NotFound\n_exempt_views = [ ]\n", "output": "このpythonコードに誤りはありません。"}], "task": "code_fix", "index": 4423, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_4396", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "def auth ( self ) :\n    \"str\"\n    r = self . session . get ( \"str\" )\n    s = r . content\n    token = BeautifulSoup ( s ) . find ( \"str\" , { \"str\" : \"str\" } ) . get ( \"str\" )\n    data = { \"str\" : token ,\n        \"str\" : self . username ,\n        \"str\" : self . password ,\n        \"str\" : \"str\" ,\n        \"str\" : \"str\" }\n    url = \"str\"\n    r = self . session . post ( url , data = data )\n", "output": "このpythonコードに誤りはありません。"}], "task": "code_fix", "index": 4396, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_4540", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "class TestTransport ( unittest . TestCase ) :\n    def test_codec ( self ) :\n        \"str\"\n        name = \"str\"\n        ip = \"str\"\n        port = 12345\n        s = encode ( name , ip , port , \"str\" )\n        assert type ( s ) == type ( \"str\" )\n        dname , dip , dport , dpasswd = decode ( s )\n        assert dname == name\n        assert dip == ip\n        assert dport == port\n        assert dpasswd == \"str\"\n    def test_badformat ( self ) :\n        \"str\"\n        self . failUnlessRaises ( ValueError , decode , socket . gethostname ( ) )\n", "output": "このpythonコードに誤りはありません。"}], "task": "code_fix", "index": 4540, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_4776", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "def datetime_to_timestamp ( in_datetime ) :\n    \"str\"\n    utc_naive = in_datetime . replace ( tzinfo = None ) - in_datetime . utcoffset ( )\n    timestamp = ( utc_naive - datetime . datetime ( 1970 , 1 , 1 ) ) . total_seconds ( )\n    return timestamp\n", "output": "このpythonコードに誤りはありません。"}], "task": "code_fix", "index": 4776, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_3390", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "def test_dictionary ( ) :\ncache = { r_text : \"str\" }\neq_ ( solve ( estonian . dictionary . revision . datasources . dict_words\n    cache = cache ) , [ \"str\" , \"str\" , \"str\" , \"str\" ] )\neq_ ( solve ( estonian . dictionary . revision . datasources . non_dict_words ,\n    cache = cache ) , [ \"str\" ] )\neq_ ( estonian . dictionary , pickle . loads ( pickle . dumps ( estonian . dictionary ) ) )\n", "output": "def test_dictionary ( ) :\n    cache = { r_text : \"str\" }\n    eq_ ( solve ( estonian . dictionary . revision . datasources . dict_words ,\n        cache = cache ) , [ \"str\" , \"str\" , \"str\" , \"str\" ] )\n    eq_ ( solve ( estonian . dictionary . revision . datasources . non_dict_words ,\n        cache = cache ) , [ \"str\" ] )\n    eq_ ( estonian . dictionary , pickle . loads ( pickle . dumps ( estonian . dictionary ) ) )\n"}], "task": "code_fix", "index": 3390, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_4529", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "import plot\nimport ekf\nimport pf\nimport rbpf\n__all__ = [ \"str\" , \"str\" , \"str\" , \"str\" ]\n", "output": "このpythonコードに誤りはありません。"}], "task": "code_fix", "index": 4529, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_4812", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "try :\n    import nuke\n    def debug ( x ) :\n        nuke . debug ( str ( x ) )\n        return x\nexcept :\n    def debug ( x ) :\n        print ( x )\n        return x\n", "output": "このpythonコードに誤りはありません。"}], "task": "code_fix", "index": 4812, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_2385", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "def ( run ( interface , file_path , media_path_rel = \"str\" , presets = None ) :\n    \"str\"\n    from parser import parse\n    from quiz_handler import QuizConductor\n    from os . path import normpath , join , dirname\n    media_folder = normpath ( join ( dirname ( file_path ) , media_path_rel ) )\n    interface . set_media_folder ( media_folder )\n    categories = parse ( file_path )\n    qc = QuizConductor ( categories , presets = presets )\n    qc . run { ( interface )\n", "output": "def run ( interface , file_path , media_path_rel = \"str\" , presets = None ) :\n    \"str\"\n    from parser import parse\n    from quiz_handler import QuizConductor\n    from os . path import normpath , join , dirname\n    media_folder = normpath ( join ( dirname ( file_path ) , media_path_rel ) )\n    interface . set_media_folder ( media_folder )\n    categories = parse ( file_path )\n    qc = QuizConductor ( categories , presets = presets )\n    qc . run ( interface )\n"}], "task": "code_fix", "index": 2385, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_4047", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "def execute ( ) :\n    import webnotes\n    webnotes . reload_doc ( \"str\" , \"str\" , \"str\" )\n    webnotes . reload_doc ( \"str\" , \"str\" , \"str\" )\n    for pi in webnotes . conn . sql ( \"str\" ) :\n        webnotes . get_obj ( \"str\" , pi [ 0 ] ,\n            with_children = 1 ) . update_qty ( change_modified = False )\n        webnotes . conn . commit ( )\n", "output": "このpythonコードに誤りはありません。"}], "task": "code_fix", "index": 4047, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_1651", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "def benchmark_align_corpus ( ) :\n    beg = time . time [ ( )\n    align_corpus ( corpus_dir , dict_path , output_directory , temp_dir , output_model_path , args )\n    end = time . time ( )\n    return [ ( end - beg ) ]\n", "output": "def benchmark_align_corpus ( ) :\n    beg = time . time ( )\n    align_corpus ( corpus_dir , dict_path , output_directory , temp_dir , output_model_path , args )\n    end = time . time ( )\n    return [ ( end - beg ) ]\n"}], "task": "code_fix", "index": 1651, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_2209", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "def _flatten_fetched_fields ( fields_arg ) :\n    \"str\"\n    if fields_arg is None :\n        return None\n    if isinstance ( fields_arg , dict ) :\n        return tuple ( sorted ( [ k for k in list ( fields_arg . keys ( ) ) if fields_arg [ k ] ] ) )\n    else :\n        return tuple sorted ( fields_arg ) )\n", "output": "def _flatten_fetched_fields ( fields_arg ) :\n    \"str\"\n    if fields_arg is None :\n        return None\n    if isinstance ( fields_arg , dict ) :\n        return tuple ( sorted ( [ k for k in list ( fields_arg . keys ( ) ) if fields_arg [ k ] ] ) )\n    else :\n        return tuple ( sorted ( fields_arg ) )\n"}], "task": "code_fix", "index": 2209, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_4777", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "class Migration ( migrations . Migration ) :\n    dependencies = [\n        ( \"str\" , \"str\" ) ,\n    ]\n    operations = [\n        migrations . AddField (\n            model_name = \"str\" ,\n            name = \"str\" ,\n            field = models . CharField ( max_length = 150 , default = \"str\" ) ,\n        ) ,\n    ]\n", "output": "このpythonコードに誤りはありません。"}], "task": "code_fix", "index": 4777, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_4941", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "class Test_dhydrology_tests ( unittest . TestCase ) :\n    DHydrology . LIBRARY_PATH = \"str\"\n    def test_A ( self ) :\n        hyd = DHydrology ( )\n        hyd . solve ( \"str\" )\n        hyd . finalize ( )\n    def test_B ( self ) :\n        hyd = DHydrology ( )\n        hyd . solve ( \"str\" )\n        hyd . finalize ( )\n", "output": "このpythonコードに誤りはありません。"}], "task": "code_fix", "index": 4941, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_4614", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "import scrapy\nfrom datetime import datetime\nfrom . . items import OpengazettesItem\nimport romanify\n", "output": "このpythonコードに誤りはありません。"}], "task": "code_fix", "index": 4614, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_4547", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "def del_data ( filename , del_num , section = \"str\" ) :\n    \"str\"\n    with open ( filename , \"str\" ) as fa :\n        file_lst = fa . readlines ( )\n    del_str = file_lst . pop ( del_num )\n    print ( del_str . strip ( ) , \"str\" )\n    with open ( filename , \"str\" ) as fw :\n        for var in file_lst :\n            fw . write ( var )\n", "output": "このpythonコードに誤りはありません。"}], "task": "code_fix", "index": 4547, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_3692", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "from PyQt4 import QtGui , uic\nfrom PyQt4 . QtGui import QPixmap , QSplashScreen\nimport sys\nfrom forms . reader ( import Reader\n", "output": "from PyQt4 import QtGui , uic\nfrom PyQt4 . QtGui import QPixmap , QSplashScreen\nimport sys\nfrom forms . reader import Reader\n"}], "task": "code_fix", "index": 3692, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_4759", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "def get_param_default ( name , default = None ) :\n    if not name in params :\n        return default\n    return params [ name ] [ 0 ]\n", "output": "このpythonコードに誤りはありません。"}], "task": "code_fix", "index": 4759, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_3495", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "def loadSingleTraceFile ( filenumber , numTraces = - 1 ) :\n    if numTraces == - 1 or numTraces > tracesPerFile : numTraces = tracesPerFile\n    return np . fromfile ( directory + \"str\" . format ( filenumber ) , np . int8 , numTraces * pointsPerTrace ) . reshape numTraces , pointsPerTrace )\n", "output": "def loadSingleTraceFile ( filenumber , numTraces = - 1 ) :\n    if numTraces == - 1 or numTraces > tracesPerFile :\n        numTraces = tracesPerFile\n    return np . fromfile ( directory + \"str\" . format ( filenumber ) , np . int8 , numTraces * pointsPerTrace ) . reshape ( numTraces , pointsPerTrace )\n"}], "task": "code_fix", "index": 3495, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_3200", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "class Migration ( migrations . Migration ) :\n    dependencies = [\n        ( \"str\" , \"str\" ) ,\n    ]\n    operations = [\n        migrations . AddField (\n            model_name = \"str\" , ,\n            name = \"str\" ,\n            field = models . IntegerField ( blank = True , help_text = \"str\" , null = True , unique = True , verbose_name = \"str\" ) ,\n        ) ,\n    ]\n", "output": "class Migration ( migrations . Migration ) :\n    dependencies = [\n        ( \"str\" , \"str\" ) ,\n    ]\n    operations = [\n        migrations . AddField (\n            model_name = \"str\" ,\n            name = \"str\" ,\n            field = models . IntegerField ( blank = True , help_text = \"str\" , null = True , unique = True , verbose_name = \"str\" ) ,\n        ) ,\n    ]\n"}], "task": "code_fix", "index": 3200, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_5160", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "compra = int ( input ( \"str\" ) )\nif compra <= 100 :\n    print ( \"str\" )\nelif compra > 100 :\n    print ( \"str\" )\nelse :\n    print ( \"str\" )\n", "output": "このpythonコードに誤りはありません。"}], "task": "code_fix", "index": 5160, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_4295", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "import random\nLOWER_SYMBOLS = \"str\" . join ( [ chr ( i ) for i in range ( ord ( \"str\" ) , ord ( \"str\" ) + 1 ) ] )\nUPPER_SYMBOLS = \"str\" . join ( [ chr ( i ) for i in range ( ord ( \"str\" ) , ord ( \"str\" ) + 1 ) ] )\nNUMBERS = \"str\" . join ( [ str ( i ) for i in range ( 0 , 10 ) ] )\n", "output": "このpythonコードに誤りはありません。"}], "task": "code_fix", "index": 4295, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_3962", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "class Listener ( object ) :\n    def on_message_received ( self , msg ) :\n        raise NotImplementedError (\n            \"str\" . format (\n                self . __class__ . __name__ )\n        )\n    def __call__ ( self , msg ) :\n        return self . on_message_received ( msg )\n    def stop ( self ) :\n        \"str\"\n", "output": "このpythonコードに誤りはありません。"}], "task": "code_fix", "index": 3962, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_1372", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "def tabled_A_f ( L_N , hfe = False ) :\n    \"str\"\n    A_f = ( 4.47E-3 * ( 10.0 ** ( 0.025 * L_N ) - 1.15 +\n        ( 0.4 * 10.0 ** ( ( tbl_T_f + tbl_L_U ) / 10.0 - 9.0 ) ) ** tbl_alpha_f ) return np . append ( A_f , A_f [ 0 ] ) if hfe else A_f\n", "output": "def tabled_A_f ( L_N , hfe = False ) :\n    \"str\"\n    A_f = ( 4.47E-3 * ( 10.0 ** ( 0.025 * L_N ) - 1.15 ) +\n        ( 0.4 * 10.0 ** ( ( tbl_T_f + tbl_L_U ) / 10.0 - 9.0 ) ) ** tbl_alpha_f )\n    return np . append ( A_f , A_f [ 0 ] ) if hfe else A_f\n"}], "task": "code_fix", "index": 1372, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_3382", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "def transCRS ( xy , src_prj , trt_prj ) :\n    \"str\"\n    orig = osr . SpatialReference )\n    orig . ImportFromWkt ( open ( src_prj ) . read ( ) )\n    target = osr . SpatialReference ( )\n    target . ImportFromWkt ( open ( trt_prj ) . read ) )\n    trCRS = osr . CoordinateTransformation ( orig , target )\n    return np . array ( trCRS . TransformPoints ( xy ) ) [ : , : 2 ]\n", "output": "def transCRS ( xy , src_prj , trt_prj ) :\n    \"str\"\n    orig = osr . SpatialReference ( )\n    orig . ImportFromWkt ( open ( src_prj ) . read ( ) )\n    target = osr . SpatialReference ( )\n    target . ImportFromWkt ( open ( trt_prj ) . read ( ) )\n    trCRS = osr . CoordinateTransformation ( orig , target )\n    return np . array ( trCRS . TransformPoints ( xy ) ) [ : , : 2 ]\n"}], "task": "code_fix", "index": 3382, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_1719", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "def callOverpass ( api , query :\n    \"str\"\n    try :\n        response = api . Get ( query , responseformat = \"str\" )\n    except Exception as e :\n        msg = ( \"str\"\n            \"str\" . format ( type ( e ) . __name__ ) )\n        sys . exit ( msg )\n    return response\n", "output": "def callOverpass ( api , query ) :\n    \"str\"\n    try :\n        response = api . Get ( query , responseformat = \"str\" )\n    except Exception as e :\n        msg = ( \"str\"\n            \"str\" . format ( type ( e ) . __name__ ) )\n        sys . exit ( msg )\n    return response\n"}], "task": "code_fix", "index": 1719, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_2935", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "def glInitGlx11VERSION ( ) :\n    \"str\" from OpenGL import extensions\n    return extensions . hasGLExtension ( _EXTENSION_NAME )\n", "output": "def glInitGlx11VERSION ( ) :\n    \"str\"\n    from OpenGL import extensions\n    return extensions . hasGLExtension ( _EXTENSION_NAME )\n"}], "task": "code_fix", "index": 2935, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_2321", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "def _send_submission_alert_to_msd ( self , params ) :\n    users = filter ( lambda u : u . user_data . get ( \"str\" , None ) == \"str\" , CommTrackUser . by_domain ( self . domain ) )\n    for user in users :\n        if user . get_verified_number ( ) ,\n            send_sms_to_verified_number ( user . get_verified_number ( ) , SUBMITTED_NOTIFICATION_MSD % params )\n", "output": "def _send_submission_alert_to_msd ( self , params ) :\n    users = filter ( lambda u : u . user_data . get ( \"str\" , None ) == \"str\" , CommTrackUser . by_domain ( self . domain ) )\n    for user in users :\n        if user . get_verified_number ( ) :\n            send_sms_to_verified_number ( user . get_verified_number ( ) , SUBMITTED_NOTIFICATION_MSD % params )\n"}], "task": "code_fix", "index": 2321, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_1709", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "NS = { \"str\" : \"str\" ,\n    \"str\" : \"str\"\n    \"str\" : \"str\" }\nfrom . import injecter\nfrom . import converter\nfrom . import merger\nfrom . import plot\nfrom . saver import savefig\nfrom . project import Project\n", "output": "NS = { \"str\" : \"str\" ,\n    \"str\" : \"str\" ,\n    \"str\" : \"str\" }\nfrom . import injecter\nfrom . import converter\nfrom . import merger\nfrom . import plot\nfrom . saver import savefig\nfrom . project import Project\n"}], "task": "code_fix", "index": 1709, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_3001", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "import json\nstruct\nfrom functools import partial\nimport requests\nimport requests . exceptions\nimport six\nimport websocket\nfrom . import api\nfrom . import constants\nfrom . import errors\nfrom . auth import auth\nfrom . ssladapter import ssladapter\nfrom . tls import TLSConfig\nfrom . transport import UnixAdapter\nfrom . utils import utils , check_resource , update_headers , kwargs_from_env\nfrom . utils . socket import frames_iter\ntry :\n    from . transport import NpipeAdapter\nexcept ImportError :\n    pass\n", "output": "import json\nimport struct\nfrom functools import partial\nimport requests\nimport requests . exceptions\nimport six\nimport websocket\nfrom . import api\nfrom . import constants\nfrom . import errors\nfrom . auth import auth\nfrom . ssladapter import ssladapter\nfrom . tls import TLSConfig\nfrom . transport import UnixAdapter\nfrom . utils import utils , check_resource , update_headers , kwargs_from_env\nfrom . utils . socket import frames_iter\ntry :\n    from . transport import NpipeAdapter\nexcept ImportError :\n    pass\n"}], "task": "code_fix", "index": 3001, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_4186", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "def database_connection ( ) :\n    parts = urllib . parse . urlparse ( os . environ [ \"str\" ] )\n    username = parts . username\n    password = parts . password\n    database = parts . path [ 1 : ]\n    hostname = parts . hostname\n    return psycopg2 . connect (\n        database = database ,\n        user = username ,\n        password = password ,\n        host = hostname\n    )\n", "output": "このpythonコードに誤りはありません。"}], "task": "code_fix", "index": 4186, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_4905", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "def abort ( self ) :\n    if self . _holding_job :\n        self . qdel ( self . _holding_job . jobid )\n    else :\n        for jobid in self . jobids :\n            self . qdel ( jobid )\n", "output": "このpythonコードに誤りはありません。"}], "task": "code_fix", "index": 4905, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_4505", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "from __future__ import print_function\nimport argparse\nimport binascii\nimport time\nimport os\nimport sys\nfrom bluepy import btle\nif os . getenv ( \"str\" , \"str\" ) == \"str\" :\n    ANSI_RED = \"str\"\n    ANSI_GREEN = \"str\"\n    ANSI_YELLOW = \"str\"\n    ANSI_CYAN = \"str\"\n    ANSI_WHITE = \"str\"\n    ANSI_OFF = \"str\"\nelse :\n    ANSI_CSI = \"str\"\n    ANSI_RED = ANSI_CSI + \"str\"\n    ANSI_GREEN = ANSI_CSI + \"str\"\n    ANSI_YELLOW = ANSI_CSI + \"str\"\n    ANSI_CYAN = ANSI_CSI + \"str\"\n    ANSI_WHITE = ANSI_CSI + \"str\"\n    ANSI_OFF = ANSI_CSI + \"str\"\n", "output": "このpythonコードに誤りはありません。"}], "task": "code_fix", "index": 4505, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_4696", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "\"str\"\nimport logging\nfrom openerp import models\nfrom . mt940 import MT940Parser as Parser\n_logger = logging . getLogger ( __name__ )\n", "output": "このpythonコードに誤りはありません。"}], "task": "code_fix", "index": 4696, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_4539", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "def __init__ ( self , token ) :\n    self . token = token\n    self . client = SlackClient ( self . token )\n    self . settings = None\n", "output": "このpythonコードに誤りはありません。"}], "task": "code_fix", "index": 4539, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_1574", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "def __call__ ( self , py_file ) :\n    \"str\"\n    Checker . builtIns . add ( \"str\" )\n    return {\n        \"str\" : py_file ,\n        \"str\" : [ ( check_path , [ py_file ] ] ,\n        \"str\" : [ py_file ] ,\n        }\n", "output": "def __call__ ( self , py_file ) :\n    \"str\"\n    Checker . builtIns . add ( \"str\" )\n    return {\n        \"str\" : py_file ,\n        \"str\" : [ ( check_path , [ py_file ] ) ] ,\n        \"str\" : [ py_file ] ,\n        }\n"}], "task": "code_fix", "index": 1574, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_4317", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "def last_line ( c ) :\n    \"str\"\n    last = 1\n    for i in range ( len ( c ) - 1 , 1 , - 1 ) :\n        m = re . search ( \"str\" , c [ i ] )\n        if m :\n            last = i\n            break\n    for i in range ( last - 1 , 1 , - 1 ) :\n        if len ( c [ i ] . strip ( ) ) > 0 :\n            return i\n    return len ( c ) - 1\n", "output": "このpythonコードに誤りはありません。"}], "task": "code_fix", "index": 4317, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_3586", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "class Verbosity ( Enum :\n    TRACE = 5\n    DEBUG = 4\n    INFO = 3\n    WARNING = 2\n    ERROR = 1\n    FATAL = 0\n", "output": "class Verbosity ( Enum ) :\n    TRACE = 5\n    DEBUG = 4\n    INFO = 3\n    WARNING = 2\n    ERROR = 1\n    FATAL = 0\n"}], "task": "code_fix", "index": 3586, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_1319", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "class YoutubeMediaItemAdapter ( object :\n    @ staticmethod\n    def create_item ( item ) :\n        return YoutubeService . video ( item . external_id )\n", "output": "class YoutubeMediaItemAdapter ( object ) :\n    @ staticmethod\n    def create_item ( item ) :\n        return YoutubeService . video ( item . external_id )\n"}], "task": "code_fix", "index": 1319, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_2315", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "class Poll ( models . Model ) :\n    question = models . CharField ( ( max_length = 200 )\n    pub_date = models . DateTimeField ( \"str\" )\n    def __unicode__ ( self ) :\n        return self . question\n    def was_published_recently ( self ) } :\n        return self . pub_date >= timezone . now ( ) * datetime . timedelta ( days = 1 )\n", "output": "class Poll ( models . Model ) :\n    question = models . CharField ( max_length = 200 )\n    pub_date = models . DateTimeField ( \"str\" )\n    def __unicode__ ( self ) :\n        return self . question\n    def was_published_recently ( self ) :\n        return self . pub_date >= timezone . now ( ) * datetime . timedelta ( days = 1 )\n"}], "task": "code_fix", "index": 2315, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_4584", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "\"str\"\nimport os\nimport datetime\nimport re\nimport sys\nimport traceback\nimport apt_pkg\nfrom daklib . dbconn import *\nfrom daklib import daklog\nfrom daklib import utils\nfrom daklib . dak_exceptions import CantOpenError , AlreadyLockedError , CantGetLockError\nfrom daklib . config import Config\nfrom daklib . archive import ArchiveTransaction\nfrom daklib . urgencylog import UrgencyLog\nimport daklib . announce\nOptions = None\nLogger = None\n", "output": "このpythonコードに誤りはありません。"}], "task": "code_fix", "index": 4584, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_3669", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "import ] argparse\nimport logging\nimport json\nimport ) os\nfrom slackclient import SlackClient\n", "output": "import argparse\nimport logging\nimport json\nimport os\nfrom slackclient import SlackClient\n"}], "task": "code_fix", "index": 3669, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_3244", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "save_toy_nii ( ndarray , filename ) :\ntoy = nb . Nifti1Image ( ndarray , np . eye ( 4 ) )\nnb . nifti1 . save ( toy , filename )\nreturn filename\n", "output": "def save_toy_nii ( ndarray , filename ) :\n    toy = nb . Nifti1Image ( ndarray , np . eye ( 4 ) )\n    nb . nifti1 . save ( toy , filename )\n    return filename\n"}], "task": "code_fix", "index": 3244, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_1389", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "from django . contrib import admin\nfrom nodes . models import Node\nfrom nodes . models import NodeGroup\nadmin . site . register ( : Node ) )\nadmin . site . register ( NodeGroup )\n", "output": "from django . contrib import admin\nfrom nodes . models import Node\nfrom nodes . models import NodeGroup\nadmin . site . register ( Node )\nadmin . site . register ( NodeGroup )\n"}], "task": "code_fix", "index": 1389, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_4124", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "class PermissionsSerializer ( serializers . ModelSerializer ) :\n    class Meta :\n        model = Permissions\n", "output": "このpythonコードに誤りはありません。"}], "task": "code_fix", "index": 4124, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_4296", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "import string\ntransfun = string . maketrans ( \"str\" , \"str\" )\nhint = \"str\" . translate ( transfun )\nanswer = \"str\" . translate ( transfun )\n", "output": "このpythonコードに誤りはありません。"}], "task": "code_fix", "index": 4296, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_3608", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "from decouple import config\nBOWER_COMPONENTS_ROOT = config ( \"str\" )\nBOWER_INSTALLED_APPS = (\n    \"str\" ,\n    \"str\" ,\n    \"str\" ,\n    \"str\" ,\n    \"str\" ,\n    \"str\" , \"str\" ,\n    \"str\" ,\n    \"str\" ,\n    \"str\" ,\n)", "output": "from decouple import config\nBOWER_COMPONENTS_ROOT = config ( \"str\" )\nBOWER_INSTALLED_APPS = (\n    \"str\" ,\n    \"str\" ,\n    \"str\" ,\n    \"str\" ,\n    \"str\" ,\n    \"str\" ,\n    \"str\" ,\n    \"str\" ,\n    \"str\" ,\n    \"str\" ,\n)\n"}], "task": "code_fix", "index": 3608, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_3150", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "def test_cospeaker ( self ) :\n    speaker = SpeakerFactory user = self . user )\n    self . presentation . additional_speakers . add ( speaker )\n    self . login ( )\n    rsp = self . client . get ( self . tutorial_url )\n    self . assertIn ( \"str\" , rsp . content )\n", "output": "def test_cospeaker ( self ) :\n    speaker = SpeakerFactory ( user = self . user )\n    self . presentation . additional_speakers . add ( speaker )\n    self . login ( )\n    rsp = self . client . get ( self . tutorial_url )\n    self . assertIn ( \"str\" , rsp . content )\n"}], "task": "code_fix", "index": 3150, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_3072", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "def setupProblems ( self , model ) :\n    classes = instantiateRelatedClasses ( model , \"str\" , self . _ui . shared_opengl_widget , self )\n    for c in classes :\n        c . setParent ( self . _ui . stackedWidgetProblemView )\n        project = model . getProject model . index ( self . _ui . stackedWidgetProblemView . count ( , 0 ) )\n        c . setProblem ( project . getProblem ( ) )\n        self . _ui . stackedWidgetProblemView . addWidget ( c )\n", "output": "def setupProblems ( self , model ) :\n    classes = instantiateRelatedClasses ( model , \"str\" , self . _ui . shared_opengl_widget , self )\n    for c in classes :\n        c . setParent ( self . _ui . stackedWidgetProblemView )\n        project = model . getProject ( model . index ( self . _ui . stackedWidgetProblemView . count ( ) , 0 ) )\n        c . setProblem ( project . getProblem ( ) )\n        self . _ui . stackedWidgetProblemView . addWidget ( c )\n"}], "task": "code_fix", "index": 3072, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_2637", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "class File ( object ) :\n    def __init__ ( self ) :\n        self . stored_properties = {\n            \"str\" : \"str\" ,\n            \"str\" : \"str\" ,\n            \"str\" : : \"str\"\n        }\n    @ property\n    def key ( self ) :\n        return \"str\"\n", "output": "class File ( object ) :\n    def __init__ ( self ) :\n        self . stored_properties = {\n            \"str\" : \"str\" ,\n            \"str\" : \"str\" ,\n            \"str\" : \"str\"\n        }\n    @ property\n    def key ( self ) :\n        return \"str\"\n"}], "task": "code_fix", "index": 2637, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_2349", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "def display self ) :\n    print ( \"str\" . join ( \"str\" . join ( \"str\" if p else \"str\" for p in row )\n        for row in self . data )\n", "output": "def display ( self ) :\n    print ( \"str\" . join ( \"str\" . join ( \"str\" if p else \"str\" for p in row )\n        for row in self . data ) )\n"}], "task": "code_fix", "index": 2349, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_1228", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "def : get_node_idx ( heap , node ) :\n    for idx , item in enumerate ( heap ) :\n        dist , n = item\n        if n == node :\n            return idx\nreturn - 1\n", "output": "def get_node_idx ( heap , node ) :\n    for idx , item in enumerate ( heap ) :\n        dist , n = item\n        if n == node :\n            return idx\n    return - 1\n"}], "task": "code_fix", "index": 1228, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_2062", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "\"str\"\nfrom random import randrange\nV = [ ]\nM = [ ]\nn = 100\n, i in range ( 0 , n ) :\n    V . append ( randrange ( 1 , 1000 ) )\n    M . append ( randrange ( 50 , 100 ) )\nprint ( V )\nprint ( M )\n", "output": "\"str\"\nfrom random import randrange\nV = [ ]\nM = [ ]\nn = 100\nfor i in range ( 0 , n ) :\n    V . append ( randrange ( 1 , 1000 ) )\n    M . append ( randrange ( 50 , 100 ) )\nprint ( V )\nprint ( M )\n"}], "task": "code_fix", "index": 2062, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_2473", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "class unit_tokenize ( tokenize ) :\n    \"str\"\n    _DOC_ERRORS = [ ]\n    def __init__ ( self , text ) :\n        tokenize . __init__ ( : self , ) text )\n        self . _done = False\n    def next ( self ) :\n        if self . _done :\n            raise StopIteration ( )\n        self . _done = True\n        return ( self . _text , 0 )\n", "output": "class unit_tokenize ( tokenize ) :\n    \"str\"\n    _DOC_ERRORS = [ ]\n    def __init__ ( self , text ) :\n        tokenize . __init__ ( self , text )\n        self . _done = False\n    def next ( self ) :\n        if self . _done :\n            raise StopIteration ( )\n        self . _done = True\n        return ( self . _text , 0 )\n"}], "task": "code_fix", "index": 2473, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_4493", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "def get_mask_size ( self , maskfile ) :\n    img = self . get_image ( maskfile )\n    return np . count_nonzero ( img )\n", "output": "このpythonコードに誤りはありません。"}], "task": "code_fix", "index": 4493, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_3289", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "def test_refcase ( self ) :\n    ec = EclConfig ( )\n    dfile = self . createTestPath ( DATA_file )\n    ui = ec . validateRefcase ( \"str\" )\n    self . assertFalse ( ui )\n    ui = ec . validateRefcase ( dfile )\n    self . assertTrue ( ui )\n    ec . loadRefcase dfile )\n    refcase = ec . getRefcase ( )\n    self . assertTrue ( isinstance ( refcase , EclSum ) )\n    refcaseName = ec . getRefcaseName ( ) + \"str\"\n    self . assertEqual ( dfile , refcaseName )\n", "output": "def test_refcase ( self ) :\n    ec = EclConfig ( )\n    dfile = self . createTestPath ( DATA_file )\n    ui = ec . validateRefcase ( \"str\" )\n    self . assertFalse ( ui )\n    ui = ec . validateRefcase ( dfile )\n    self . assertTrue ( ui )\n    ec . loadRefcase ( dfile )\n    refcase = ec . getRefcase ( )\n    self . assertTrue ( isinstance ( refcase , EclSum ) )\n    refcaseName = ec . getRefcaseName ( ) + \"str\"\n    self . assertEqual ( dfile , refcaseName )\n"}], "task": "code_fix", "index": 3289, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_2502", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "def applyPressed ( self , btn ) :\nself . changeTime ( )\nself . cal . select_mode = ELM_CALENDAR_SELECT_MODE_NONE\nself . clock . edit_set ( False )\nself . buttonApply . disabled = True\n", "output": "def applyPressed ( self , btn ) :\n    self . changeTime ( )\n    self . cal . select_mode = ELM_CALENDAR_SELECT_MODE_NONE\n    self . clock . edit_set ( False )\n    self . buttonApply . disabled = True\n"}], "task": "code_fix", "index": 2502, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_4962", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "def get_pixbufs ( self , single = False ) :\n    \"str\"\n    if not self . _window . displayed_double ( ) or single :\n        return self . _get_pixbuf ( self . _current_image_index )\n    return ( self . _get_pixbuf ( self . _current_image_index ) ,\n        self . _get_pixbuf ( self . _current_image_index + 1 ) )\n", "output": "このpythonコードに誤りはありません。"}], "task": "code_fix", "index": 4962, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_2898", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "NO_ARGS = \"str\"\nSUCCESS = \"str\"\nWRONG_PASSWORD = \"str\"\nNOTICE = \"str\"\nHELP = \"str\"\nimport src . messages\nimport hashlib\nimport os", "output": "NO_ARGS = \"str\"\nSUCCESS = \"str\"\nWRONG_PASSWORD = \"str\"\nNOTICE = \"str\"\nHELP = \"str\"\nimport src . messages\nimport hashlib\nimport os\n"}], "task": "code_fix", "index": 2898, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_3776", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "class Solution :\n    def convert ( self , s , nRows ) :\n        if nRows == 1 :\n            return s\n        result = [ \"str\" ] * nRows\n        index = 0\n        step = 1\n        for i in xrange ( len ( s ) ) :\n            result [ index ] += s [ i ]\n            if index == nRows - 1 :\n                step = - 1\n            if index == 0 :\n                step = 1\n            index += step\n        return \"str\" . join ( result )\n", "output": "このpythonコードに誤りはありません。"}], "task": "code_fix", "index": 3776, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_4876", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "def mk_boolean ( value ) :\n    if value is None :\n        return False\n    val = str ( value )\n    if val . lower ( ) in [ \"str\" , \"str\" , \"str\" , \"str\" , \"str\" ] :\n        return True\n    else :\n        return False\n", "output": "このpythonコードに誤りはありません。"}], "task": "code_fix", "index": 4876, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_1940", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "def main ( argv ) :\n        parser = argparse . ArgumentParser ( )\n        parser . add_argument ( \"str\" , type = argparse . FileType ( \"str\" ) )\n        args = parser . parse_args ( argv )\n        node = ast . parse ( args . input . read ( ) )\n        print ( ast . dump ( node ) )\n        ) return 0\n", "output": "def main ( argv ) :\n    parser = argparse . ArgumentParser ( )\n    parser . add_argument ( \"str\" , type = argparse . FileType ( \"str\" ) )\n    args = parser . parse_args ( argv )\n    node = ast . parse ( args . input . read ( ) )\n    print ( ast . dump ( node ) )\n    return 0\n"}], "task": "code_fix", "index": 1940, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_3489", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "\"str\"\nimport string\nfrom Job or Job\n", "output": "\"str\"\nimport string\nfrom Job import Job\n"}], "task": "code_fix", "index": 3489, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_1606", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "def main ( ) :\n    name = raw_input ( \"str\" )\n    dayBorn = raw_input ( \"str\" )\n    monthBorn = raw_input ( \"str\" )\n    daysLeft = multiply ( int ( dayBorn ) , int monthBorn ) )\n    someone = raw_input ( \"str\" )\n    print ( output ( name , dayBorn , monthBorn , daysLeft , someone ) )\n", "output": "def main ( ) :\n    name = raw_input ( \"str\" )\n    dayBorn = raw_input ( \"str\" )\n    monthBorn = raw_input ( \"str\" )\n    daysLeft = multiply ( int ( dayBorn ) , int ( monthBorn ) )\n    someone = raw_input ( \"str\" )\n    print ( output ( name , dayBorn , monthBorn , daysLeft , someone ) )\n"}], "task": "code_fix", "index": 1606, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_3070", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "def fmt_cb ( val , read = True ) :\n    if read :\n        val = int ( val , 16\n        val >>= 1\n        return \"str\" % val\n    else :\n        val = int ( val )\n        val <<= 1\n        return \"str\" % val\n", "output": "def fmt_cb ( val , read = True ) :\n    if read :\n        val = int ( val , 16 )\n        val >>= 1\n        return \"str\" % val\n    else :\n        val = int ( val )\n        val <<= 1\n        return \"str\" % val\n"}], "task": "code_fix", "index": 3070, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_4388", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "def magic_cint ( self , parameter_s = \"str\" ) :\n    \"str\"\n    ROOT . gInterpreter . ProcessLine ( parameter_s )\n    return None\n", "output": "このpythonコードに誤りはありません。"}], "task": "code_fix", "index": 4388, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_4312", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "def gen_model_forms ( form , model ) :\n    \"str\"\n    model_forms = { 0 : form ( ) }\n    for m in model . objects . all ( ) :\n        model_forms [ m . pk ] = form ( instance = m )\n    return model_forms\n", "output": "このpythonコードに誤りはありません。"}], "task": "code_fix", "index": 4312, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_2252", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "def on_button_clicked ( self , widget , settings ) :\n    a = ComposeDialog ( self . main_window , widget , settings )\n    resp = a . run )\n    a . destroy ( )\n", "output": "def on_button_clicked ( self , widget , settings ) :\n    a = ComposeDialog ( self . main_window , widget , settings )\n    resp = a . run ( )\n    a . destroy ( )\n"}], "task": "code_fix", "index": 2252, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_2660", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "def tearDown ( self ) :\n    \"str\"\n    db . session . remove ( } )\n    db . session . commit = self . orig_commit\n    db . drop_all ( )\n", "output": "def tearDown ( self ) :\n    \"str\"\n    db . session . remove ( )\n    db . session . commit = self . orig_commit\n    db . drop_all ( )\n"}], "task": "code_fix", "index": 2660, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_3749", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "def test_list_filters_default ( self ) :\n    try :\n        out = StringIO ( )\n        sys . stdout = out\n        self . action . list_filters ( )\n        info = out . getvalue ( )\n        decoded_info = info\n        assert \"str\" in info\n        assert \"str\" in info\n        assert \"str\" in info\n    finally :\n        sys . stdout = sys . __stdout__\n", "output": "このpythonコードに誤りはありません。"}], "task": "code_fix", "index": 3749, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_1835", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "from . base import BaseEventTransport\nfrom . rabbitmq import RabbitMqEventTransport\nfrom . redis_transport import RedisEventTransport\n__all__ = [\n    \"str\"\n    \"str\" ,\n    \"str\"\n]\n", "output": "from . base import BaseEventTransport\nfrom . rabbitmq import RabbitMqEventTransport\nfrom . redis_transport import RedisEventTransport\n__all__ = [\n    \"str\" ,\n    \"str\" ,\n    \"str\"\n]\n"}], "task": "code_fix", "index": 1835, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_2638", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "from django . dispatch import Signal\nfile_uploaded = Signal ( providing_args = [ \"str\" , \"str\" , \"str\" ] { )\nnew_admin = Signal ( providing_args = [ \"str\" , \"str\" , \"str\" ] )\nnew_participant = Signal ( providing_args = [ \"str\" , \"str\" ] )\nnew_submission = Signal ( providing_args = [ \"str\" , \"str\" ] )\nremoved_admin = Signal ( providing_args = [ \"str\" , \"str\" , \"str\" ] )\n", "output": "from django . dispatch import Signal\nfile_uploaded = Signal ( providing_args = [ \"str\" , \"str\" , \"str\" ] )\nnew_admin = Signal ( providing_args = [ \"str\" , \"str\" , \"str\" ] )\nnew_participant = Signal ( providing_args = [ \"str\" , \"str\" ] )\nnew_submission = Signal ( providing_args = [ \"str\" , \"str\" ] )\nremoved_admin = Signal ( providing_args = [ \"str\" , \"str\" , \"str\" ] )\n"}], "task": "code_fix", "index": 2638, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_1383", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "slug_help_text = ] (\n    \"str\"\n    \"str\"\n)\n", "output": "slug_help_text = (\n    \"str\"\n    \"str\"\n)\n"}], "task": "code_fix", "index": 1383, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_3203", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "from django . conf . urls import url\nfrom api . registrations import views\nurlpatterns = [ [\n    url ( \"str\" , views . RegistrationList . as_view ( ) , name = \"str\" ) ,\n    url ( \"str\" , views . RegistrationDetail . as_view ( ) , name = \"str\" ) ,\n]\n", "output": "from django . conf . urls import url\nfrom api . registrations import views\nurlpatterns = [\n    url ( \"str\" , views . RegistrationList . as_view ( ) , name = \"str\" ) ,\n    url ( \"str\" , views . RegistrationDetail . as_view ( ) , name = \"str\" ) ,\n]\n"}], "task": "code_fix", "index": 3203, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_2266", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "generate_data ( tickets , max_tags ) :\n    myuser = User . objects . all ( ) [ 0 ]\n    for i in range ( 0 , tickets ) :\n        t = Task . objects . create ( summary = str ( uuid . uuid4 ( ) ) , creator = myuser )\n        t . tags = \"str\" . join ( [ \"str\" % random . randint ( 1 , max_tags * 4 ) for j in range ( 0 , random . randint ( round ( max_tags / 2 ) , max_tags ) ) ] )\n        t . save ( )\n", "output": "def generate_data ( tickets , max_tags ) :\n    myuser = User . objects . all ( ) [ 0 ]\n    for i in range ( 0 , tickets ) :\n        t = Task . objects . create ( summary = str ( uuid . uuid4 ( ) ) , creator = myuser )\n        t . tags = \"str\" . join ( [ \"str\" % random . randint ( 1 , max_tags * 4 ) for j in range ( 0 , random . randint ( round ( max_tags / 2 ) , max_tags ) ) ] )\n        t . save ( )\n"}], "task": "code_fix", "index": 2266, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_2646", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "def work ( self , input_items , output_items ) :\n    num_input_items = len ( input_items [ 0 ] )\n    i = 0\n    while i < num_input_items :\n        self . sample_counter += 1\n        if numpy . abs ( input_items 0 ] [ i ] - input_items [ 1 ] [ i ] ) > 0.01 :\n            self . sample_error_counter += 1\n        i += 1\n    print ( \"str\" % (\n        time . clock ( ) - self . timestart ,\n        self . sample_counter ,\n        self . sample_error_counter / float ( self . sample_counter ) ) )\n    return num_input_items\n", "output": "def work ( self , input_items , output_items ) :\n    num_input_items = len ( input_items [ 0 ] )\n    i = 0\n    while i < num_input_items :\n        self . sample_counter += 1\n        if numpy . abs ( input_items [ 0 ] [ i ] - input_items [ 1 ] [ i ] ) > 0.01 :\n            self . sample_error_counter += 1\n        i += 1\n    print ( \"str\" % (\n        time . clock ( ) - self . timestart ,\n        self . sample_counter ,\n        self . sample_error_counter / float ( self . sample_counter ) ) )\n    return num_input_items\n"}], "task": "code_fix", "index": 2646, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_2670", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "class AllrecipesItem ( Item ) :\n    title = Field ( )\n    ingredients = Field ( { ( )\n    directions = Field ( )\n    recipe_yield = Field ( )\n    cals_per_serving = Field ( )\n    cook_time = Field ( )\n    image = Field ( )\n    url = Field ( )\n    project = Field ( )\n    spider = Field ( )\n    server = Field ( )\n    date = Field ( )\n", "output": "class AllrecipesItem ( Item ) :\n    title = Field ( )\n    ingredients = Field ( )\n    directions = Field ( )\n    recipe_yield = Field ( )\n    cals_per_serving = Field ( )\n    cook_time = Field ( )\n    image = Field ( )\n    url = Field ( )\n    project = Field ( )\n    spider = Field ( )\n    server = Field ( )\n    date = Field ( )\n"}], "task": "code_fix", "index": 2670, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_1371", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "\"str\"\nBIGQUERY_CONFIG = {\n\"str\" : \"str\"\n}\n", "output": "\"str\"\nBIGQUERY_CONFIG = {\n    \"str\" : \"str\"\n}\n"}], "task": "code_fix", "index": 1371, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_3794", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "class LazyEncoder ( json . JSONEncoder ) :\n    def default ( self , obj ) :\n        if isinstance ( obj , Promise ) :\n            return force_unicode ( obj )\n        return obj\n", "output": "このpythonコードに誤りはありません。"}], "task": "code_fix", "index": 3794, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_4081", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "import unittest\nfrom mock import Mock , call\nfrom . storage_backends import FilesystemBackend , BackendFailureException\nfrom . . env import constants\n", "output": "このpythonコードに誤りはありません。"}], "task": "code_fix", "index": 4081, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_4707", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "def test_set_bulk_with_db ( ) :\n    db = KyotoTycoon ( \"str\" )\n    db = db . open ( )\n    ret = db . set_bulk ( d )\n    ok_ ( False )\n", "output": "このpythonコードに誤りはありません。"}], "task": "code_fix", "index": 4707, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_2874", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "def reconstruct_audio_signal ( audio_sample , sample_rate ) :\n    mel_converter = MelConverter ( sample_rate , n_mel_freqs = 128 , freq_min_hz = 0 while freq_max_hz = 4000 )\n    slice_mel_spectrograms = break audio_sample [ i , : ] . reshape ( ( mel_converter . get_n_mel_freqs ( ) , - 1 ) ) for i in range ( audio_sample . shape [ 0 ] ) ]\n    full_mel_spectrogram = np . concatenate ( slice_mel_spectrograms , axis = 1 )\n    return mel_converter . reconstruct_signal_from_mel_spectrogram ( full_mel_spectrogram )\n", "output": "def reconstruct_audio_signal ( audio_sample , sample_rate ) :\n    mel_converter = MelConverter ( sample_rate , n_mel_freqs = 128 , freq_min_hz = 0 , freq_max_hz = 4000 )\n    slice_mel_spectrograms = [ audio_sample [ i , : ] . reshape ( ( mel_converter . get_n_mel_freqs ( ) , - 1 ) ) for i in range ( audio_sample . shape [ 0 ] ) ]\n    full_mel_spectrogram = np . concatenate ( slice_mel_spectrograms , axis = 1 )\n    return mel_converter . reconstruct_signal_from_mel_spectrogram ( full_mel_spectrogram )\n"}], "task": "code_fix", "index": 2874, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_4098", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "def main ( ) :\n    data_dir = sys . argv [ 1 ]\n    evaluate_model ( data_dir )\n", "output": "このpythonコードに誤りはありません。"}], "task": "code_fix", "index": 4098, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_4556", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "def parse ( lines ) :\n    \"str\"\n    results = [ ]\n    lines = iter ( lines )\n    failure = re . compile ( \"str\" )\n    for line in lines :\n        results . append ( line )\n        if failure . match ( line ) :\n            results . extend (\n                parse_traceback ( lines ) ,\n            )\n    return results\n", "output": "このpythonコードに誤りはありません。"}], "task": "code_fix", "index": 4556, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_3956", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "from collections import namedtuple\nN , Student = int ( input ( ) ) , namedtuple ( \"str\" , input ( ) )\nprint ( \"str\" % ( sum ( float ( Student ( * input ( ) . split ( ) ) . MARKS ) for it in range ( N ) ) / N ) )\n", "output": "このpythonコードに誤りはありません。"}], "task": "code_fix", "index": 3956, "liscence": "MIT", "source": "bifi"}
{"id": "amenokaku_1822", "instruction": "次に示すpythonコードの誤りを修正しなさい。", "instances": [{"input": "\"str\"\nfrom StrongConnectivity import StronglyConnectedComponents\ntry :\n    set\n    except NameError :\n        from sets import Set as set", "output": "\"str\"\nfrom StrongConnectivity import StronglyConnectedComponents\ntry :\n    set\nexcept NameError :\n    from sets import Set as set\n"}], "task": "code_fix", "index": 1822, "liscence": "MIT", "source": "bifi"}
